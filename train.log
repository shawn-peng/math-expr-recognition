I0421 23:13:34.851579 32711 caffe.cpp:185] Using GPUs 0
I0421 23:13:34.857180 32711 caffe.cpp:190] GPU 0: GeForce GTX 980M
I0421 23:13:35.005902 32711 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.00023
display: 1000
max_iter: 60000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.00028
snapshot: 5000
snapshot_prefix: "/home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/"
solver_mode: GPU
device_id: 0
net: "/home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt"
I0421 23:13:35.006124 32711 solver.cpp:91] Creating training net from net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:13:35.006425 32711 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0421 23:13:35.006439 32711 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0421 23:13:35.006530 32711 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/train"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:13:35.006620 32711 layer_factory.hpp:77] Creating layer mnist
I0421 23:13:35.007053 32711 net.cpp:91] Creating Layer mnist
I0421 23:13:35.007083 32711 net.cpp:399] mnist -> data
I0421 23:13:35.007158 32711 net.cpp:399] mnist -> label
I0421 23:13:35.007905 32721 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/train
I0421 23:13:35.014251 32711 data_layer.cpp:41] output data size: 64,1,28,28
I0421 23:13:35.015017 32711 net.cpp:141] Setting up mnist
I0421 23:13:35.015055 32711 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0421 23:13:35.015079 32711 net.cpp:148] Top shape: 64 (64)
I0421 23:13:35.015082 32711 net.cpp:156] Memory required for data: 200960
I0421 23:13:35.015135 32711 layer_factory.hpp:77] Creating layer conv1
I0421 23:13:35.015154 32711 net.cpp:91] Creating Layer conv1
I0421 23:13:35.015161 32711 net.cpp:425] conv1 <- data
I0421 23:13:35.015174 32711 net.cpp:399] conv1 -> conv1
I0421 23:13:35.141183 32711 net.cpp:141] Setting up conv1
I0421 23:13:35.141232 32711 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0421 23:13:35.141263 32711 net.cpp:156] Memory required for data: 3150080
I0421 23:13:35.141302 32711 layer_factory.hpp:77] Creating layer pool1
I0421 23:13:35.141314 32711 net.cpp:91] Creating Layer pool1
I0421 23:13:35.141330 32711 net.cpp:425] pool1 <- conv1
I0421 23:13:35.141350 32711 net.cpp:399] pool1 -> pool1
I0421 23:13:35.141401 32711 net.cpp:141] Setting up pool1
I0421 23:13:35.141408 32711 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0421 23:13:35.141412 32711 net.cpp:156] Memory required for data: 3887360
I0421 23:13:35.141417 32711 layer_factory.hpp:77] Creating layer conv2
I0421 23:13:35.141425 32711 net.cpp:91] Creating Layer conv2
I0421 23:13:35.141429 32711 net.cpp:425] conv2 <- pool1
I0421 23:13:35.141434 32711 net.cpp:399] conv2 -> conv2
I0421 23:13:35.142267 32711 net.cpp:141] Setting up conv2
I0421 23:13:35.142279 32711 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0421 23:13:35.142283 32711 net.cpp:156] Memory required for data: 4706560
I0421 23:13:35.142309 32711 layer_factory.hpp:77] Creating layer pool2
I0421 23:13:35.142316 32711 net.cpp:91] Creating Layer pool2
I0421 23:13:35.142320 32711 net.cpp:425] pool2 <- conv2
I0421 23:13:35.142325 32711 net.cpp:399] pool2 -> pool2
I0421 23:13:35.142369 32711 net.cpp:141] Setting up pool2
I0421 23:13:35.142375 32711 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0421 23:13:35.142379 32711 net.cpp:156] Memory required for data: 4911360
I0421 23:13:35.142382 32711 layer_factory.hpp:77] Creating layer ip1
I0421 23:13:35.142423 32711 net.cpp:91] Creating Layer ip1
I0421 23:13:35.142426 32711 net.cpp:425] ip1 <- pool2
I0421 23:13:35.142432 32711 net.cpp:399] ip1 -> ip1
I0421 23:13:35.145037 32711 net.cpp:141] Setting up ip1
I0421 23:13:35.145048 32711 net.cpp:148] Top shape: 64 500 (32000)
I0421 23:13:35.145052 32711 net.cpp:156] Memory required for data: 5039360
I0421 23:13:35.145077 32711 layer_factory.hpp:77] Creating layer relu1
I0421 23:13:35.145083 32711 net.cpp:91] Creating Layer relu1
I0421 23:13:35.145087 32711 net.cpp:425] relu1 <- ip1
I0421 23:13:35.145092 32711 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:13:35.145318 32711 net.cpp:141] Setting up relu1
I0421 23:13:35.145328 32711 net.cpp:148] Top shape: 64 500 (32000)
I0421 23:13:35.145331 32711 net.cpp:156] Memory required for data: 5167360
I0421 23:13:35.145335 32711 layer_factory.hpp:77] Creating layer ip2
I0421 23:13:35.145359 32711 net.cpp:91] Creating Layer ip2
I0421 23:13:35.145364 32711 net.cpp:425] ip2 <- ip1
I0421 23:13:35.145369 32711 net.cpp:399] ip2 -> ip2
I0421 23:13:35.145741 32711 net.cpp:141] Setting up ip2
I0421 23:13:35.145748 32711 net.cpp:148] Top shape: 64 101 (6464)
I0421 23:13:35.145751 32711 net.cpp:156] Memory required for data: 5193216
I0421 23:13:35.145774 32711 layer_factory.hpp:77] Creating layer loss
I0421 23:13:35.145781 32711 net.cpp:91] Creating Layer loss
I0421 23:13:35.145784 32711 net.cpp:425] loss <- ip2
I0421 23:13:35.145789 32711 net.cpp:425] loss <- label
I0421 23:13:35.145794 32711 net.cpp:399] loss -> loss
I0421 23:13:35.145807 32711 layer_factory.hpp:77] Creating layer loss
I0421 23:13:35.146368 32711 net.cpp:141] Setting up loss
I0421 23:13:35.146379 32711 net.cpp:148] Top shape: (1)
I0421 23:13:35.146384 32711 net.cpp:151]     with loss weight 1
I0421 23:13:35.146436 32711 net.cpp:156] Memory required for data: 5193220
I0421 23:13:35.146441 32711 net.cpp:217] loss needs backward computation.
I0421 23:13:35.146445 32711 net.cpp:217] ip2 needs backward computation.
I0421 23:13:35.146450 32711 net.cpp:217] relu1 needs backward computation.
I0421 23:13:35.146452 32711 net.cpp:217] ip1 needs backward computation.
I0421 23:13:35.146456 32711 net.cpp:217] pool2 needs backward computation.
I0421 23:13:35.146459 32711 net.cpp:217] conv2 needs backward computation.
I0421 23:13:35.146463 32711 net.cpp:217] pool1 needs backward computation.
I0421 23:13:35.146467 32711 net.cpp:217] conv1 needs backward computation.
I0421 23:13:35.146471 32711 net.cpp:219] mnist does not need backward computation.
I0421 23:13:35.146474 32711 net.cpp:261] This network produces output loss
I0421 23:13:35.146486 32711 net.cpp:274] Network initialization done.
I0421 23:13:35.146741 32711 solver.cpp:181] Creating test net (#0) specified by net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:13:35.146780 32711 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0421 23:13:35.146858 32711 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/test"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:13:35.146937 32711 layer_factory.hpp:77] Creating layer mnist
I0421 23:13:35.147388 32711 net.cpp:91] Creating Layer mnist
I0421 23:13:35.147397 32711 net.cpp:399] mnist -> data
I0421 23:13:35.147423 32711 net.cpp:399] mnist -> label
I0421 23:13:35.148205 32723 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/test
I0421 23:13:35.148387 32711 data_layer.cpp:41] output data size: 1000,1,28,28
I0421 23:13:35.152263 32711 net.cpp:141] Setting up mnist
I0421 23:13:35.152308 32711 net.cpp:148] Top shape: 1000 1 28 28 (784000)
I0421 23:13:35.152331 32711 net.cpp:148] Top shape: 1000 (1000)
I0421 23:13:35.152335 32711 net.cpp:156] Memory required for data: 3140000
I0421 23:13:35.152354 32711 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0421 23:13:35.152386 32711 net.cpp:91] Creating Layer label_mnist_1_split
I0421 23:13:35.152392 32711 net.cpp:425] label_mnist_1_split <- label
I0421 23:13:35.152398 32711 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I0421 23:13:35.152407 32711 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I0421 23:13:35.152511 32711 net.cpp:141] Setting up label_mnist_1_split
I0421 23:13:35.152528 32711 net.cpp:148] Top shape: 1000 (1000)
I0421 23:13:35.152550 32711 net.cpp:148] Top shape: 1000 (1000)
I0421 23:13:35.152554 32711 net.cpp:156] Memory required for data: 3148000
I0421 23:13:35.152557 32711 layer_factory.hpp:77] Creating layer conv1
I0421 23:13:35.152570 32711 net.cpp:91] Creating Layer conv1
I0421 23:13:35.152573 32711 net.cpp:425] conv1 <- data
I0421 23:13:35.152585 32711 net.cpp:399] conv1 -> conv1
I0421 23:13:35.154070 32711 net.cpp:141] Setting up conv1
I0421 23:13:35.154088 32711 net.cpp:148] Top shape: 1000 20 24 24 (11520000)
I0421 23:13:35.154093 32711 net.cpp:156] Memory required for data: 49228000
I0421 23:13:35.154136 32711 layer_factory.hpp:77] Creating layer pool1
I0421 23:13:35.154144 32711 net.cpp:91] Creating Layer pool1
I0421 23:13:35.154165 32711 net.cpp:425] pool1 <- conv1
I0421 23:13:35.154172 32711 net.cpp:399] pool1 -> pool1
I0421 23:13:35.154256 32711 net.cpp:141] Setting up pool1
I0421 23:13:35.154263 32711 net.cpp:148] Top shape: 1000 20 12 12 (2880000)
I0421 23:13:35.154266 32711 net.cpp:156] Memory required for data: 60748000
I0421 23:13:35.154269 32711 layer_factory.hpp:77] Creating layer conv2
I0421 23:13:35.154314 32711 net.cpp:91] Creating Layer conv2
I0421 23:13:35.154320 32711 net.cpp:425] conv2 <- pool1
I0421 23:13:35.154343 32711 net.cpp:399] conv2 -> conv2
I0421 23:13:35.155468 32711 net.cpp:141] Setting up conv2
I0421 23:13:35.155480 32711 net.cpp:148] Top shape: 1000 50 8 8 (3200000)
I0421 23:13:35.155484 32711 net.cpp:156] Memory required for data: 73548000
I0421 23:13:35.155511 32711 layer_factory.hpp:77] Creating layer pool2
I0421 23:13:35.155531 32711 net.cpp:91] Creating Layer pool2
I0421 23:13:35.155535 32711 net.cpp:425] pool2 <- conv2
I0421 23:13:35.155541 32711 net.cpp:399] pool2 -> pool2
I0421 23:13:35.155593 32711 net.cpp:141] Setting up pool2
I0421 23:13:35.155614 32711 net.cpp:148] Top shape: 1000 50 4 4 (800000)
I0421 23:13:35.155637 32711 net.cpp:156] Memory required for data: 76748000
I0421 23:13:35.155642 32711 layer_factory.hpp:77] Creating layer ip1
I0421 23:13:35.155663 32711 net.cpp:91] Creating Layer ip1
I0421 23:13:35.155665 32711 net.cpp:425] ip1 <- pool2
I0421 23:13:35.155673 32711 net.cpp:399] ip1 -> ip1
I0421 23:13:35.158897 32711 net.cpp:141] Setting up ip1
I0421 23:13:35.158926 32711 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:13:35.158928 32711 net.cpp:156] Memory required for data: 78748000
I0421 23:13:35.158956 32711 layer_factory.hpp:77] Creating layer relu1
I0421 23:13:35.158962 32711 net.cpp:91] Creating Layer relu1
I0421 23:13:35.158980 32711 net.cpp:425] relu1 <- ip1
I0421 23:13:35.158987 32711 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:13:35.159255 32711 net.cpp:141] Setting up relu1
I0421 23:13:35.159265 32711 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:13:35.159270 32711 net.cpp:156] Memory required for data: 80748000
I0421 23:13:35.159291 32711 layer_factory.hpp:77] Creating layer ip2
I0421 23:13:35.159299 32711 net.cpp:91] Creating Layer ip2
I0421 23:13:35.159303 32711 net.cpp:425] ip2 <- ip1
I0421 23:13:35.159310 32711 net.cpp:399] ip2 -> ip2
I0421 23:13:35.160190 32711 net.cpp:141] Setting up ip2
I0421 23:13:35.160202 32711 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:13:35.160204 32711 net.cpp:156] Memory required for data: 81152000
I0421 23:13:35.160228 32711 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0421 23:13:35.160235 32711 net.cpp:91] Creating Layer ip2_ip2_0_split
I0421 23:13:35.160239 32711 net.cpp:425] ip2_ip2_0_split <- ip2
I0421 23:13:35.160245 32711 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0421 23:13:35.160251 32711 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0421 23:13:35.160300 32711 net.cpp:141] Setting up ip2_ip2_0_split
I0421 23:13:35.160305 32711 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:13:35.160326 32711 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:13:35.160329 32711 net.cpp:156] Memory required for data: 81960000
I0421 23:13:35.160332 32711 layer_factory.hpp:77] Creating layer accuracy
I0421 23:13:35.160358 32711 net.cpp:91] Creating Layer accuracy
I0421 23:13:35.160362 32711 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0421 23:13:35.160367 32711 net.cpp:425] accuracy <- label_mnist_1_split_0
I0421 23:13:35.160372 32711 net.cpp:399] accuracy -> accuracy
I0421 23:13:35.160392 32711 net.cpp:141] Setting up accuracy
I0421 23:13:35.160398 32711 net.cpp:148] Top shape: (1)
I0421 23:13:35.160414 32711 net.cpp:156] Memory required for data: 81960004
I0421 23:13:35.160429 32711 layer_factory.hpp:77] Creating layer loss
I0421 23:13:35.160435 32711 net.cpp:91] Creating Layer loss
I0421 23:13:35.160439 32711 net.cpp:425] loss <- ip2_ip2_0_split_1
I0421 23:13:35.160457 32711 net.cpp:425] loss <- label_mnist_1_split_1
I0421 23:13:35.160462 32711 net.cpp:399] loss -> loss
I0421 23:13:35.160470 32711 layer_factory.hpp:77] Creating layer loss
I0421 23:13:35.161218 32711 net.cpp:141] Setting up loss
I0421 23:13:35.161232 32711 net.cpp:148] Top shape: (1)
I0421 23:13:35.161237 32711 net.cpp:151]     with loss weight 1
I0421 23:13:35.161262 32711 net.cpp:156] Memory required for data: 81960008
I0421 23:13:35.161280 32711 net.cpp:217] loss needs backward computation.
I0421 23:13:35.161284 32711 net.cpp:219] accuracy does not need backward computation.
I0421 23:13:35.161303 32711 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0421 23:13:35.161306 32711 net.cpp:217] ip2 needs backward computation.
I0421 23:13:35.161309 32711 net.cpp:217] relu1 needs backward computation.
I0421 23:13:35.161314 32711 net.cpp:217] ip1 needs backward computation.
I0421 23:13:35.161317 32711 net.cpp:217] pool2 needs backward computation.
I0421 23:13:35.161320 32711 net.cpp:217] conv2 needs backward computation.
I0421 23:13:35.161324 32711 net.cpp:217] pool1 needs backward computation.
I0421 23:13:35.161329 32711 net.cpp:217] conv1 needs backward computation.
I0421 23:13:35.161334 32711 net.cpp:219] label_mnist_1_split does not need backward computation.
I0421 23:13:35.161337 32711 net.cpp:219] mnist does not need backward computation.
I0421 23:13:35.161340 32711 net.cpp:261] This network produces output accuracy
I0421 23:13:35.161345 32711 net.cpp:261] This network produces output loss
I0421 23:13:35.161357 32711 net.cpp:274] Network initialization done.
I0421 23:13:35.161415 32711 solver.cpp:60] Solver scaffolding done.
I0421 23:13:35.161718 32711 caffe.cpp:219] Starting Optimization
I0421 23:13:35.161726 32711 solver.cpp:279] Solving LeNet
I0421 23:13:35.161730 32711 solver.cpp:280] Learning Rate Policy: inv
I0421 23:13:35.162148 32711 solver.cpp:337] Iteration 0, Testing net (#0)
I0421 23:13:35.162879 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:13:37.250027 32711 solver.cpp:404]     Test net output #0: accuracy = 0.00693001
I0421 23:13:37.250077 32711 solver.cpp:404]     Test net output #1: loss = 4.73403 (* 1 = 4.73403 loss)
I0421 23:13:37.252300 32711 solver.cpp:228] Iteration 0, loss = 4.74566
I0421 23:13:37.252317 32711 solver.cpp:244]     Train net output #0: loss = 4.74566 (* 1 = 4.74566 loss)
I0421 23:13:37.252334 32711 sgd_solver.cpp:106] Iteration 0, lr = 0.00023
I0421 23:13:38.777694 32711 solver.cpp:337] Iteration 500, Testing net (#0)
I0421 23:13:40.818011 32711 solver.cpp:404]     Test net output #0: accuracy = 0.0198
I0421 23:13:40.818048 32711 solver.cpp:404]     Test net output #1: loss = 4.9897 (* 1 = 4.9897 loss)
I0421 23:13:42.381001 32711 solver.cpp:337] Iteration 1000, Testing net (#0)
I0421 23:13:44.431413 32711 solver.cpp:404]     Test net output #0: accuracy = 0.0702901
I0421 23:13:44.431449 32711 solver.cpp:404]     Test net output #1: loss = 4.43967 (* 1 = 4.43967 loss)
I0421 23:13:44.432381 32711 solver.cpp:228] Iteration 1000, loss = 5.2441
I0421 23:13:44.432412 32711 solver.cpp:244]     Train net output #0: loss = 5.2441 (* 1 = 5.2441 loss)
I0421 23:13:44.432435 32711 sgd_solver.cpp:106] Iteration 1000, lr = 0.000214133
I0421 23:13:46.025326 32711 solver.cpp:337] Iteration 1500, Testing net (#0)
I0421 23:13:48.002825 32711 solver.cpp:404]     Test net output #0: accuracy = 0.04257
I0421 23:13:48.002861 32711 solver.cpp:404]     Test net output #1: loss = 4.39918 (* 1 = 4.39918 loss)
I0421 23:13:49.514961 32711 solver.cpp:337] Iteration 2000, Testing net (#0)
I0421 23:13:51.523835 32711 solver.cpp:404]     Test net output #0: accuracy = 0.0396
I0421 23:13:51.523898 32711 solver.cpp:404]     Test net output #1: loss = 5.49416 (* 1 = 5.49416 loss)
I0421 23:13:51.525049 32711 solver.cpp:228] Iteration 2000, loss = 3.58972
I0421 23:13:51.525071 32711 solver.cpp:244]     Train net output #0: loss = 3.58972 (* 1 = 3.58972 loss)
I0421 23:13:51.525100 32711 sgd_solver.cpp:106] Iteration 2000, lr = 0.000200605
I0421 23:13:53.022300 32711 solver.cpp:337] Iteration 2500, Testing net (#0)
I0421 23:13:55.080987 32711 solver.cpp:404]     Test net output #0: accuracy = 0.06732
I0421 23:13:55.081055 32711 solver.cpp:404]     Test net output #1: loss = 4.1509 (* 1 = 4.1509 loss)
I0421 23:13:56.637467 32711 solver.cpp:337] Iteration 3000, Testing net (#0)
I0421 23:13:58.702090 32711 solver.cpp:404]     Test net output #0: accuracy = 0.0811799
I0421 23:13:58.702177 32711 solver.cpp:404]     Test net output #1: loss = 4.10016 (* 1 = 4.10016 loss)
I0421 23:13:58.703214 32711 solver.cpp:228] Iteration 3000, loss = 3.87404
I0421 23:13:58.703238 32711 solver.cpp:244]     Train net output #0: loss = 3.87404 (* 1 = 3.87404 loss)
I0421 23:13:58.703250 32711 sgd_solver.cpp:106] Iteration 3000, lr = 0.000188917
I0421 23:14:00.282376 32711 solver.cpp:337] Iteration 3500, Testing net (#0)
I0421 23:14:02.286694 32711 solver.cpp:404]     Test net output #0: accuracy = 0.09702
I0421 23:14:02.286738 32711 solver.cpp:404]     Test net output #1: loss = 4.50879 (* 1 = 4.50879 loss)
I0421 23:14:03.805637 32711 solver.cpp:337] Iteration 4000, Testing net (#0)
I0421 23:14:05.845806 32711 solver.cpp:404]     Test net output #0: accuracy = 0.07227
I0421 23:14:05.846096 32711 solver.cpp:404]     Test net output #1: loss = 4.72891 (* 1 = 4.72891 loss)
I0421 23:14:05.847054 32711 solver.cpp:228] Iteration 4000, loss = 1.23214
I0421 23:14:05.847069 32711 solver.cpp:244]     Train net output #0: loss = 1.23214 (* 1 = 1.23214 loss)
I0421 23:14:05.847077 32711 sgd_solver.cpp:106] Iteration 4000, lr = 0.000178703
I0421 23:14:07.343014 32711 solver.cpp:337] Iteration 4500, Testing net (#0)
I0421 23:14:09.417568 32711 solver.cpp:404]     Test net output #0: accuracy = 0.09702
I0421 23:14:09.417626 32711 solver.cpp:404]     Test net output #1: loss = 5.18055 (* 1 = 5.18055 loss)
I0421 23:14:10.969828 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.caffemodel
I0421 23:14:10.978543 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.solverstate
I0421 23:14:10.981937 32711 solver.cpp:337] Iteration 5000, Testing net (#0)
I0421 23:14:11.704582 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:14:12.993824 32711 solver.cpp:404]     Test net output #0: accuracy = 0.12283
I0421 23:14:12.993862 32711 solver.cpp:404]     Test net output #1: loss = 5.10118 (* 1 = 5.10118 loss)
I0421 23:14:12.994786 32711 solver.cpp:228] Iteration 5000, loss = 1.09507
I0421 23:14:12.994802 32711 solver.cpp:244]     Train net output #0: loss = 1.09507 (* 1 = 1.09507 loss)
I0421 23:14:12.994829 32711 sgd_solver.cpp:106] Iteration 5000, lr = 0.000169691
I0421 23:14:14.614311 32711 solver.cpp:337] Iteration 5500, Testing net (#0)
I0421 23:14:16.637171 32711 solver.cpp:404]     Test net output #0: accuracy = 0.08712
I0421 23:14:16.637209 32711 solver.cpp:404]     Test net output #1: loss = 5.649 (* 1 = 5.649 loss)
I0421 23:14:18.161391 32711 solver.cpp:337] Iteration 6000, Testing net (#0)
I0421 23:14:20.211524 32711 solver.cpp:404]     Test net output #0: accuracy = 0.11682
I0421 23:14:20.211563 32711 solver.cpp:404]     Test net output #1: loss = 5.41208 (* 1 = 5.41208 loss)
I0421 23:14:20.212505 32711 solver.cpp:228] Iteration 6000, loss = 0.992425
I0421 23:14:20.212538 32711 solver.cpp:244]     Train net output #0: loss = 0.992424 (* 1 = 0.992424 loss)
I0421 23:14:20.212560 32711 sgd_solver.cpp:106] Iteration 6000, lr = 0.000161673
I0421 23:14:21.848469 32711 solver.cpp:337] Iteration 6500, Testing net (#0)
I0421 23:14:23.924322 32711 solver.cpp:404]     Test net output #0: accuracy = 0.12679
I0421 23:14:23.924401 32711 solver.cpp:404]     Test net output #1: loss = 5.01973 (* 1 = 5.01973 loss)
I0421 23:14:25.658455 32711 solver.cpp:337] Iteration 7000, Testing net (#0)
I0421 23:14:27.762864 32711 solver.cpp:404]     Test net output #0: accuracy = 0.08415
I0421 23:14:27.762902 32711 solver.cpp:404]     Test net output #1: loss = 6.14284 (* 1 = 6.14284 loss)
I0421 23:14:27.763851 32711 solver.cpp:228] Iteration 7000, loss = 1.10882
I0421 23:14:27.763883 32711 solver.cpp:244]     Train net output #0: loss = 1.10882 (* 1 = 1.10882 loss)
I0421 23:14:27.763906 32711 sgd_solver.cpp:106] Iteration 7000, lr = 0.000154487
I0421 23:14:29.360427 32711 solver.cpp:337] Iteration 7500, Testing net (#0)
I0421 23:14:31.383291 32711 solver.cpp:404]     Test net output #0: accuracy = 0.1683
I0421 23:14:31.383328 32711 solver.cpp:404]     Test net output #1: loss = 4.81487 (* 1 = 4.81487 loss)
I0421 23:14:32.899158 32711 solver.cpp:337] Iteration 8000, Testing net (#0)
I0421 23:14:35.073745 32711 solver.cpp:404]     Test net output #0: accuracy = 0.15048
I0421 23:14:35.073803 32711 solver.cpp:404]     Test net output #1: loss = 5.27772 (* 1 = 5.27772 loss)
I0421 23:14:35.074800 32711 solver.cpp:228] Iteration 8000, loss = 0.0612096
I0421 23:14:35.074820 32711 solver.cpp:244]     Train net output #0: loss = 0.0612091 (* 1 = 0.0612091 loss)
I0421 23:14:35.074831 32711 sgd_solver.cpp:106] Iteration 8000, lr = 0.000148004
I0421 23:14:36.567507 32711 solver.cpp:337] Iteration 8500, Testing net (#0)
I0421 23:14:38.574746 32711 solver.cpp:404]     Test net output #0: accuracy = 0.10593
I0421 23:14:38.574813 32711 solver.cpp:404]     Test net output #1: loss = 6.77038 (* 1 = 6.77038 loss)
I0421 23:14:40.132664 32711 solver.cpp:337] Iteration 9000, Testing net (#0)
I0421 23:14:42.133879 32711 solver.cpp:404]     Test net output #0: accuracy = 0.20493
I0421 23:14:42.133946 32711 solver.cpp:404]     Test net output #1: loss = 4.36183 (* 1 = 4.36183 loss)
I0421 23:14:42.134867 32711 solver.cpp:228] Iteration 9000, loss = 0.61599
I0421 23:14:42.134901 32711 solver.cpp:244]     Train net output #0: loss = 0.61599 (* 1 = 0.61599 loss)
I0421 23:14:42.134912 32711 sgd_solver.cpp:106] Iteration 9000, lr = 0.000142122
I0421 23:14:43.718925 32711 solver.cpp:337] Iteration 9500, Testing net (#0)
I0421 23:14:45.758641 32711 solver.cpp:404]     Test net output #0: accuracy = 0.12474
I0421 23:14:45.758692 32711 solver.cpp:404]     Test net output #1: loss = 5.78809 (* 1 = 5.78809 loss)
I0421 23:14:47.286075 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.caffemodel
I0421 23:14:47.292707 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.solverstate
I0421 23:14:47.295287 32711 solver.cpp:337] Iteration 10000, Testing net (#0)
I0421 23:14:48.817030 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:14:49.290356 32711 solver.cpp:404]     Test net output #0: accuracy = 0.1495
I0421 23:14:49.290421 32711 solver.cpp:404]     Test net output #1: loss = 4.89628 (* 1 = 4.89628 loss)
I0421 23:14:49.291538 32711 solver.cpp:228] Iteration 10000, loss = 0.167381
I0421 23:14:49.291590 32711 solver.cpp:244]     Train net output #0: loss = 0.167381 (* 1 = 0.167381 loss)
I0421 23:14:49.291610 32711 sgd_solver.cpp:106] Iteration 10000, lr = 0.000136759
I0421 23:14:50.780712 32711 solver.cpp:337] Iteration 10500, Testing net (#0)
I0421 23:14:52.990893 32711 solver.cpp:404]     Test net output #0: accuracy = 0.18612
I0421 23:14:52.990952 32711 solver.cpp:404]     Test net output #1: loss = 4.40248 (* 1 = 4.40248 loss)
I0421 23:14:54.545786 32711 solver.cpp:337] Iteration 11000, Testing net (#0)
I0421 23:14:56.532691 32711 solver.cpp:404]     Test net output #0: accuracy = 0.18219
I0421 23:14:56.532763 32711 solver.cpp:404]     Test net output #1: loss = 4.57295 (* 1 = 4.57295 loss)
I0421 23:14:56.533670 32711 solver.cpp:228] Iteration 11000, loss = 1.75659
I0421 23:14:56.533707 32711 solver.cpp:244]     Train net output #0: loss = 1.75659 (* 1 = 1.75659 loss)
I0421 23:14:56.533720 32711 sgd_solver.cpp:106] Iteration 11000, lr = 0.000131845
I0421 23:14:58.109554 32711 solver.cpp:337] Iteration 11500, Testing net (#0)
I0421 23:15:00.160356 32711 solver.cpp:404]     Test net output #0: accuracy = 0.22078
I0421 23:15:00.160393 32711 solver.cpp:404]     Test net output #1: loss = 4.81712 (* 1 = 4.81712 loss)
I0421 23:15:01.689108 32711 solver.cpp:337] Iteration 12000, Testing net (#0)
I0421 23:15:03.680204 32711 solver.cpp:404]     Test net output #0: accuracy = 0.18909
I0421 23:15:03.680312 32711 solver.cpp:404]     Test net output #1: loss = 4.84969 (* 1 = 4.84969 loss)
I0421 23:15:03.681304 32711 solver.cpp:228] Iteration 12000, loss = 0.900906
I0421 23:15:03.681325 32711 solver.cpp:244]     Train net output #0: loss = 0.900905 (* 1 = 0.900905 loss)
I0421 23:15:03.681365 32711 sgd_solver.cpp:106] Iteration 12000, lr = 0.000127324
I0421 23:15:05.162086 32711 solver.cpp:337] Iteration 12500, Testing net (#0)
I0421 23:15:07.098080 32711 solver.cpp:404]     Test net output #0: accuracy = 0.19998
I0421 23:15:07.098280 32711 solver.cpp:404]     Test net output #1: loss = 4.26926 (* 1 = 4.26926 loss)
I0421 23:15:08.649418 32711 solver.cpp:337] Iteration 13000, Testing net (#0)
I0421 23:15:10.812799 32711 solver.cpp:404]     Test net output #0: accuracy = 0.22473
I0421 23:15:10.812839 32711 solver.cpp:404]     Test net output #1: loss = 4.49164 (* 1 = 4.49164 loss)
I0421 23:15:10.813860 32711 solver.cpp:228] Iteration 13000, loss = 2.4324
I0421 23:15:10.813890 32711 solver.cpp:244]     Train net output #0: loss = 2.4324 (* 1 = 2.4324 loss)
I0421 23:15:10.813899 32711 sgd_solver.cpp:106] Iteration 13000, lr = 0.000123149
I0421 23:15:12.402999 32711 solver.cpp:337] Iteration 13500, Testing net (#0)
I0421 23:15:14.562664 32711 solver.cpp:404]     Test net output #0: accuracy = 0.19305
I0421 23:15:14.562717 32711 solver.cpp:404]     Test net output #1: loss = 5.38101 (* 1 = 5.38101 loss)
I0421 23:15:16.093168 32711 solver.cpp:337] Iteration 14000, Testing net (#0)
I0421 23:15:18.110369 32711 solver.cpp:404]     Test net output #0: accuracy = 0.22671
I0421 23:15:18.110406 32711 solver.cpp:404]     Test net output #1: loss = 3.95658 (* 1 = 3.95658 loss)
I0421 23:15:18.111505 32711 solver.cpp:228] Iteration 14000, loss = 0.286953
I0421 23:15:18.111521 32711 solver.cpp:244]     Train net output #0: loss = 0.286953 (* 1 = 0.286953 loss)
I0421 23:15:18.111531 32711 sgd_solver.cpp:106] Iteration 14000, lr = 0.00011928
I0421 23:15:19.597201 32711 solver.cpp:337] Iteration 14500, Testing net (#0)
I0421 23:15:21.600090 32711 solver.cpp:404]     Test net output #0: accuracy = 0.26929
I0421 23:15:21.600147 32711 solver.cpp:404]     Test net output #1: loss = 3.66596 (* 1 = 3.66596 loss)
I0421 23:15:23.168103 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.caffemodel
I0421 23:15:23.174670 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.solverstate
I0421 23:15:23.177583 32711 solver.cpp:337] Iteration 15000, Testing net (#0)
I0421 23:15:25.226577 32711 solver.cpp:404]     Test net output #0: accuracy = 0.19305
I0421 23:15:25.226649 32711 solver.cpp:404]     Test net output #1: loss = 5.62184 (* 1 = 5.62184 loss)
I0421 23:15:25.227622 32711 solver.cpp:228] Iteration 15000, loss = 0.33626
I0421 23:15:25.227651 32711 solver.cpp:244]     Train net output #0: loss = 0.33626 (* 1 = 0.33626 loss)
I0421 23:15:25.227663 32711 sgd_solver.cpp:106] Iteration 15000, lr = 0.000115684
I0421 23:15:26.797520 32711 solver.cpp:337] Iteration 15500, Testing net (#0)
I0421 23:15:27.125180 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:15:28.801412 32711 solver.cpp:404]     Test net output #0: accuracy = 0.22375
I0421 23:15:28.801455 32711 solver.cpp:404]     Test net output #1: loss = 4.4191 (* 1 = 4.4191 loss)
I0421 23:15:30.404888 32711 solver.cpp:337] Iteration 16000, Testing net (#0)
I0421 23:15:32.345345 32711 solver.cpp:404]     Test net output #0: accuracy = 0.25744
I0421 23:15:32.345417 32711 solver.cpp:404]     Test net output #1: loss = 3.82398 (* 1 = 3.82398 loss)
I0421 23:15:32.346652 32711 solver.cpp:228] Iteration 16000, loss = 0.00346039
I0421 23:15:32.346701 32711 solver.cpp:244]     Train net output #0: loss = 0.00346015 (* 1 = 0.00346015 loss)
I0421 23:15:32.346712 32711 sgd_solver.cpp:106] Iteration 16000, lr = 0.000112331
I0421 23:15:33.985085 32711 solver.cpp:337] Iteration 16500, Testing net (#0)
I0421 23:15:35.978068 32711 solver.cpp:404]     Test net output #0: accuracy = 0.19703
I0421 23:15:35.978121 32711 solver.cpp:404]     Test net output #1: loss = 5.05133 (* 1 = 5.05133 loss)
I0421 23:15:37.575151 32711 solver.cpp:337] Iteration 17000, Testing net (#0)
I0421 23:15:39.633101 32711 solver.cpp:404]     Test net output #0: accuracy = 0.24948
I0421 23:15:39.633155 32711 solver.cpp:404]     Test net output #1: loss = 4.54628 (* 1 = 4.54628 loss)
I0421 23:15:39.634219 32711 solver.cpp:228] Iteration 17000, loss = 0.0879863
I0421 23:15:39.634238 32711 solver.cpp:244]     Train net output #0: loss = 0.0879864 (* 1 = 0.0879864 loss)
I0421 23:15:39.634248 32711 sgd_solver.cpp:106] Iteration 17000, lr = 0.000109196
I0421 23:15:41.274550 32711 solver.cpp:337] Iteration 17500, Testing net (#0)
I0421 23:15:43.281731 32711 solver.cpp:404]     Test net output #0: accuracy = 0.24552
I0421 23:15:43.281788 32711 solver.cpp:404]     Test net output #1: loss = 4.00252 (* 1 = 4.00252 loss)
I0421 23:15:44.873404 32711 solver.cpp:337] Iteration 18000, Testing net (#0)
I0421 23:15:46.860218 32711 solver.cpp:404]     Test net output #0: accuracy = 0.20205
I0421 23:15:46.860263 32711 solver.cpp:404]     Test net output #1: loss = 5.47053 (* 1 = 5.47053 loss)
I0421 23:15:46.861256 32711 solver.cpp:228] Iteration 18000, loss = 0.666855
I0421 23:15:46.861299 32711 solver.cpp:244]     Train net output #0: loss = 0.666855 (* 1 = 0.666855 loss)
I0421 23:15:46.861307 32711 sgd_solver.cpp:106] Iteration 18000, lr = 0.000106257
I0421 23:15:48.345218 32711 solver.cpp:337] Iteration 18500, Testing net (#0)
I0421 23:15:50.379396 32711 solver.cpp:404]     Test net output #0: accuracy = 0.26437
I0421 23:15:50.379452 32711 solver.cpp:404]     Test net output #1: loss = 4.32914 (* 1 = 4.32914 loss)
I0421 23:15:51.952422 32711 solver.cpp:337] Iteration 19000, Testing net (#0)
I0421 23:15:53.991086 32711 solver.cpp:404]     Test net output #0: accuracy = 0.23958
I0421 23:15:53.991158 32711 solver.cpp:404]     Test net output #1: loss = 3.97291 (* 1 = 3.97291 loss)
I0421 23:15:53.992133 32711 solver.cpp:228] Iteration 19000, loss = 0.392836
I0421 23:15:53.992166 32711 solver.cpp:244]     Train net output #0: loss = 0.392836 (* 1 = 0.392836 loss)
I0421 23:15:53.992175 32711 sgd_solver.cpp:106] Iteration 19000, lr = 0.000103497
I0421 23:15:55.566206 32711 solver.cpp:337] Iteration 19500, Testing net (#0)
I0421 23:15:57.601475 32711 solver.cpp:404]     Test net output #0: accuracy = 0.23961
I0421 23:15:57.601514 32711 solver.cpp:404]     Test net output #1: loss = 4.39308 (* 1 = 4.39308 loss)
I0421 23:15:59.120255 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.caffemodel
I0421 23:15:59.127082 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.solverstate
I0421 23:15:59.129745 32711 solver.cpp:337] Iteration 20000, Testing net (#0)
I0421 23:16:01.093543 32711 solver.cpp:404]     Test net output #0: accuracy = 0.2475
I0421 23:16:01.093582 32711 solver.cpp:404]     Test net output #1: loss = 4.27326 (* 1 = 4.27326 loss)
I0421 23:16:01.094609 32711 solver.cpp:228] Iteration 20000, loss = 0.330178
I0421 23:16:01.094629 32711 solver.cpp:244]     Train net output #0: loss = 0.330178 (* 1 = 0.330178 loss)
I0421 23:16:01.094638 32711 sgd_solver.cpp:106] Iteration 20000, lr = 0.000100899
I0421 23:16:02.577003 32711 solver.cpp:337] Iteration 20500, Testing net (#0)
I0421 23:16:03.694864 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:16:04.583750 32711 solver.cpp:404]     Test net output #0: accuracy = 0.35742
I0421 23:16:04.583801 32711 solver.cpp:404]     Test net output #1: loss = 3.03376 (* 1 = 3.03376 loss)
I0421 23:16:06.127826 32711 solver.cpp:337] Iteration 21000, Testing net (#0)
I0421 23:16:08.135915 32711 solver.cpp:404]     Test net output #0: accuracy = 0.28809
I0421 23:16:08.136157 32711 solver.cpp:404]     Test net output #1: loss = 3.62801 (* 1 = 3.62801 loss)
I0421 23:16:08.137135 32711 solver.cpp:228] Iteration 21000, loss = 1.05942
I0421 23:16:08.137153 32711 solver.cpp:244]     Train net output #0: loss = 1.05942 (* 1 = 1.05942 loss)
I0421 23:16:08.137166 32711 sgd_solver.cpp:106] Iteration 21000, lr = 9.84479e-05
I0421 23:16:09.733994 32711 solver.cpp:337] Iteration 21500, Testing net (#0)
I0421 23:16:11.743269 32711 solver.cpp:404]     Test net output #0: accuracy = 0.26438
I0421 23:16:11.743342 32711 solver.cpp:404]     Test net output #1: loss = 4.48344 (* 1 = 4.48344 loss)
I0421 23:16:13.256700 32711 solver.cpp:337] Iteration 22000, Testing net (#0)
I0421 23:16:15.283926 32711 solver.cpp:404]     Test net output #0: accuracy = 0.34155
I0421 23:16:15.283983 32711 solver.cpp:404]     Test net output #1: loss = 3.07628 (* 1 = 3.07628 loss)
I0421 23:16:15.285120 32711 solver.cpp:228] Iteration 22000, loss = 0.548251
I0421 23:16:15.285152 32711 solver.cpp:244]     Train net output #0: loss = 0.548251 (* 1 = 0.548251 loss)
I0421 23:16:15.285161 32711 sgd_solver.cpp:106] Iteration 22000, lr = 9.61314e-05
I0421 23:16:16.788960 32711 solver.cpp:337] Iteration 22500, Testing net (#0)
I0421 23:16:18.796794 32711 solver.cpp:404]     Test net output #0: accuracy = 0.27621
I0421 23:16:18.796867 32711 solver.cpp:404]     Test net output #1: loss = 4.17169 (* 1 = 4.17169 loss)
I0421 23:16:20.346912 32711 solver.cpp:337] Iteration 23000, Testing net (#0)
I0421 23:16:22.369473 32711 solver.cpp:404]     Test net output #0: accuracy = 0.27723
I0421 23:16:22.369516 32711 solver.cpp:404]     Test net output #1: loss = 4.56916 (* 1 = 4.56916 loss)
I0421 23:16:22.370640 32711 solver.cpp:228] Iteration 23000, loss = 1.03121
I0421 23:16:22.370664 32711 solver.cpp:244]     Train net output #0: loss = 1.03121 (* 1 = 1.03121 loss)
I0421 23:16:22.370692 32711 sgd_solver.cpp:106] Iteration 23000, lr = 9.39382e-05
I0421 23:16:23.951020 32711 solver.cpp:337] Iteration 23500, Testing net (#0)
I0421 23:16:25.965762 32711 solver.cpp:404]     Test net output #0: accuracy = 0.297
I0421 23:16:25.965826 32711 solver.cpp:404]     Test net output #1: loss = 3.96445 (* 1 = 3.96445 loss)
I0421 23:16:27.479951 32711 solver.cpp:337] Iteration 24000, Testing net (#0)
I0421 23:16:29.510489 32711 solver.cpp:404]     Test net output #0: accuracy = 0.3287
I0421 23:16:29.510578 32711 solver.cpp:404]     Test net output #1: loss = 3.52656 (* 1 = 3.52656 loss)
I0421 23:16:29.511528 32711 solver.cpp:228] Iteration 24000, loss = 0.581759
I0421 23:16:29.511559 32711 solver.cpp:244]     Train net output #0: loss = 0.581759 (* 1 = 0.581759 loss)
I0421 23:16:29.511569 32711 sgd_solver.cpp:106] Iteration 24000, lr = 9.18584e-05
I0421 23:16:30.998086 32711 solver.cpp:337] Iteration 24500, Testing net (#0)
I0421 23:16:32.990449 32711 solver.cpp:404]     Test net output #0: accuracy = 0.36838
I0421 23:16:32.990505 32711 solver.cpp:404]     Test net output #1: loss = 3.20636 (* 1 = 3.20636 loss)
I0421 23:16:34.540170 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_25000.caffemodel
I0421 23:16:34.546855 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_25000.solverstate
I0421 23:16:34.549494 32711 solver.cpp:337] Iteration 25000, Testing net (#0)
I0421 23:16:36.553854 32711 solver.cpp:404]     Test net output #0: accuracy = 0.3178
I0421 23:16:36.553892 32711 solver.cpp:404]     Test net output #1: loss = 3.92293 (* 1 = 3.92293 loss)
I0421 23:16:36.554870 32711 solver.cpp:228] Iteration 25000, loss = 0.202091
I0421 23:16:36.554896 32711 solver.cpp:244]     Train net output #0: loss = 0.202091 (* 1 = 0.202091 loss)
I0421 23:16:36.554904 32711 sgd_solver.cpp:106] Iteration 25000, lr = 8.98828e-05
I0421 23:16:38.147397 32711 solver.cpp:337] Iteration 25500, Testing net (#0)
I0421 23:16:40.138757 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:16:40.203860 32711 solver.cpp:404]     Test net output #0: accuracy = 0.42868
I0421 23:16:40.203897 32711 solver.cpp:404]     Test net output #1: loss = 2.77528 (* 1 = 2.77528 loss)
I0421 23:16:41.714440 32711 solver.cpp:337] Iteration 26000, Testing net (#0)
I0421 23:16:43.766814 32711 solver.cpp:404]     Test net output #0: accuracy = 0.25349
I0421 23:16:43.766886 32711 solver.cpp:404]     Test net output #1: loss = 4.91329 (* 1 = 4.91329 loss)
I0421 23:16:43.767818 32711 solver.cpp:228] Iteration 26000, loss = 0.625399
I0421 23:16:43.767841 32711 solver.cpp:244]     Train net output #0: loss = 0.6254 (* 1 = 0.6254 loss)
I0421 23:16:43.767856 32711 sgd_solver.cpp:106] Iteration 26000, lr = 8.80037e-05
I0421 23:16:45.281541 32711 solver.cpp:337] Iteration 26500, Testing net (#0)
I0421 23:16:47.275868 32711 solver.cpp:404]     Test net output #0: accuracy = 0.34947
I0421 23:16:47.275943 32711 solver.cpp:404]     Test net output #1: loss = 3.52724 (* 1 = 3.52724 loss)
I0421 23:16:48.827467 32711 solver.cpp:337] Iteration 27000, Testing net (#0)
I0421 23:16:50.941287 32711 solver.cpp:404]     Test net output #0: accuracy = 0.38515
I0421 23:16:50.941328 32711 solver.cpp:404]     Test net output #1: loss = 3.23654 (* 1 = 3.23654 loss)
I0421 23:16:50.942481 32711 solver.cpp:228] Iteration 27000, loss = 0.129029
I0421 23:16:50.942530 32711 solver.cpp:244]     Train net output #0: loss = 0.12903 (* 1 = 0.12903 loss)
I0421 23:16:50.942564 32711 sgd_solver.cpp:106] Iteration 27000, lr = 8.62138e-05
I0421 23:16:52.523520 32711 solver.cpp:337] Iteration 27500, Testing net (#0)
I0421 23:16:54.568621 32711 solver.cpp:404]     Test net output #0: accuracy = 0.28611
I0421 23:16:54.568677 32711 solver.cpp:404]     Test net output #1: loss = 4.82073 (* 1 = 4.82073 loss)
I0421 23:16:56.085057 32711 solver.cpp:337] Iteration 28000, Testing net (#0)
I0421 23:16:58.051198 32711 solver.cpp:404]     Test net output #0: accuracy = 0.33165
I0421 23:16:58.051261 32711 solver.cpp:404]     Test net output #1: loss = 3.70777 (* 1 = 3.70777 loss)
I0421 23:16:58.052172 32711 solver.cpp:228] Iteration 28000, loss = 0.352259
I0421 23:16:58.052191 32711 solver.cpp:244]     Train net output #0: loss = 0.352259 (* 1 = 0.352259 loss)
I0421 23:16:58.052199 32711 sgd_solver.cpp:106] Iteration 28000, lr = 8.45065e-05
I0421 23:16:59.541833 32711 solver.cpp:337] Iteration 28500, Testing net (#0)
I0421 23:17:01.507151 32711 solver.cpp:404]     Test net output #0: accuracy = 0.37424
I0421 23:17:01.507190 32711 solver.cpp:404]     Test net output #1: loss = 3.36591 (* 1 = 3.36591 loss)
I0421 23:17:03.060623 32711 solver.cpp:337] Iteration 29000, Testing net (#0)
I0421 23:17:05.078578 32711 solver.cpp:404]     Test net output #0: accuracy = 0.33564
I0421 23:17:05.078635 32711 solver.cpp:404]     Test net output #1: loss = 3.75925 (* 1 = 3.75925 loss)
I0421 23:17:05.079736 32711 solver.cpp:228] Iteration 29000, loss = 0.808164
I0421 23:17:05.079756 32711 solver.cpp:244]     Train net output #0: loss = 0.808165 (* 1 = 0.808165 loss)
I0421 23:17:05.079768 32711 sgd_solver.cpp:106] Iteration 29000, lr = 8.28761e-05
I0421 23:17:06.657975 32711 solver.cpp:337] Iteration 29500, Testing net (#0)
I0421 23:17:08.666079 32711 solver.cpp:404]     Test net output #0: accuracy = 0.33571
I0421 23:17:08.666149 32711 solver.cpp:404]     Test net output #1: loss = 3.64695 (* 1 = 3.64695 loss)
I0421 23:17:10.180723 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30000.caffemodel
I0421 23:17:10.187369 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30000.solverstate
I0421 23:17:10.189946 32711 solver.cpp:337] Iteration 30000, Testing net (#0)
I0421 23:17:12.226353 32711 solver.cpp:404]     Test net output #0: accuracy = 0.41192
I0421 23:17:12.226452 32711 solver.cpp:404]     Test net output #1: loss = 2.96828 (* 1 = 2.96828 loss)
I0421 23:17:12.227361 32711 solver.cpp:228] Iteration 30000, loss = 0.263023
I0421 23:17:12.227411 32711 solver.cpp:244]     Train net output #0: loss = 0.263024 (* 1 = 0.263024 loss)
I0421 23:17:12.227439 32711 sgd_solver.cpp:106] Iteration 30000, lr = 8.13173e-05
I0421 23:17:13.731513 32711 solver.cpp:337] Iteration 30500, Testing net (#0)
I0421 23:17:15.768038 32711 solver.cpp:404]     Test net output #0: accuracy = 0.38513
I0421 23:17:15.768096 32711 solver.cpp:404]     Test net output #1: loss = 2.9735 (* 1 = 2.9735 loss)
I0421 23:17:17.331250 32711 solver.cpp:337] Iteration 31000, Testing net (#0)
I0421 23:17:18.073429 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:17:19.388514 32711 solver.cpp:404]     Test net output #0: accuracy = 0.34462
I0421 23:17:19.388559 32711 solver.cpp:404]     Test net output #1: loss = 3.86696 (* 1 = 3.86696 loss)
I0421 23:17:19.389596 32711 solver.cpp:228] Iteration 31000, loss = 1.64606
I0421 23:17:19.389614 32711 solver.cpp:244]     Train net output #0: loss = 1.64606 (* 1 = 1.64606 loss)
I0421 23:17:19.389622 32711 sgd_solver.cpp:106] Iteration 31000, lr = 7.98252e-05
I0421 23:17:20.966657 32711 solver.cpp:337] Iteration 31500, Testing net (#0)
I0421 23:17:22.983335 32711 solver.cpp:404]     Test net output #0: accuracy = 0.45738
I0421 23:17:22.983392 32711 solver.cpp:404]     Test net output #1: loss = 2.51047 (* 1 = 2.51047 loss)
I0421 23:17:24.498898 32711 solver.cpp:337] Iteration 32000, Testing net (#0)
I0421 23:17:26.523089 32711 solver.cpp:404]     Test net output #0: accuracy = 0.4287
I0421 23:17:26.523145 32711 solver.cpp:404]     Test net output #1: loss = 2.80302 (* 1 = 2.80302 loss)
I0421 23:17:26.524178 32711 solver.cpp:228] Iteration 32000, loss = 0.709705
I0421 23:17:26.524212 32711 solver.cpp:244]     Train net output #0: loss = 0.709706 (* 1 = 0.709706 loss)
I0421 23:17:26.524240 32711 sgd_solver.cpp:106] Iteration 32000, lr = 7.83955e-05
I0421 23:17:28.017462 32711 solver.cpp:337] Iteration 32500, Testing net (#0)
I0421 23:17:30.059799 32711 solver.cpp:404]     Test net output #0: accuracy = 0.32877
I0421 23:17:30.059883 32711 solver.cpp:404]     Test net output #1: loss = 4.50256 (* 1 = 4.50256 loss)
I0421 23:17:31.621934 32711 solver.cpp:337] Iteration 33000, Testing net (#0)
I0421 23:17:33.581611 32711 solver.cpp:404]     Test net output #0: accuracy = 0.40888
I0421 23:17:33.581653 32711 solver.cpp:404]     Test net output #1: loss = 2.84624 (* 1 = 2.84624 loss)
I0421 23:17:33.582547 32711 solver.cpp:228] Iteration 33000, loss = 0.32348
I0421 23:17:33.582566 32711 solver.cpp:244]     Train net output #0: loss = 0.323481 (* 1 = 0.323481 loss)
I0421 23:17:33.582574 32711 sgd_solver.cpp:106] Iteration 33000, lr = 7.70241e-05
I0421 23:17:35.158550 32711 solver.cpp:337] Iteration 33500, Testing net (#0)
I0421 23:17:37.119637 32711 solver.cpp:404]     Test net output #0: accuracy = 0.4584
I0421 23:17:37.119700 32711 solver.cpp:404]     Test net output #1: loss = 2.68162 (* 1 = 2.68162 loss)
I0421 23:17:38.637928 32711 solver.cpp:337] Iteration 34000, Testing net (#0)
I0421 23:17:40.719275 32711 solver.cpp:404]     Test net output #0: accuracy = 0.3495
I0421 23:17:40.719462 32711 solver.cpp:404]     Test net output #1: loss = 4.33182 (* 1 = 4.33182 loss)
I0421 23:17:40.720504 32711 solver.cpp:228] Iteration 34000, loss = 0.287864
I0421 23:17:40.720527 32711 solver.cpp:244]     Train net output #0: loss = 0.287865 (* 1 = 0.287865 loss)
I0421 23:17:40.720556 32711 sgd_solver.cpp:106] Iteration 34000, lr = 7.57074e-05
I0421 23:17:42.224632 32711 solver.cpp:337] Iteration 34500, Testing net (#0)
I0421 23:17:44.244397 32711 solver.cpp:404]     Test net output #0: accuracy = 0.40402
I0421 23:17:44.244488 32711 solver.cpp:404]     Test net output #1: loss = 2.92065 (* 1 = 2.92065 loss)
I0421 23:17:45.826814 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_35000.caffemodel
I0421 23:17:45.833432 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_35000.solverstate
I0421 23:17:45.836158 32711 solver.cpp:337] Iteration 35000, Testing net (#0)
I0421 23:17:47.831837 32711 solver.cpp:404]     Test net output #0: accuracy = 0.48016
I0421 23:17:47.831904 32711 solver.cpp:404]     Test net output #1: loss = 2.55851 (* 1 = 2.55851 loss)
I0421 23:17:47.832849 32711 solver.cpp:228] Iteration 35000, loss = 1.80024
I0421 23:17:47.832865 32711 solver.cpp:244]     Train net output #0: loss = 1.80024 (* 1 = 1.80024 loss)
I0421 23:17:47.832891 32711 sgd_solver.cpp:106] Iteration 35000, lr = 7.44421e-05
I0421 23:17:49.422179 32711 solver.cpp:337] Iteration 35500, Testing net (#0)
I0421 23:17:51.412928 32711 solver.cpp:404]     Test net output #0: accuracy = 0.40594
I0421 23:17:51.412966 32711 solver.cpp:404]     Test net output #1: loss = 3.50226 (* 1 = 3.50226 loss)
I0421 23:17:52.936518 32711 solver.cpp:337] Iteration 36000, Testing net (#0)
I0421 23:17:54.472520 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:17:54.967547 32711 solver.cpp:404]     Test net output #0: accuracy = 0.38422
I0421 23:17:54.967605 32711 solver.cpp:404]     Test net output #1: loss = 3.15891 (* 1 = 3.15891 loss)
I0421 23:17:54.968804 32711 solver.cpp:228] Iteration 36000, loss = 0.132285
I0421 23:17:54.968823 32711 solver.cpp:244]     Train net output #0: loss = 0.132286 (* 1 = 0.132286 loss)
I0421 23:17:54.968833 32711 sgd_solver.cpp:106] Iteration 36000, lr = 7.3225e-05
I0421 23:17:56.455947 32711 solver.cpp:337] Iteration 36500, Testing net (#0)
I0421 23:17:58.439385 32711 solver.cpp:404]     Test net output #0: accuracy = 0.47817
I0421 23:17:58.439427 32711 solver.cpp:404]     Test net output #1: loss = 2.50329 (* 1 = 2.50329 loss)
I0421 23:17:59.982300 32711 solver.cpp:337] Iteration 37000, Testing net (#0)
I0421 23:18:01.963332 32711 solver.cpp:404]     Test net output #0: accuracy = 0.39608
I0421 23:18:01.963369 32711 solver.cpp:404]     Test net output #1: loss = 3.28809 (* 1 = 3.28809 loss)
I0421 23:18:01.964318 32711 solver.cpp:228] Iteration 37000, loss = 0.427082
I0421 23:18:01.964337 32711 solver.cpp:244]     Train net output #0: loss = 0.427083 (* 1 = 0.427083 loss)
I0421 23:18:01.964345 32711 sgd_solver.cpp:106] Iteration 37000, lr = 7.20534e-05
I0421 23:18:03.542479 32711 solver.cpp:337] Iteration 37500, Testing net (#0)
I0421 23:18:05.599056 32711 solver.cpp:404]     Test net output #0: accuracy = 0.36836
I0421 23:18:05.599135 32711 solver.cpp:404]     Test net output #1: loss = 3.56314 (* 1 = 3.56314 loss)
I0421 23:18:07.117054 32711 solver.cpp:337] Iteration 38000, Testing net (#0)
I0421 23:18:09.166012 32711 solver.cpp:404]     Test net output #0: accuracy = 0.49104
I0421 23:18:09.166054 32711 solver.cpp:404]     Test net output #1: loss = 2.42595 (* 1 = 2.42595 loss)
I0421 23:18:09.167085 32711 solver.cpp:228] Iteration 38000, loss = 0.0437922
I0421 23:18:09.167120 32711 solver.cpp:244]     Train net output #0: loss = 0.0437931 (* 1 = 0.0437931 loss)
I0421 23:18:09.167151 32711 sgd_solver.cpp:106] Iteration 38000, lr = 7.09246e-05
I0421 23:18:10.658140 32711 solver.cpp:337] Iteration 38500, Testing net (#0)
I0421 23:18:12.679368 32711 solver.cpp:404]     Test net output #0: accuracy = 0.39903
I0421 23:18:12.679491 32711 solver.cpp:404]     Test net output #1: loss = 3.27395 (* 1 = 3.27395 loss)
I0421 23:18:14.250640 32711 solver.cpp:337] Iteration 39000, Testing net (#0)
I0421 23:18:16.254595 32711 solver.cpp:404]     Test net output #0: accuracy = 0.35644
I0421 23:18:16.254654 32711 solver.cpp:404]     Test net output #1: loss = 4.16658 (* 1 = 4.16658 loss)
I0421 23:18:16.255646 32711 solver.cpp:228] Iteration 39000, loss = 0.192014
I0421 23:18:16.255666 32711 solver.cpp:244]     Train net output #0: loss = 0.192014 (* 1 = 0.192014 loss)
I0421 23:18:16.255703 32711 sgd_solver.cpp:106] Iteration 39000, lr = 6.98362e-05
I0421 23:18:17.843714 32711 solver.cpp:337] Iteration 39500, Testing net (#0)
I0421 23:18:19.902936 32711 solver.cpp:404]     Test net output #0: accuracy = 0.50693
I0421 23:18:19.902979 32711 solver.cpp:404]     Test net output #1: loss = 2.21216 (* 1 = 2.21216 loss)
I0421 23:18:21.425926 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_40000.caffemodel
I0421 23:18:21.432706 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_40000.solverstate
I0421 23:18:21.435340 32711 solver.cpp:337] Iteration 40000, Testing net (#0)
I0421 23:18:23.465214 32711 solver.cpp:404]     Test net output #0: accuracy = 0.43762
I0421 23:18:23.465278 32711 solver.cpp:404]     Test net output #1: loss = 2.92568 (* 1 = 2.92568 loss)
I0421 23:18:23.466420 32711 solver.cpp:228] Iteration 40000, loss = 0.691794
I0421 23:18:23.466451 32711 solver.cpp:244]     Train net output #0: loss = 0.691794 (* 1 = 0.691794 loss)
I0421 23:18:23.466469 32711 sgd_solver.cpp:106] Iteration 40000, lr = 6.8786e-05
I0421 23:18:24.967929 32711 solver.cpp:337] Iteration 40500, Testing net (#0)
I0421 23:18:27.002848 32711 solver.cpp:404]     Test net output #0: accuracy = 0.36041
I0421 23:18:27.002923 32711 solver.cpp:404]     Test net output #1: loss = 4.25599 (* 1 = 4.25599 loss)
I0421 23:18:28.558290 32711 solver.cpp:337] Iteration 41000, Testing net (#0)
I0421 23:18:30.590193 32711 solver.cpp:404]     Test net output #0: accuracy = 0.49508
I0421 23:18:30.590243 32711 solver.cpp:404]     Test net output #1: loss = 2.30516 (* 1 = 2.30516 loss)
I0421 23:18:30.591367 32711 solver.cpp:228] Iteration 41000, loss = 0.453168
I0421 23:18:30.591389 32711 solver.cpp:244]     Train net output #0: loss = 0.453168 (* 1 = 0.453168 loss)
I0421 23:18:30.591401 32711 sgd_solver.cpp:106] Iteration 41000, lr = 6.7772e-05
I0421 23:18:32.167234 32711 solver.cpp:337] Iteration 41500, Testing net (#0)
I0421 23:18:32.526840 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:18:34.186825 32711 solver.cpp:404]     Test net output #0: accuracy = 0.48213
I0421 23:18:34.186909 32711 solver.cpp:404]     Test net output #1: loss = 2.5519 (* 1 = 2.5519 loss)
I0421 23:18:35.710588 32711 solver.cpp:337] Iteration 42000, Testing net (#0)
I0421 23:18:37.724117 32711 solver.cpp:404]     Test net output #0: accuracy = 0.39007
I0421 23:18:37.724154 32711 solver.cpp:404]     Test net output #1: loss = 3.85896 (* 1 = 3.85896 loss)
I0421 23:18:37.725378 32711 solver.cpp:228] Iteration 42000, loss = 0.506614
I0421 23:18:37.725399 32711 solver.cpp:244]     Train net output #0: loss = 0.506614 (* 1 = 0.506614 loss)
I0421 23:18:37.725414 32711 sgd_solver.cpp:106] Iteration 42000, lr = 6.67921e-05
I0421 23:18:39.220721 32711 solver.cpp:337] Iteration 42500, Testing net (#0)
I0421 23:18:41.205253 32711 solver.cpp:404]     Test net output #0: accuracy = 0.48418
I0421 23:18:41.205319 32711 solver.cpp:404]     Test net output #1: loss = 2.41883 (* 1 = 2.41883 loss)
I0421 23:18:42.754297 32711 solver.cpp:337] Iteration 43000, Testing net (#0)
I0421 23:18:44.770364 32711 solver.cpp:404]     Test net output #0: accuracy = 0.55244
I0421 23:18:44.770406 32711 solver.cpp:404]     Test net output #1: loss = 2.04677 (* 1 = 2.04677 loss)
I0421 23:18:44.771517 32711 solver.cpp:228] Iteration 43000, loss = 0.758279
I0421 23:18:44.771539 32711 solver.cpp:244]     Train net output #0: loss = 0.758279 (* 1 = 0.758279 loss)
I0421 23:18:44.771553 32711 sgd_solver.cpp:106] Iteration 43000, lr = 6.58447e-05
I0421 23:18:46.367728 32711 solver.cpp:337] Iteration 43500, Testing net (#0)
I0421 23:18:48.367769 32711 solver.cpp:404]     Test net output #0: accuracy = 0.42174
I0421 23:18:48.367813 32711 solver.cpp:404]     Test net output #1: loss = 3.38753 (* 1 = 3.38753 loss)
I0421 23:18:49.884171 32711 solver.cpp:337] Iteration 44000, Testing net (#0)
I0421 23:18:51.882611 32711 solver.cpp:404]     Test net output #0: accuracy = 0.47035
I0421 23:18:51.882701 32711 solver.cpp:404]     Test net output #1: loss = 2.50503 (* 1 = 2.50503 loss)
I0421 23:18:51.883666 32711 solver.cpp:228] Iteration 44000, loss = 0.404229
I0421 23:18:51.883685 32711 solver.cpp:244]     Train net output #0: loss = 0.40423 (* 1 = 0.40423 loss)
I0421 23:18:51.883697 32711 sgd_solver.cpp:106] Iteration 44000, lr = 6.49281e-05
I0421 23:18:53.372416 32711 solver.cpp:337] Iteration 44500, Testing net (#0)
I0421 23:18:55.458247 32711 solver.cpp:404]     Test net output #0: accuracy = 0.56826
I0421 23:18:55.458305 32711 solver.cpp:404]     Test net output #1: loss = 1.94215 (* 1 = 1.94215 loss)
I0421 23:18:57.021070 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_45000.caffemodel
I0421 23:18:57.027871 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_45000.solverstate
I0421 23:18:57.030647 32711 solver.cpp:337] Iteration 45000, Testing net (#0)
I0421 23:18:59.120589 32711 solver.cpp:404]     Test net output #0: accuracy = 0.47621
I0421 23:18:59.120645 32711 solver.cpp:404]     Test net output #1: loss = 2.82119 (* 1 = 2.82119 loss)
I0421 23:18:59.121716 32711 solver.cpp:228] Iteration 45000, loss = 0.328513
I0421 23:18:59.121739 32711 solver.cpp:244]     Train net output #0: loss = 0.328514 (* 1 = 0.328514 loss)
I0421 23:18:59.121767 32711 sgd_solver.cpp:106] Iteration 45000, lr = 6.40407e-05
I0421 23:19:00.695890 32711 solver.cpp:337] Iteration 45500, Testing net (#0)
I0421 23:19:02.720513 32711 solver.cpp:404]     Test net output #0: accuracy = 0.44658
I0421 23:19:02.720569 32711 solver.cpp:404]     Test net output #1: loss = 2.84255 (* 1 = 2.84255 loss)
I0421 23:19:04.238245 32711 solver.cpp:337] Iteration 46000, Testing net (#0)
I0421 23:19:06.253217 32711 solver.cpp:404]     Test net output #0: accuracy = 0.57727
I0421 23:19:06.253300 32711 solver.cpp:404]     Test net output #1: loss = 1.82295 (* 1 = 1.82295 loss)
I0421 23:19:06.256098 32711 solver.cpp:228] Iteration 46000, loss = 0.0833927
I0421 23:19:06.256120 32711 solver.cpp:244]     Train net output #0: loss = 0.0833939 (* 1 = 0.0833939 loss)
I0421 23:19:06.256131 32711 sgd_solver.cpp:106] Iteration 46000, lr = 6.3181e-05
I0421 23:19:07.740569 32711 solver.cpp:337] Iteration 46500, Testing net (#0)
I0421 23:19:08.949229 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:19:09.780714 32711 solver.cpp:404]     Test net output #0: accuracy = 0.49896
I0421 23:19:09.780757 32711 solver.cpp:404]     Test net output #1: loss = 2.54764 (* 1 = 2.54764 loss)
I0421 23:19:11.335769 32711 solver.cpp:337] Iteration 47000, Testing net (#0)
I0421 23:19:13.328351 32711 solver.cpp:404]     Test net output #0: accuracy = 0.43073
I0421 23:19:13.328515 32711 solver.cpp:404]     Test net output #1: loss = 3.17637 (* 1 = 3.17637 loss)
I0421 23:19:13.329498 32711 solver.cpp:228] Iteration 47000, loss = 0.138818
I0421 23:19:13.329514 32711 solver.cpp:244]     Train net output #0: loss = 0.138819 (* 1 = 0.138819 loss)
I0421 23:19:13.329522 32711 sgd_solver.cpp:106] Iteration 47000, lr = 6.23479e-05
I0421 23:19:14.930691 32711 solver.cpp:337] Iteration 47500, Testing net (#0)
I0421 23:19:16.936125 32711 solver.cpp:404]     Test net output #0: accuracy = 0.56439
I0421 23:19:16.936180 32711 solver.cpp:404]     Test net output #1: loss = 2.02974 (* 1 = 2.02974 loss)
I0421 23:19:18.460774 32711 solver.cpp:337] Iteration 48000, Testing net (#0)
I0421 23:19:20.526190 32711 solver.cpp:404]     Test net output #0: accuracy = 0.49011
I0421 23:19:20.526259 32711 solver.cpp:404]     Test net output #1: loss = 2.58643 (* 1 = 2.58643 loss)
I0421 23:19:20.527585 32711 solver.cpp:228] Iteration 48000, loss = 0.474711
I0421 23:19:20.527628 32711 solver.cpp:244]     Train net output #0: loss = 0.474712 (* 1 = 0.474712 loss)
I0421 23:19:20.527670 32711 sgd_solver.cpp:106] Iteration 48000, lr = 6.15399e-05
I0421 23:19:22.016773 32711 solver.cpp:337] Iteration 48500, Testing net (#0)
I0421 23:19:24.021739 32711 solver.cpp:404]     Test net output #0: accuracy = 0.38907
I0421 23:19:24.021785 32711 solver.cpp:404]     Test net output #1: loss = 3.85381 (* 1 = 3.85381 loss)
I0421 23:19:25.583096 32711 solver.cpp:337] Iteration 49000, Testing net (#0)
I0421 23:19:27.580402 32711 solver.cpp:404]     Test net output #0: accuracy = 0.58914
I0421 23:19:27.580453 32711 solver.cpp:404]     Test net output #1: loss = 1.77662 (* 1 = 1.77662 loss)
I0421 23:19:27.581353 32711 solver.cpp:228] Iteration 49000, loss = 0.0831617
I0421 23:19:27.581370 32711 solver.cpp:244]     Train net output #0: loss = 0.0831631 (* 1 = 0.0831631 loss)
I0421 23:19:27.581399 32711 sgd_solver.cpp:106] Iteration 49000, lr = 6.0756e-05
I0421 23:19:29.161806 32711 solver.cpp:337] Iteration 49500, Testing net (#0)
I0421 23:19:31.151739 32711 solver.cpp:404]     Test net output #0: accuracy = 0.52173
I0421 23:19:31.151813 32711 solver.cpp:404]     Test net output #1: loss = 2.43405 (* 1 = 2.43405 loss)
I0421 23:19:32.673312 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_50000.caffemodel
I0421 23:19:32.680897 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_50000.solverstate
I0421 23:19:32.683621 32711 solver.cpp:337] Iteration 50000, Testing net (#0)
I0421 23:19:34.735266 32711 solver.cpp:404]     Test net output #0: accuracy = 0.44155
I0421 23:19:34.735306 32711 solver.cpp:404]     Test net output #1: loss = 3.35964 (* 1 = 3.35964 loss)
I0421 23:19:34.736356 32711 solver.cpp:228] Iteration 50000, loss = 0.578199
I0421 23:19:34.736394 32711 solver.cpp:244]     Train net output #0: loss = 0.5782 (* 1 = 0.5782 loss)
I0421 23:19:34.736426 32711 sgd_solver.cpp:106] Iteration 50000, lr = 5.99949e-05
I0421 23:19:36.240021 32711 solver.cpp:337] Iteration 50500, Testing net (#0)
I0421 23:19:38.270707 32711 solver.cpp:404]     Test net output #0: accuracy = 0.55251
I0421 23:19:38.270743 32711 solver.cpp:404]     Test net output #1: loss = 2.02896 (* 1 = 2.02896 loss)
I0421 23:19:39.819555 32711 solver.cpp:337] Iteration 51000, Testing net (#0)
I0421 23:19:41.828265 32711 solver.cpp:404]     Test net output #0: accuracy = 0.58311
I0421 23:19:41.828354 32711 solver.cpp:404]     Test net output #1: loss = 1.92949 (* 1 = 1.92949 loss)
I0421 23:19:41.829303 32711 solver.cpp:228] Iteration 51000, loss = 0.145277
I0421 23:19:41.829322 32711 solver.cpp:244]     Train net output #0: loss = 0.145279 (* 1 = 0.145279 loss)
I0421 23:19:41.829365 32711 sgd_solver.cpp:106] Iteration 51000, lr = 5.92557e-05
I0421 23:19:43.409068 32711 solver.cpp:337] Iteration 51500, Testing net (#0)
I0421 23:19:45.456152 32711 solver.cpp:404]     Test net output #0: accuracy = 0.48814
I0421 23:19:45.456192 32711 solver.cpp:404]     Test net output #1: loss = 2.82561 (* 1 = 2.82561 loss)
I0421 23:19:46.985481 32711 solver.cpp:337] Iteration 52000, Testing net (#0)
I0421 23:19:47.023478 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:19:49.010433 32711 solver.cpp:404]     Test net output #0: accuracy = 0.54456
I0421 23:19:49.010480 32711 solver.cpp:404]     Test net output #1: loss = 2.03545 (* 1 = 2.03545 loss)
I0421 23:19:49.011407 32711 solver.cpp:228] Iteration 52000, loss = 0.662378
I0421 23:19:49.011428 32711 solver.cpp:244]     Train net output #0: loss = 0.66238 (* 1 = 0.66238 loss)
I0421 23:19:49.011441 32711 sgd_solver.cpp:106] Iteration 52000, lr = 5.85375e-05
I0421 23:19:50.500047 32711 solver.cpp:337] Iteration 52500, Testing net (#0)
I0421 23:19:52.470996 32711 solver.cpp:404]     Test net output #0: accuracy = 0.60596
I0421 23:19:52.471061 32711 solver.cpp:404]     Test net output #1: loss = 1.76451 (* 1 = 1.76451 loss)
I0421 23:19:54.019889 32711 solver.cpp:337] Iteration 53000, Testing net (#0)
I0421 23:19:56.005548 32711 solver.cpp:404]     Test net output #0: accuracy = 0.51885
I0421 23:19:56.005606 32711 solver.cpp:404]     Test net output #1: loss = 2.3482 (* 1 = 2.3482 loss)
I0421 23:19:56.008536 32711 solver.cpp:228] Iteration 53000, loss = 0.133133
I0421 23:19:56.008555 32711 solver.cpp:244]     Train net output #0: loss = 0.133134 (* 1 = 0.133134 loss)
I0421 23:19:56.008564 32711 sgd_solver.cpp:106] Iteration 53000, lr = 5.78392e-05
I0421 23:19:57.586781 32711 solver.cpp:337] Iteration 53500, Testing net (#0)
I0421 23:19:59.624464 32711 solver.cpp:404]     Test net output #0: accuracy = 0.50991
I0421 23:19:59.624512 32711 solver.cpp:404]     Test net output #1: loss = 2.32481 (* 1 = 2.32481 loss)
I0421 23:20:01.152657 32711 solver.cpp:337] Iteration 54000, Testing net (#0)
I0421 23:20:03.170192 32711 solver.cpp:404]     Test net output #0: accuracy = 0.6139
I0421 23:20:03.170248 32711 solver.cpp:404]     Test net output #1: loss = 1.65356 (* 1 = 1.65356 loss)
I0421 23:20:03.171267 32711 solver.cpp:228] Iteration 54000, loss = 0.0418901
I0421 23:20:03.171299 32711 solver.cpp:244]     Train net output #0: loss = 0.0418915 (* 1 = 0.0418915 loss)
I0421 23:20:03.171324 32711 sgd_solver.cpp:106] Iteration 54000, lr = 5.71601e-05
I0421 23:20:04.662138 32711 solver.cpp:337] Iteration 54500, Testing net (#0)
I0421 23:20:06.680546 32711 solver.cpp:404]     Test net output #0: accuracy = 0.54651
I0421 23:20:06.680621 32711 solver.cpp:404]     Test net output #1: loss = 2.29732 (* 1 = 2.29732 loss)
I0421 23:20:08.237880 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_55000.caffemodel
I0421 23:20:08.244403 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_55000.solverstate
I0421 23:20:08.246877 32711 solver.cpp:337] Iteration 55000, Testing net (#0)
I0421 23:20:10.276576 32711 solver.cpp:404]     Test net output #0: accuracy = 0.48024
I0421 23:20:10.276675 32711 solver.cpp:404]     Test net output #1: loss = 2.55476 (* 1 = 2.55476 loss)
I0421 23:20:10.277726 32711 solver.cpp:228] Iteration 55000, loss = 0.324173
I0421 23:20:10.277761 32711 solver.cpp:244]     Train net output #0: loss = 0.324174 (* 1 = 0.324174 loss)
I0421 23:20:10.277786 32711 sgd_solver.cpp:106] Iteration 55000, lr = 5.64993e-05
I0421 23:20:11.855314 32711 solver.cpp:337] Iteration 55500, Testing net (#0)
I0421 23:20:13.872804 32711 solver.cpp:404]     Test net output #0: accuracy = 0.62077
I0421 23:20:13.873014 32711 solver.cpp:404]     Test net output #1: loss = 1.59559 (* 1 = 1.59559 loss)
I0421 23:20:15.436136 32711 solver.cpp:337] Iteration 56000, Testing net (#0)
I0421 23:20:17.424325 32711 solver.cpp:404]     Test net output #0: accuracy = 0.59308
I0421 23:20:17.424377 32711 solver.cpp:404]     Test net output #1: loss = 2.01009 (* 1 = 2.01009 loss)
I0421 23:20:17.425343 32711 solver.cpp:228] Iteration 56000, loss = 0.147046
I0421 23:20:17.425361 32711 solver.cpp:244]     Train net output #0: loss = 0.147047 (* 1 = 0.147047 loss)
I0421 23:20:17.425405 32711 sgd_solver.cpp:106] Iteration 56000, lr = 5.5856e-05
I0421 23:20:18.918959 32711 solver.cpp:337] Iteration 56500, Testing net (#0)
I0421 23:20:20.952249 32711 solver.cpp:404]     Test net output #0: accuracy = 0.51185
I0421 23:20:20.952332 32711 solver.cpp:404]     Test net output #1: loss = 2.46695 (* 1 = 2.46695 loss)
I0421 23:20:22.508596 32711 solver.cpp:337] Iteration 57000, Testing net (#0)
I0421 23:20:23.377476 32711 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:20:24.529403 32711 solver.cpp:404]     Test net output #0: accuracy = 0.604
I0421 23:20:24.529508 32711 solver.cpp:404]     Test net output #1: loss = 1.70493 (* 1 = 1.70493 loss)
I0421 23:20:24.530400 32711 solver.cpp:228] Iteration 57000, loss = 0.114745
I0421 23:20:24.530418 32711 solver.cpp:244]     Train net output #0: loss = 0.114747 (* 1 = 0.114747 loss)
I0421 23:20:24.530426 32711 sgd_solver.cpp:106] Iteration 57000, lr = 5.52296e-05
I0421 23:20:26.108424 32711 solver.cpp:337] Iteration 57500, Testing net (#0)
I0421 23:20:28.114832 32711 solver.cpp:404]     Test net output #0: accuracy = 0.59013
I0421 23:20:28.114868 32711 solver.cpp:404]     Test net output #1: loss = 2.0061 (* 1 = 2.0061 loss)
I0421 23:20:29.642593 32711 solver.cpp:337] Iteration 58000, Testing net (#0)
I0421 23:20:31.666282 32711 solver.cpp:404]     Test net output #0: accuracy = 0.50787
I0421 23:20:31.666349 32711 solver.cpp:404]     Test net output #1: loss = 2.63303 (* 1 = 2.63303 loss)
I0421 23:20:31.667279 32711 solver.cpp:228] Iteration 58000, loss = 0.0722418
I0421 23:20:31.667310 32711 solver.cpp:244]     Train net output #0: loss = 0.0722431 (* 1 = 0.0722431 loss)
I0421 23:20:31.667323 32711 sgd_solver.cpp:106] Iteration 58000, lr = 5.46193e-05
I0421 23:20:33.153755 32711 solver.cpp:337] Iteration 58500, Testing net (#0)
I0421 23:20:35.135509 32711 solver.cpp:404]     Test net output #0: accuracy = 0.61786
I0421 23:20:35.135567 32711 solver.cpp:404]     Test net output #1: loss = 1.65826 (* 1 = 1.65826 loss)
I0421 23:20:36.684211 32711 solver.cpp:337] Iteration 59000, Testing net (#0)
I0421 23:20:38.712081 32711 solver.cpp:404]     Test net output #0: accuracy = 0.61587
I0421 23:20:38.712164 32711 solver.cpp:404]     Test net output #1: loss = 1.78067 (* 1 = 1.78067 loss)
I0421 23:20:38.713325 32711 solver.cpp:228] Iteration 59000, loss = 0.711119
I0421 23:20:38.713364 32711 solver.cpp:244]     Train net output #0: loss = 0.71112 (* 1 = 0.71112 loss)
I0421 23:20:38.713392 32711 sgd_solver.cpp:106] Iteration 59000, lr = 5.40245e-05
I0421 23:20:40.296907 32711 solver.cpp:337] Iteration 59500, Testing net (#0)
I0421 23:20:42.314931 32711 solver.cpp:404]     Test net output #0: accuracy = 0.51579
I0421 23:20:42.314990 32711 solver.cpp:404]     Test net output #1: loss = 2.62366 (* 1 = 2.62366 loss)
I0421 23:20:43.831723 32711 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_60000.caffemodel
I0421 23:20:43.838698 32711 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_60000.solverstate
I0421 23:20:43.842051 32711 solver.cpp:317] Iteration 60000, loss = 0.0183333
I0421 23:20:43.842089 32711 solver.cpp:337] Iteration 60000, Testing net (#0)
I0421 23:20:45.908802 32711 solver.cpp:404]     Test net output #0: accuracy = 0.61977
I0421 23:20:45.908987 32711 solver.cpp:404]     Test net output #1: loss = 1.59852 (* 1 = 1.59852 loss)
I0421 23:20:45.908996 32711 solver.cpp:322] Optimization Done.
I0421 23:20:45.909003 32711 caffe.cpp:222] Optimization Done.
I0421 23:25:45.986099  1684 caffe.cpp:185] Using GPUs 0
I0421 23:25:45.990104  1684 caffe.cpp:190] GPU 0: GeForce GTX 980M
I0421 23:25:46.141701  1684 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 5.3e-06
display: 1000
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.00028
snapshot: 5000
snapshot_prefix: "/home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/"
solver_mode: GPU
device_id: 0
net: "/home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt"
I0421 23:25:46.141898  1684 solver.cpp:91] Creating training net from net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:25:46.142252  1684 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0421 23:25:46.142268  1684 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0421 23:25:46.142360  1684 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/train"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:25:46.142493  1684 layer_factory.hpp:77] Creating layer mnist
I0421 23:25:46.142974  1684 net.cpp:91] Creating Layer mnist
I0421 23:25:46.143019  1684 net.cpp:399] mnist -> data
I0421 23:25:46.143102  1684 net.cpp:399] mnist -> label
I0421 23:25:46.143795  1695 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/train
I0421 23:25:46.150552  1684 data_layer.cpp:41] output data size: 64,1,28,28
I0421 23:25:46.151290  1684 net.cpp:141] Setting up mnist
I0421 23:25:46.151327  1684 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0421 23:25:46.151334  1684 net.cpp:148] Top shape: 64 (64)
I0421 23:25:46.151350  1684 net.cpp:156] Memory required for data: 200960
I0421 23:25:46.151360  1684 layer_factory.hpp:77] Creating layer conv1
I0421 23:25:46.151392  1684 net.cpp:91] Creating Layer conv1
I0421 23:25:46.151399  1684 net.cpp:425] conv1 <- data
I0421 23:25:46.151413  1684 net.cpp:399] conv1 -> conv1
I0421 23:25:46.281376  1684 net.cpp:141] Setting up conv1
I0421 23:25:46.281426  1684 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0421 23:25:46.281431  1684 net.cpp:156] Memory required for data: 3150080
I0421 23:25:46.281497  1684 layer_factory.hpp:77] Creating layer pool1
I0421 23:25:46.281524  1684 net.cpp:91] Creating Layer pool1
I0421 23:25:46.281528  1684 net.cpp:425] pool1 <- conv1
I0421 23:25:46.281534  1684 net.cpp:399] pool1 -> pool1
I0421 23:25:46.281577  1684 net.cpp:141] Setting up pool1
I0421 23:25:46.281585  1684 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0421 23:25:46.281589  1684 net.cpp:156] Memory required for data: 3887360
I0421 23:25:46.281592  1684 layer_factory.hpp:77] Creating layer conv2
I0421 23:25:46.281632  1684 net.cpp:91] Creating Layer conv2
I0421 23:25:46.281636  1684 net.cpp:425] conv2 <- pool1
I0421 23:25:46.281641  1684 net.cpp:399] conv2 -> conv2
I0421 23:25:46.282449  1684 net.cpp:141] Setting up conv2
I0421 23:25:46.282460  1684 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0421 23:25:46.282464  1684 net.cpp:156] Memory required for data: 4706560
I0421 23:25:46.282496  1684 layer_factory.hpp:77] Creating layer pool2
I0421 23:25:46.282508  1684 net.cpp:91] Creating Layer pool2
I0421 23:25:46.282512  1684 net.cpp:425] pool2 <- conv2
I0421 23:25:46.282517  1684 net.cpp:399] pool2 -> pool2
I0421 23:25:46.282562  1684 net.cpp:141] Setting up pool2
I0421 23:25:46.282568  1684 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0421 23:25:46.282572  1684 net.cpp:156] Memory required for data: 4911360
I0421 23:25:46.282575  1684 layer_factory.hpp:77] Creating layer ip1
I0421 23:25:46.282603  1684 net.cpp:91] Creating Layer ip1
I0421 23:25:46.282606  1684 net.cpp:425] ip1 <- pool2
I0421 23:25:46.282615  1684 net.cpp:399] ip1 -> ip1
I0421 23:25:46.285166  1684 net.cpp:141] Setting up ip1
I0421 23:25:46.285178  1684 net.cpp:148] Top shape: 64 500 (32000)
I0421 23:25:46.285182  1684 net.cpp:156] Memory required for data: 5039360
I0421 23:25:46.285207  1684 layer_factory.hpp:77] Creating layer relu1
I0421 23:25:46.285213  1684 net.cpp:91] Creating Layer relu1
I0421 23:25:46.285218  1684 net.cpp:425] relu1 <- ip1
I0421 23:25:46.285223  1684 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:25:46.285446  1684 net.cpp:141] Setting up relu1
I0421 23:25:46.285456  1684 net.cpp:148] Top shape: 64 500 (32000)
I0421 23:25:46.285459  1684 net.cpp:156] Memory required for data: 5167360
I0421 23:25:46.285483  1684 layer_factory.hpp:77] Creating layer ip2
I0421 23:25:46.285496  1684 net.cpp:91] Creating Layer ip2
I0421 23:25:46.285501  1684 net.cpp:425] ip2 <- ip1
I0421 23:25:46.285506  1684 net.cpp:399] ip2 -> ip2
I0421 23:25:46.285857  1684 net.cpp:141] Setting up ip2
I0421 23:25:46.285864  1684 net.cpp:148] Top shape: 64 101 (6464)
I0421 23:25:46.285867  1684 net.cpp:156] Memory required for data: 5193216
I0421 23:25:46.285872  1684 layer_factory.hpp:77] Creating layer loss
I0421 23:25:46.285897  1684 net.cpp:91] Creating Layer loss
I0421 23:25:46.285902  1684 net.cpp:425] loss <- ip2
I0421 23:25:46.285905  1684 net.cpp:425] loss <- label
I0421 23:25:46.285910  1684 net.cpp:399] loss -> loss
I0421 23:25:46.285923  1684 layer_factory.hpp:77] Creating layer loss
I0421 23:25:46.286541  1684 net.cpp:141] Setting up loss
I0421 23:25:46.286551  1684 net.cpp:148] Top shape: (1)
I0421 23:25:46.286556  1684 net.cpp:151]     with loss weight 1
I0421 23:25:46.286604  1684 net.cpp:156] Memory required for data: 5193220
I0421 23:25:46.286608  1684 net.cpp:217] loss needs backward computation.
I0421 23:25:46.286612  1684 net.cpp:217] ip2 needs backward computation.
I0421 23:25:46.286617  1684 net.cpp:217] relu1 needs backward computation.
I0421 23:25:46.286619  1684 net.cpp:217] ip1 needs backward computation.
I0421 23:25:46.286623  1684 net.cpp:217] pool2 needs backward computation.
I0421 23:25:46.286628  1684 net.cpp:217] conv2 needs backward computation.
I0421 23:25:46.286631  1684 net.cpp:217] pool1 needs backward computation.
I0421 23:25:46.286638  1684 net.cpp:217] conv1 needs backward computation.
I0421 23:25:46.286641  1684 net.cpp:219] mnist does not need backward computation.
I0421 23:25:46.286646  1684 net.cpp:261] This network produces output loss
I0421 23:25:46.286655  1684 net.cpp:274] Network initialization done.
I0421 23:25:46.286900  1684 solver.cpp:181] Creating test net (#0) specified by net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:25:46.286958  1684 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0421 23:25:46.287055  1684 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/test"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:25:46.287145  1684 layer_factory.hpp:77] Creating layer mnist
I0421 23:25:46.287803  1684 net.cpp:91] Creating Layer mnist
I0421 23:25:46.287832  1684 net.cpp:399] mnist -> data
I0421 23:25:46.287858  1684 net.cpp:399] mnist -> label
I0421 23:25:46.288511  1711 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/test
I0421 23:25:46.288727  1684 data_layer.cpp:41] output data size: 1000,1,28,28
I0421 23:25:46.292731  1684 net.cpp:141] Setting up mnist
I0421 23:25:46.292789  1684 net.cpp:148] Top shape: 1000 1 28 28 (784000)
I0421 23:25:46.292794  1684 net.cpp:148] Top shape: 1000 (1000)
I0421 23:25:46.292816  1684 net.cpp:156] Memory required for data: 3140000
I0421 23:25:46.292822  1684 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0421 23:25:46.292881  1684 net.cpp:91] Creating Layer label_mnist_1_split
I0421 23:25:46.292901  1684 net.cpp:425] label_mnist_1_split <- label
I0421 23:25:46.292907  1684 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I0421 23:25:46.292917  1684 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I0421 23:25:46.293035  1684 net.cpp:141] Setting up label_mnist_1_split
I0421 23:25:46.293051  1684 net.cpp:148] Top shape: 1000 (1000)
I0421 23:25:46.293074  1684 net.cpp:148] Top shape: 1000 (1000)
I0421 23:25:46.293078  1684 net.cpp:156] Memory required for data: 3148000
I0421 23:25:46.293107  1684 layer_factory.hpp:77] Creating layer conv1
I0421 23:25:46.293119  1684 net.cpp:91] Creating Layer conv1
I0421 23:25:46.293123  1684 net.cpp:425] conv1 <- data
I0421 23:25:46.293129  1684 net.cpp:399] conv1 -> conv1
I0421 23:25:46.294499  1684 net.cpp:141] Setting up conv1
I0421 23:25:46.294515  1684 net.cpp:148] Top shape: 1000 20 24 24 (11520000)
I0421 23:25:46.294538  1684 net.cpp:156] Memory required for data: 49228000
I0421 23:25:46.294548  1684 layer_factory.hpp:77] Creating layer pool1
I0421 23:25:46.294555  1684 net.cpp:91] Creating Layer pool1
I0421 23:25:46.294559  1684 net.cpp:425] pool1 <- conv1
I0421 23:25:46.294565  1684 net.cpp:399] pool1 -> pool1
I0421 23:25:46.294610  1684 net.cpp:141] Setting up pool1
I0421 23:25:46.294616  1684 net.cpp:148] Top shape: 1000 20 12 12 (2880000)
I0421 23:25:46.294620  1684 net.cpp:156] Memory required for data: 60748000
I0421 23:25:46.294623  1684 layer_factory.hpp:77] Creating layer conv2
I0421 23:25:46.294632  1684 net.cpp:91] Creating Layer conv2
I0421 23:25:46.294636  1684 net.cpp:425] conv2 <- pool1
I0421 23:25:46.294641  1684 net.cpp:399] conv2 -> conv2
I0421 23:25:46.295670  1684 net.cpp:141] Setting up conv2
I0421 23:25:46.295696  1684 net.cpp:148] Top shape: 1000 50 8 8 (3200000)
I0421 23:25:46.295701  1684 net.cpp:156] Memory required for data: 73548000
I0421 23:25:46.295708  1684 layer_factory.hpp:77] Creating layer pool2
I0421 23:25:46.295717  1684 net.cpp:91] Creating Layer pool2
I0421 23:25:46.295720  1684 net.cpp:425] pool2 <- conv2
I0421 23:25:46.295725  1684 net.cpp:399] pool2 -> pool2
I0421 23:25:46.295759  1684 net.cpp:141] Setting up pool2
I0421 23:25:46.295766  1684 net.cpp:148] Top shape: 1000 50 4 4 (800000)
I0421 23:25:46.295769  1684 net.cpp:156] Memory required for data: 76748000
I0421 23:25:46.295778  1684 layer_factory.hpp:77] Creating layer ip1
I0421 23:25:46.295784  1684 net.cpp:91] Creating Layer ip1
I0421 23:25:46.295809  1684 net.cpp:425] ip1 <- pool2
I0421 23:25:46.295814  1684 net.cpp:399] ip1 -> ip1
I0421 23:25:46.298502  1684 net.cpp:141] Setting up ip1
I0421 23:25:46.298513  1684 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:25:46.298517  1684 net.cpp:156] Memory required for data: 78748000
I0421 23:25:46.298543  1684 layer_factory.hpp:77] Creating layer relu1
I0421 23:25:46.298550  1684 net.cpp:91] Creating Layer relu1
I0421 23:25:46.298554  1684 net.cpp:425] relu1 <- ip1
I0421 23:25:46.298560  1684 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:25:46.298810  1684 net.cpp:141] Setting up relu1
I0421 23:25:46.298821  1684 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:25:46.298825  1684 net.cpp:156] Memory required for data: 80748000
I0421 23:25:46.298847  1684 layer_factory.hpp:77] Creating layer ip2
I0421 23:25:46.298856  1684 net.cpp:91] Creating Layer ip2
I0421 23:25:46.298859  1684 net.cpp:425] ip2 <- ip1
I0421 23:25:46.298866  1684 net.cpp:399] ip2 -> ip2
I0421 23:25:46.299551  1684 net.cpp:141] Setting up ip2
I0421 23:25:46.299563  1684 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:25:46.299566  1684 net.cpp:156] Memory required for data: 81152000
I0421 23:25:46.299592  1684 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0421 23:25:46.299650  1684 net.cpp:91] Creating Layer ip2_ip2_0_split
I0421 23:25:46.299670  1684 net.cpp:425] ip2_ip2_0_split <- ip2
I0421 23:25:46.299677  1684 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0421 23:25:46.299684  1684 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0421 23:25:46.299716  1684 net.cpp:141] Setting up ip2_ip2_0_split
I0421 23:25:46.299724  1684 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:25:46.299728  1684 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:25:46.299732  1684 net.cpp:156] Memory required for data: 81960000
I0421 23:25:46.299736  1684 layer_factory.hpp:77] Creating layer accuracy
I0421 23:25:46.299741  1684 net.cpp:91] Creating Layer accuracy
I0421 23:25:46.299746  1684 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0421 23:25:46.299749  1684 net.cpp:425] accuracy <- label_mnist_1_split_0
I0421 23:25:46.299754  1684 net.cpp:399] accuracy -> accuracy
I0421 23:25:46.299762  1684 net.cpp:141] Setting up accuracy
I0421 23:25:46.299767  1684 net.cpp:148] Top shape: (1)
I0421 23:25:46.299770  1684 net.cpp:156] Memory required for data: 81960004
I0421 23:25:46.299782  1684 layer_factory.hpp:77] Creating layer loss
I0421 23:25:46.299788  1684 net.cpp:91] Creating Layer loss
I0421 23:25:46.299793  1684 net.cpp:425] loss <- ip2_ip2_0_split_1
I0421 23:25:46.299798  1684 net.cpp:425] loss <- label_mnist_1_split_1
I0421 23:25:46.299803  1684 net.cpp:399] loss -> loss
I0421 23:25:46.299809  1684 layer_factory.hpp:77] Creating layer loss
I0421 23:25:46.300406  1684 net.cpp:141] Setting up loss
I0421 23:25:46.300418  1684 net.cpp:148] Top shape: (1)
I0421 23:25:46.300422  1684 net.cpp:151]     with loss weight 1
I0421 23:25:46.300431  1684 net.cpp:156] Memory required for data: 81960008
I0421 23:25:46.300436  1684 net.cpp:217] loss needs backward computation.
I0421 23:25:46.300439  1684 net.cpp:219] accuracy does not need backward computation.
I0421 23:25:46.300443  1684 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0421 23:25:46.300448  1684 net.cpp:217] ip2 needs backward computation.
I0421 23:25:46.300453  1684 net.cpp:217] relu1 needs backward computation.
I0421 23:25:46.300457  1684 net.cpp:217] ip1 needs backward computation.
I0421 23:25:46.300460  1684 net.cpp:217] pool2 needs backward computation.
I0421 23:25:46.300464  1684 net.cpp:217] conv2 needs backward computation.
I0421 23:25:46.300468  1684 net.cpp:217] pool1 needs backward computation.
I0421 23:25:46.300472  1684 net.cpp:217] conv1 needs backward computation.
I0421 23:25:46.300477  1684 net.cpp:219] label_mnist_1_split does not need backward computation.
I0421 23:25:46.300482  1684 net.cpp:219] mnist does not need backward computation.
I0421 23:25:46.300487  1684 net.cpp:261] This network produces output accuracy
I0421 23:25:46.300493  1684 net.cpp:261] This network produces output loss
I0421 23:25:46.300503  1684 net.cpp:274] Network initialization done.
I0421 23:25:46.300546  1684 solver.cpp:60] Solver scaffolding done.
I0421 23:25:46.300802  1684 caffe.cpp:129] Finetuning from ./snapshot/_iter_60000.caffemodel
I0421 23:25:46.306666  1684 caffe.cpp:219] Starting Optimization
I0421 23:25:46.306694  1684 solver.cpp:279] Solving LeNet
I0421 23:25:46.306699  1684 solver.cpp:280] Learning Rate Policy: inv
I0421 23:25:46.307102  1684 solver.cpp:337] Iteration 0, Testing net (#0)
I0421 23:25:46.307778  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:25:48.357084  1684 solver.cpp:404]     Test net output #0: accuracy = 0.61983
I0421 23:25:48.357167  1684 solver.cpp:404]     Test net output #1: loss = 1.59827 (* 1 = 1.59827 loss)
I0421 23:25:48.359390  1684 solver.cpp:228] Iteration 0, loss = 0.182563
I0421 23:25:48.359452  1684 solver.cpp:244]     Train net output #0: loss = 0.182563 (* 1 = 0.182563 loss)
I0421 23:25:48.359469  1684 sgd_solver.cpp:106] Iteration 0, lr = 5.3e-06
I0421 23:25:49.855216  1684 solver.cpp:337] Iteration 500, Testing net (#0)
I0421 23:25:51.851300  1684 solver.cpp:404]     Test net output #0: accuracy = 0.65242
I0421 23:25:51.851372  1684 solver.cpp:404]     Test net output #1: loss = 1.43212 (* 1 = 1.43212 loss)
I0421 23:25:53.600671  1684 solver.cpp:337] Iteration 1000, Testing net (#0)
I0421 23:25:55.628794  1684 solver.cpp:404]     Test net output #0: accuracy = 0.69904
I0421 23:25:55.628834  1684 solver.cpp:404]     Test net output #1: loss = 1.15404 (* 1 = 1.15404 loss)
I0421 23:25:55.629941  1684 solver.cpp:228] Iteration 1000, loss = 1.46973
I0421 23:25:55.629962  1684 solver.cpp:244]     Train net output #0: loss = 1.46973 (* 1 = 1.46973 loss)
I0421 23:25:55.629992  1684 sgd_solver.cpp:106] Iteration 1000, lr = 4.93437e-06
I0421 23:25:57.270673  1684 solver.cpp:337] Iteration 1500, Testing net (#0)
I0421 23:25:59.255861  1684 solver.cpp:404]     Test net output #0: accuracy = 0.73864
I0421 23:25:59.255928  1684 solver.cpp:404]     Test net output #1: loss = 1.04316 (* 1 = 1.04316 loss)
I0421 23:26:00.780522  1684 solver.cpp:337] Iteration 2000, Testing net (#0)
I0421 23:26:02.764787  1684 solver.cpp:404]     Test net output #0: accuracy = 0.73961
I0421 23:26:02.764840  1684 solver.cpp:404]     Test net output #1: loss = 1.00646 (* 1 = 1.00646 loss)
I0421 23:26:02.765825  1684 solver.cpp:228] Iteration 2000, loss = 0.0270583
I0421 23:26:02.765842  1684 solver.cpp:244]     Train net output #0: loss = 0.0270583 (* 1 = 0.0270583 loss)
I0421 23:26:02.765864  1684 sgd_solver.cpp:106] Iteration 2000, lr = 4.62264e-06
I0421 23:26:04.256079  1684 solver.cpp:337] Iteration 2500, Testing net (#0)
I0421 23:26:06.224649  1684 solver.cpp:404]     Test net output #0: accuracy = 0.72774
I0421 23:26:06.224704  1684 solver.cpp:404]     Test net output #1: loss = 1.01997 (* 1 = 1.01997 loss)
I0421 23:26:07.766597  1684 solver.cpp:337] Iteration 3000, Testing net (#0)
I0421 23:26:09.764269  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7495
I0421 23:26:09.764343  1684 solver.cpp:404]     Test net output #1: loss = 0.978882 (* 1 = 0.978882 loss)
I0421 23:26:09.765241  1684 solver.cpp:228] Iteration 3000, loss = 0.365713
I0421 23:26:09.765259  1684 solver.cpp:244]     Train net output #0: loss = 0.365713 (* 1 = 0.365713 loss)
I0421 23:26:09.765282  1684 sgd_solver.cpp:106] Iteration 3000, lr = 4.3533e-06
I0421 23:26:11.341177  1684 solver.cpp:337] Iteration 3500, Testing net (#0)
I0421 23:26:13.325781  1684 solver.cpp:404]     Test net output #0: accuracy = 0.75745
I0421 23:26:13.325839  1684 solver.cpp:404]     Test net output #1: loss = 0.947339 (* 1 = 0.947339 loss)
I0421 23:26:14.873087  1684 solver.cpp:337] Iteration 4000, Testing net (#0)
I0421 23:26:16.846680  1684 solver.cpp:404]     Test net output #0: accuracy = 0.74853
I0421 23:26:16.846757  1684 solver.cpp:404]     Test net output #1: loss = 0.959513 (* 1 = 0.959513 loss)
I0421 23:26:16.847759  1684 solver.cpp:228] Iteration 4000, loss = 0.207884
I0421 23:26:16.847776  1684 solver.cpp:244]     Train net output #0: loss = 0.207884 (* 1 = 0.207884 loss)
I0421 23:26:16.847785  1684 sgd_solver.cpp:106] Iteration 4000, lr = 4.11794e-06
I0421 23:26:18.335001  1684 solver.cpp:337] Iteration 4500, Testing net (#0)
I0421 23:26:20.281250  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76042
I0421 23:26:20.281317  1684 solver.cpp:404]     Test net output #1: loss = 0.940438 (* 1 = 0.940438 loss)
I0421 23:26:21.825018  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.caffemodel
I0421 23:26:21.832748  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.solverstate
I0421 23:26:21.835546  1684 solver.cpp:337] Iteration 5000, Testing net (#0)
I0421 23:26:22.590942  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:26:23.779634  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76831
I0421 23:26:23.779705  1684 solver.cpp:404]     Test net output #1: loss = 0.932341 (* 1 = 0.932341 loss)
I0421 23:26:23.780691  1684 solver.cpp:228] Iteration 5000, loss = 0.762314
I0421 23:26:23.780725  1684 solver.cpp:244]     Train net output #0: loss = 0.762314 (* 1 = 0.762314 loss)
I0421 23:26:23.780733  1684 sgd_solver.cpp:106] Iteration 5000, lr = 3.91028e-06
I0421 23:26:25.355803  1684 solver.cpp:337] Iteration 5500, Testing net (#0)
I0421 23:26:27.275610  1684 solver.cpp:404]     Test net output #0: accuracy = 0.75644
I0421 23:26:27.275676  1684 solver.cpp:404]     Test net output #1: loss = 0.934241 (* 1 = 0.934241 loss)
I0421 23:26:28.802073  1684 solver.cpp:337] Iteration 6000, Testing net (#0)
I0421 23:26:30.769042  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76041
I0421 23:26:30.769129  1684 solver.cpp:404]     Test net output #1: loss = 0.92246 (* 1 = 0.92246 loss)
I0421 23:26:30.770177  1684 solver.cpp:228] Iteration 6000, loss = 0.0319645
I0421 23:26:30.770195  1684 solver.cpp:244]     Train net output #0: loss = 0.0319645 (* 1 = 0.0319645 loss)
I0421 23:26:30.770203  1684 sgd_solver.cpp:106] Iteration 6000, lr = 3.72551e-06
I0421 23:26:32.257227  1684 solver.cpp:337] Iteration 6500, Testing net (#0)
I0421 23:26:34.236120  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77026
I0421 23:26:34.236189  1684 solver.cpp:404]     Test net output #1: loss = 0.914118 (* 1 = 0.914118 loss)
I0421 23:26:35.790098  1684 solver.cpp:337] Iteration 7000, Testing net (#0)
I0421 23:26:37.771380  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76438
I0421 23:26:37.771421  1684 solver.cpp:404]     Test net output #1: loss = 0.913649 (* 1 = 0.913649 loss)
I0421 23:26:37.772408  1684 solver.cpp:228] Iteration 7000, loss = 0.83469
I0421 23:26:37.772456  1684 solver.cpp:244]     Train net output #0: loss = 0.83469 (* 1 = 0.83469 loss)
I0421 23:26:37.772488  1684 sgd_solver.cpp:106] Iteration 7000, lr = 3.55991e-06
I0421 23:26:39.353030  1684 solver.cpp:337] Iteration 7500, Testing net (#0)
I0421 23:26:41.313858  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76042
I0421 23:26:41.313925  1684 solver.cpp:404]     Test net output #1: loss = 0.911191 (* 1 = 0.911191 loss)
I0421 23:26:42.833638  1684 solver.cpp:337] Iteration 8000, Testing net (#0)
I0421 23:26:44.829813  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77131
I0421 23:26:44.829867  1684 solver.cpp:404]     Test net output #1: loss = 0.900538 (* 1 = 0.900538 loss)
I0421 23:26:44.831025  1684 solver.cpp:228] Iteration 8000, loss = 0.00891411
I0421 23:26:44.831061  1684 solver.cpp:244]     Train net output #0: loss = 0.00891402 (* 1 = 0.00891402 loss)
I0421 23:26:44.831094  1684 sgd_solver.cpp:106] Iteration 8000, lr = 3.41053e-06
I0421 23:26:46.314777  1684 solver.cpp:337] Iteration 8500, Testing net (#0)
I0421 23:26:48.271057  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77427
I0421 23:26:48.271237  1684 solver.cpp:404]     Test net output #1: loss = 0.896957 (* 1 = 0.896957 loss)
I0421 23:26:49.824822  1684 solver.cpp:337] Iteration 9000, Testing net (#0)
I0421 23:26:51.780635  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76633
I0421 23:26:51.780707  1684 solver.cpp:404]     Test net output #1: loss = 0.905272 (* 1 = 0.905272 loss)
I0421 23:26:51.781656  1684 solver.cpp:228] Iteration 9000, loss = 0.316021
I0421 23:26:51.781673  1684 solver.cpp:244]     Train net output #0: loss = 0.316021 (* 1 = 0.316021 loss)
I0421 23:26:51.781684  1684 sgd_solver.cpp:106] Iteration 9000, lr = 3.275e-06
I0421 23:26:53.362567  1684 solver.cpp:337] Iteration 9500, Testing net (#0)
I0421 23:26:55.329186  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77128
I0421 23:26:55.329244  1684 solver.cpp:404]     Test net output #1: loss = 0.892535 (* 1 = 0.892535 loss)
I0421 23:26:56.849084  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.caffemodel
I0421 23:26:56.855782  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.solverstate
I0421 23:26:56.858526  1684 solver.cpp:337] Iteration 10000, Testing net (#0)
I0421 23:26:58.409075  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:26:58.817073  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77424
I0421 23:26:58.817138  1684 solver.cpp:404]     Test net output #1: loss = 0.885057 (* 1 = 0.885057 loss)
I0421 23:26:58.818138  1684 solver.cpp:228] Iteration 10000, loss = 0.316683
I0421 23:26:58.818184  1684 solver.cpp:244]     Train net output #0: loss = 0.316683 (* 1 = 0.316683 loss)
I0421 23:26:58.818192  1684 sgd_solver.cpp:106] Iteration 10000, lr = 3.1514e-06
I0421 23:27:00.307349  1684 solver.cpp:337] Iteration 10500, Testing net (#0)
I0421 23:27:02.250795  1684 solver.cpp:404]     Test net output #0: accuracy = 0.76734
I0421 23:27:02.250856  1684 solver.cpp:404]     Test net output #1: loss = 0.904769 (* 1 = 0.904769 loss)
I0421 23:27:03.804059  1684 solver.cpp:337] Iteration 11000, Testing net (#0)
I0421 23:27:05.737587  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77222
I0421 23:27:05.737643  1684 solver.cpp:404]     Test net output #1: loss = 0.887449 (* 1 = 0.887449 loss)
I0421 23:27:05.738638  1684 solver.cpp:228] Iteration 11000, loss = 0.477182
I0421 23:27:05.738672  1684 solver.cpp:244]     Train net output #0: loss = 0.477182 (* 1 = 0.477182 loss)
I0421 23:27:05.738680  1684 sgd_solver.cpp:106] Iteration 11000, lr = 3.03817e-06
I0421 23:27:07.309795  1684 solver.cpp:337] Iteration 11500, Testing net (#0)
I0421 23:27:09.284056  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7792
I0421 23:27:09.284098  1684 solver.cpp:404]     Test net output #1: loss = 0.87718 (* 1 = 0.87718 loss)
I0421 23:27:10.806596  1684 solver.cpp:337] Iteration 12000, Testing net (#0)
I0421 23:27:12.737087  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77031
I0421 23:27:12.737159  1684 solver.cpp:404]     Test net output #1: loss = 0.898851 (* 1 = 0.898851 loss)
I0421 23:27:12.738174  1684 solver.cpp:228] Iteration 12000, loss = 0.279012
I0421 23:27:12.738194  1684 solver.cpp:244]     Train net output #0: loss = 0.279012 (* 1 = 0.279012 loss)
I0421 23:27:12.738209  1684 sgd_solver.cpp:106] Iteration 12000, lr = 2.93399e-06
I0421 23:27:14.239078  1684 solver.cpp:337] Iteration 12500, Testing net (#0)
I0421 23:27:16.218217  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77427
I0421 23:27:16.218268  1684 solver.cpp:404]     Test net output #1: loss = 0.883701 (* 1 = 0.883701 loss)
I0421 23:27:17.765161  1684 solver.cpp:337] Iteration 13000, Testing net (#0)
I0421 23:27:19.659019  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7772
I0421 23:27:19.659164  1684 solver.cpp:404]     Test net output #1: loss = 0.873347 (* 1 = 0.873347 loss)
I0421 23:27:19.660238  1684 solver.cpp:228] Iteration 13000, loss = 0.768024
I0421 23:27:19.660281  1684 solver.cpp:244]     Train net output #0: loss = 0.768024 (* 1 = 0.768024 loss)
I0421 23:27:19.660295  1684 sgd_solver.cpp:106] Iteration 13000, lr = 2.83779e-06
I0421 23:27:21.236528  1684 solver.cpp:337] Iteration 13500, Testing net (#0)
I0421 23:27:23.230077  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7713
I0421 23:27:23.230113  1684 solver.cpp:404]     Test net output #1: loss = 0.889901 (* 1 = 0.889901 loss)
I0421 23:27:24.756497  1684 solver.cpp:337] Iteration 14000, Testing net (#0)
I0421 23:27:26.749572  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77524
I0421 23:27:26.749613  1684 solver.cpp:404]     Test net output #1: loss = 0.878853 (* 1 = 0.878853 loss)
I0421 23:27:26.750529  1684 solver.cpp:228] Iteration 14000, loss = 0.225452
I0421 23:27:26.750545  1684 solver.cpp:244]     Train net output #0: loss = 0.225452 (* 1 = 0.225452 loss)
I0421 23:27:26.750567  1684 sgd_solver.cpp:106] Iteration 14000, lr = 2.74864e-06
I0421 23:27:28.233990  1684 solver.cpp:337] Iteration 14500, Testing net (#0)
I0421 23:27:30.201688  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7802
I0421 23:27:30.201738  1684 solver.cpp:404]     Test net output #1: loss = 0.872266 (* 1 = 0.872266 loss)
I0421 23:27:31.746847  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.caffemodel
I0421 23:27:31.753885  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.solverstate
I0421 23:27:31.756783  1684 solver.cpp:337] Iteration 15000, Testing net (#0)
I0421 23:27:33.742801  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77326
I0421 23:27:33.742840  1684 solver.cpp:404]     Test net output #1: loss = 0.883031 (* 1 = 0.883031 loss)
I0421 23:27:33.743989  1684 solver.cpp:228] Iteration 15000, loss = 0.108509
I0421 23:27:33.744014  1684 solver.cpp:244]     Train net output #0: loss = 0.108509 (* 1 = 0.108509 loss)
I0421 23:27:33.744029  1684 sgd_solver.cpp:106] Iteration 15000, lr = 2.66576e-06
I0421 23:27:35.320155  1684 solver.cpp:337] Iteration 15500, Testing net (#0)
I0421 23:27:35.756732  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:27:37.304232  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77426
I0421 23:27:37.304306  1684 solver.cpp:404]     Test net output #1: loss = 0.876709 (* 1 = 0.876709 loss)
I0421 23:27:38.826537  1684 solver.cpp:337] Iteration 16000, Testing net (#0)
I0421 23:27:40.802979  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78318
I0421 23:27:40.803036  1684 solver.cpp:404]     Test net output #1: loss = 0.870607 (* 1 = 0.870607 loss)
I0421 23:27:40.804555  1684 solver.cpp:228] Iteration 16000, loss = 0.0104432
I0421 23:27:40.804581  1684 solver.cpp:244]     Train net output #0: loss = 0.010443 (* 1 = 0.010443 loss)
I0421 23:27:40.804595  1684 sgd_solver.cpp:106] Iteration 16000, lr = 2.58849e-06
I0421 23:27:42.292918  1684 solver.cpp:337] Iteration 16500, Testing net (#0)
I0421 23:27:44.259310  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77426
I0421 23:27:44.259349  1684 solver.cpp:404]     Test net output #1: loss = 0.877454 (* 1 = 0.877454 loss)
I0421 23:27:45.816025  1684 solver.cpp:337] Iteration 17000, Testing net (#0)
I0421 23:27:47.761207  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77227
I0421 23:27:47.761256  1684 solver.cpp:404]     Test net output #1: loss = 0.875665 (* 1 = 0.875665 loss)
I0421 23:27:47.762233  1684 solver.cpp:228] Iteration 17000, loss = 0.0719355
I0421 23:27:47.762265  1684 solver.cpp:244]     Train net output #0: loss = 0.0719353 (* 1 = 0.0719353 loss)
I0421 23:27:47.762274  1684 sgd_solver.cpp:106] Iteration 17000, lr = 2.51625e-06
I0421 23:27:49.342198  1684 solver.cpp:337] Iteration 17500, Testing net (#0)
I0421 23:27:51.344157  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7802
I0421 23:27:51.344230  1684 solver.cpp:404]     Test net output #1: loss = 0.866952 (* 1 = 0.866952 loss)
I0421 23:27:52.880414  1684 solver.cpp:337] Iteration 18000, Testing net (#0)
I0421 23:27:54.866225  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7752
I0421 23:27:54.866267  1684 solver.cpp:404]     Test net output #1: loss = 0.871789 (* 1 = 0.871789 loss)
I0421 23:27:54.869073  1684 solver.cpp:228] Iteration 18000, loss = 0.956133
I0421 23:27:54.869091  1684 solver.cpp:244]     Train net output #0: loss = 0.956132 (* 1 = 0.956132 loss)
I0421 23:27:54.869099  1684 sgd_solver.cpp:106] Iteration 18000, lr = 2.44854e-06
I0421 23:27:56.352257  1684 solver.cpp:337] Iteration 18500, Testing net (#0)
I0421 23:27:58.335609  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77129
I0421 23:27:58.335650  1684 solver.cpp:404]     Test net output #1: loss = 0.875575 (* 1 = 0.875575 loss)
I0421 23:27:59.867300  1684 solver.cpp:337] Iteration 19000, Testing net (#0)
I0421 23:28:01.857578  1684 solver.cpp:404]     Test net output #0: accuracy = 0.780209
I0421 23:28:01.857635  1684 solver.cpp:404]     Test net output #1: loss = 0.864991 (* 1 = 0.864991 loss)
I0421 23:28:01.858671  1684 solver.cpp:228] Iteration 19000, loss = 0.470339
I0421 23:28:01.858693  1684 solver.cpp:244]     Train net output #0: loss = 0.470338 (* 1 = 0.470338 loss)
I0421 23:28:01.858718  1684 sgd_solver.cpp:106] Iteration 19000, lr = 2.38494e-06
I0421 23:28:03.432471  1684 solver.cpp:337] Iteration 19500, Testing net (#0)
I0421 23:28:05.386869  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77723
I0421 23:28:05.386940  1684 solver.cpp:404]     Test net output #1: loss = 0.865876 (* 1 = 0.865876 loss)
I0421 23:28:06.912416  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.caffemodel
I0421 23:28:06.919073  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.solverstate
I0421 23:28:06.921844  1684 solver.cpp:337] Iteration 20000, Testing net (#0)
I0421 23:28:08.891130  1684 solver.cpp:404]     Test net output #0: accuracy = 0.7772
I0421 23:28:08.891187  1684 solver.cpp:404]     Test net output #1: loss = 0.874557 (* 1 = 0.874557 loss)
I0421 23:28:08.892086  1684 solver.cpp:228] Iteration 20000, loss = 0.196666
I0421 23:28:08.892105  1684 solver.cpp:244]     Train net output #0: loss = 0.196665 (* 1 = 0.196665 loss)
I0421 23:28:08.892117  1684 sgd_solver.cpp:106] Iteration 20000, lr = 2.32506e-06
I0421 23:28:10.382153  1684 solver.cpp:337] Iteration 20500, Testing net (#0)
I0421 23:28:11.615458  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:28:12.343205  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78217
I0421 23:28:12.343266  1684 solver.cpp:404]     Test net output #1: loss = 0.863061 (* 1 = 0.863061 loss)
I0421 23:28:13.891449  1684 solver.cpp:337] Iteration 21000, Testing net (#0)
I0421 23:28:15.873270  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78416
I0421 23:28:15.873312  1684 solver.cpp:404]     Test net output #1: loss = 0.860302 (* 1 = 0.860302 loss)
I0421 23:28:15.874310  1684 solver.cpp:228] Iteration 21000, loss = 0.286799
I0421 23:28:15.874330  1684 solver.cpp:244]     Train net output #0: loss = 0.286799 (* 1 = 0.286799 loss)
I0421 23:28:15.874341  1684 sgd_solver.cpp:106] Iteration 21000, lr = 2.26858e-06
I0421 23:28:17.455085  1684 solver.cpp:337] Iteration 21500, Testing net (#0)
I0421 23:28:19.442509  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77428
I0421 23:28:19.442610  1684 solver.cpp:404]     Test net output #1: loss = 0.875226 (* 1 = 0.875226 loss)
I0421 23:28:20.965548  1684 solver.cpp:337] Iteration 22000, Testing net (#0)
I0421 23:28:22.927014  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78015
I0421 23:28:22.927197  1684 solver.cpp:404]     Test net output #1: loss = 0.864008 (* 1 = 0.864008 loss)
I0421 23:28:22.928155  1684 solver.cpp:228] Iteration 22000, loss = 0.136066
I0421 23:28:22.928177  1684 solver.cpp:244]     Train net output #0: loss = 0.136066 (* 1 = 0.136066 loss)
I0421 23:28:22.928227  1684 sgd_solver.cpp:106] Iteration 22000, lr = 2.2152e-06
I0421 23:28:24.410234  1684 solver.cpp:337] Iteration 22500, Testing net (#0)
I0421 23:28:26.400112  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78715
I0421 23:28:26.400149  1684 solver.cpp:404]     Test net output #1: loss = 0.85808 (* 1 = 0.85808 loss)
I0421 23:28:27.949509  1684 solver.cpp:337] Iteration 23000, Testing net (#0)
I0421 23:28:29.925932  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77527
I0421 23:28:29.925971  1684 solver.cpp:404]     Test net output #1: loss = 0.873051 (* 1 = 0.873051 loss)
I0421 23:28:29.926959  1684 solver.cpp:228] Iteration 23000, loss = 0.555774
I0421 23:28:29.926975  1684 solver.cpp:244]     Train net output #0: loss = 0.555773 (* 1 = 0.555773 loss)
I0421 23:28:29.926983  1684 sgd_solver.cpp:106] Iteration 23000, lr = 2.16466e-06
I0421 23:28:31.512886  1684 solver.cpp:337] Iteration 23500, Testing net (#0)
I0421 23:28:33.450647  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78314
I0421 23:28:33.450712  1684 solver.cpp:404]     Test net output #1: loss = 0.86299 (* 1 = 0.86299 loss)
I0421 23:28:34.971352  1684 solver.cpp:337] Iteration 24000, Testing net (#0)
I0421 23:28:36.961014  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78218
I0421 23:28:36.961099  1684 solver.cpp:404]     Test net output #1: loss = 0.856048 (* 1 = 0.856048 loss)
I0421 23:28:36.962047  1684 solver.cpp:228] Iteration 24000, loss = 0.65468
I0421 23:28:36.962065  1684 solver.cpp:244]     Train net output #0: loss = 0.654679 (* 1 = 0.654679 loss)
I0421 23:28:36.962076  1684 sgd_solver.cpp:106] Iteration 24000, lr = 2.11674e-06
I0421 23:28:38.453717  1684 solver.cpp:337] Iteration 24500, Testing net (#0)
I0421 23:28:40.469036  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77622
I0421 23:28:40.469074  1684 solver.cpp:404]     Test net output #1: loss = 0.866668 (* 1 = 0.866668 loss)
I0421 23:28:42.034483  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_25000.caffemodel
I0421 23:28:42.041375  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_25000.solverstate
I0421 23:28:42.044324  1684 solver.cpp:337] Iteration 25000, Testing net (#0)
I0421 23:28:43.987292  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78417
I0421 23:28:43.987330  1684 solver.cpp:404]     Test net output #1: loss = 0.861233 (* 1 = 0.861233 loss)
I0421 23:28:43.988262  1684 solver.cpp:228] Iteration 25000, loss = 0.0820295
I0421 23:28:43.988284  1684 solver.cpp:244]     Train net output #0: loss = 0.082029 (* 1 = 0.082029 loss)
I0421 23:28:43.988293  1684 sgd_solver.cpp:106] Iteration 25000, lr = 2.07121e-06
I0421 23:28:45.567562  1684 solver.cpp:337] Iteration 25500, Testing net (#0)
I0421 23:28:47.547868  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78217
I0421 23:28:47.547950  1684 solver.cpp:404]     Test net output #1: loss = 0.856963 (* 1 = 0.856963 loss)
I0421 23:28:49.069242  1684 solver.cpp:337] Iteration 26000, Testing net (#0)
I0421 23:28:49.162408  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:28:51.053968  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77725
I0421 23:28:51.054028  1684 solver.cpp:404]     Test net output #1: loss = 0.864504 (* 1 = 0.864504 loss)
I0421 23:28:51.055413  1684 solver.cpp:228] Iteration 26000, loss = 0.107266
I0421 23:28:51.055450  1684 solver.cpp:244]     Train net output #0: loss = 0.107266 (* 1 = 0.107266 loss)
I0421 23:28:51.055477  1684 sgd_solver.cpp:106] Iteration 26000, lr = 2.02791e-06
I0421 23:28:52.543247  1684 solver.cpp:337] Iteration 26500, Testing net (#0)
I0421 23:28:54.513865  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78515
I0421 23:28:54.514093  1684 solver.cpp:404]     Test net output #1: loss = 0.861974 (* 1 = 0.861974 loss)
I0421 23:28:56.064016  1684 solver.cpp:337] Iteration 27000, Testing net (#0)
I0421 23:28:58.031685  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78121
I0421 23:28:58.031723  1684 solver.cpp:404]     Test net output #1: loss = 0.855125 (* 1 = 0.855125 loss)
I0421 23:28:58.032877  1684 solver.cpp:228] Iteration 27000, loss = 0.250523
I0421 23:28:58.032902  1684 solver.cpp:244]     Train net output #0: loss = 0.250522 (* 1 = 0.250522 loss)
I0421 23:28:58.032915  1684 sgd_solver.cpp:106] Iteration 27000, lr = 1.98666e-06
I0421 23:28:59.608547  1684 solver.cpp:337] Iteration 27500, Testing net (#0)
I0421 23:29:01.546533  1684 solver.cpp:404]     Test net output #0: accuracy = 0.77723
I0421 23:29:01.546574  1684 solver.cpp:404]     Test net output #1: loss = 0.860787 (* 1 = 0.860787 loss)
I0421 23:29:03.080559  1684 solver.cpp:337] Iteration 28000, Testing net (#0)
I0421 23:29:05.027652  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78614
I0421 23:29:05.027705  1684 solver.cpp:404]     Test net output #1: loss = 0.861537 (* 1 = 0.861537 loss)
I0421 23:29:05.030740  1684 solver.cpp:228] Iteration 28000, loss = 0.249363
I0421 23:29:05.030762  1684 solver.cpp:244]     Train net output #0: loss = 0.249362 (* 1 = 0.249362 loss)
I0421 23:29:05.030807  1684 sgd_solver.cpp:106] Iteration 28000, lr = 1.94732e-06
I0421 23:29:06.521275  1684 solver.cpp:337] Iteration 28500, Testing net (#0)
I0421 23:29:08.493258  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78217
I0421 23:29:08.493311  1684 solver.cpp:404]     Test net output #1: loss = 0.853725 (* 1 = 0.853725 loss)
I0421 23:29:10.034529  1684 solver.cpp:337] Iteration 29000, Testing net (#0)
I0421 23:29:11.901062  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78019
I0421 23:29:11.901099  1684 solver.cpp:404]     Test net output #1: loss = 0.856936 (* 1 = 0.856936 loss)
I0421 23:29:11.902043  1684 solver.cpp:228] Iteration 29000, loss = 0.898367
I0421 23:29:11.902061  1684 solver.cpp:244]     Train net output #0: loss = 0.898366 (* 1 = 0.898366 loss)
I0421 23:29:11.902070  1684 sgd_solver.cpp:106] Iteration 29000, lr = 1.90975e-06
I0421 23:29:13.476347  1684 solver.cpp:337] Iteration 29500, Testing net (#0)
I0421 23:29:15.433478  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78517
I0421 23:29:15.433532  1684 solver.cpp:404]     Test net output #1: loss = 0.861834 (* 1 = 0.861834 loss)
I0421 23:29:16.957384  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30000.caffemodel
I0421 23:29:16.963944  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30000.solverstate
I0421 23:29:16.966625  1684 solver.cpp:337] Iteration 30000, Testing net (#0)
I0421 23:29:18.950791  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78515
I0421 23:29:18.950848  1684 solver.cpp:404]     Test net output #1: loss = 0.852887 (* 1 = 0.852887 loss)
I0421 23:29:18.951907  1684 solver.cpp:228] Iteration 30000, loss = 0.739113
I0421 23:29:18.951925  1684 solver.cpp:244]     Train net output #0: loss = 0.739112 (* 1 = 0.739112 loss)
I0421 23:29:18.951949  1684 sgd_solver.cpp:106] Iteration 30000, lr = 1.87383e-06
I0421 23:29:20.446777  1684 solver.cpp:337] Iteration 30500, Testing net (#0)
I0421 23:29:22.390955  1684 solver.cpp:404]     Test net output #0: accuracy = 0.78514
I0421 23:29:22.390991  1684 solver.cpp:404]     Test net output #1: loss = 0.853533 (* 1 = 0.853533 loss)
I0421 23:29:23.979367  1684 solver.cpp:337] Iteration 31000, Testing net (#0)
I0421 23:29:24.843372  1684 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:29:24.863559  1684 solver.cpp:386] Test interrupted.
I0421 23:29:24.863606  1684 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_31000.caffemodel
I0421 23:29:24.868701  1684 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_31000.solverstate
I0421 23:29:24.871531  1684 solver.cpp:301] Optimization stopped early.
I0421 23:29:24.871542  1684 caffe.cpp:222] Optimization Done.
I0421 23:29:33.945931  2056 caffe.cpp:185] Using GPUs 0
I0421 23:29:33.950685  2056 caffe.cpp:190] GPU 0: GeForce GTX 980M
I0421 23:29:34.112074  2056 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 5.3e-06
display: 1000
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.00028
snapshot: 5000
snapshot_prefix: "/home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/"
solver_mode: GPU
device_id: 0
net: "/home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt"
I0421 23:29:34.114351  2056 solver.cpp:91] Creating training net from net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:29:34.114677  2056 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0421 23:29:34.114693  2056 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0421 23:29:34.114825  2056 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/train"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:29:34.114925  2056 layer_factory.hpp:77] Creating layer mnist
I0421 23:29:34.115417  2056 net.cpp:91] Creating Layer mnist
I0421 23:29:34.115455  2056 net.cpp:399] mnist -> data
I0421 23:29:34.115497  2056 net.cpp:399] mnist -> label
I0421 23:29:34.116240  2067 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/train
I0421 23:29:34.123126  2056 data_layer.cpp:41] output data size: 64,1,28,28
I0421 23:29:34.123950  2056 net.cpp:141] Setting up mnist
I0421 23:29:34.123970  2056 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0421 23:29:34.123993  2056 net.cpp:148] Top shape: 64 (64)
I0421 23:29:34.124011  2056 net.cpp:156] Memory required for data: 200960
I0421 23:29:34.124020  2056 layer_factory.hpp:77] Creating layer conv1
I0421 23:29:34.124053  2056 net.cpp:91] Creating Layer conv1
I0421 23:29:34.124059  2056 net.cpp:425] conv1 <- data
I0421 23:29:34.124073  2056 net.cpp:399] conv1 -> conv1
I0421 23:29:34.250284  2056 net.cpp:141] Setting up conv1
I0421 23:29:34.250314  2056 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0421 23:29:34.250337  2056 net.cpp:156] Memory required for data: 3150080
I0421 23:29:34.250408  2056 layer_factory.hpp:77] Creating layer pool1
I0421 23:29:34.250421  2056 net.cpp:91] Creating Layer pool1
I0421 23:29:34.250425  2056 net.cpp:425] pool1 <- conv1
I0421 23:29:34.250432  2056 net.cpp:399] pool1 -> pool1
I0421 23:29:34.250488  2056 net.cpp:141] Setting up pool1
I0421 23:29:34.250495  2056 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0421 23:29:34.250499  2056 net.cpp:156] Memory required for data: 3887360
I0421 23:29:34.250502  2056 layer_factory.hpp:77] Creating layer conv2
I0421 23:29:34.250511  2056 net.cpp:91] Creating Layer conv2
I0421 23:29:34.250515  2056 net.cpp:425] conv2 <- pool1
I0421 23:29:34.250520  2056 net.cpp:399] conv2 -> conv2
I0421 23:29:34.251323  2056 net.cpp:141] Setting up conv2
I0421 23:29:34.251335  2056 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0421 23:29:34.251339  2056 net.cpp:156] Memory required for data: 4706560
I0421 23:29:34.251365  2056 layer_factory.hpp:77] Creating layer pool2
I0421 23:29:34.251373  2056 net.cpp:91] Creating Layer pool2
I0421 23:29:34.251376  2056 net.cpp:425] pool2 <- conv2
I0421 23:29:34.251381  2056 net.cpp:399] pool2 -> pool2
I0421 23:29:34.251425  2056 net.cpp:141] Setting up pool2
I0421 23:29:34.251442  2056 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0421 23:29:34.251446  2056 net.cpp:156] Memory required for data: 4911360
I0421 23:29:34.251449  2056 layer_factory.hpp:77] Creating layer ip1
I0421 23:29:34.251477  2056 net.cpp:91] Creating Layer ip1
I0421 23:29:34.251480  2056 net.cpp:425] ip1 <- pool2
I0421 23:29:34.251485  2056 net.cpp:399] ip1 -> ip1
I0421 23:29:34.254061  2056 net.cpp:141] Setting up ip1
I0421 23:29:34.254081  2056 net.cpp:148] Top shape: 64 500 (32000)
I0421 23:29:34.254084  2056 net.cpp:156] Memory required for data: 5039360
I0421 23:29:34.254111  2056 layer_factory.hpp:77] Creating layer relu1
I0421 23:29:34.254129  2056 net.cpp:91] Creating Layer relu1
I0421 23:29:34.254133  2056 net.cpp:425] relu1 <- ip1
I0421 23:29:34.254138  2056 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:29:34.254362  2056 net.cpp:141] Setting up relu1
I0421 23:29:34.254372  2056 net.cpp:148] Top shape: 64 500 (32000)
I0421 23:29:34.254376  2056 net.cpp:156] Memory required for data: 5167360
I0421 23:29:34.254397  2056 layer_factory.hpp:77] Creating layer ip2
I0421 23:29:34.254403  2056 net.cpp:91] Creating Layer ip2
I0421 23:29:34.254407  2056 net.cpp:425] ip2 <- ip1
I0421 23:29:34.254412  2056 net.cpp:399] ip2 -> ip2
I0421 23:29:34.254786  2056 net.cpp:141] Setting up ip2
I0421 23:29:34.254796  2056 net.cpp:148] Top shape: 64 101 (6464)
I0421 23:29:34.254798  2056 net.cpp:156] Memory required for data: 5193216
I0421 23:29:34.254822  2056 layer_factory.hpp:77] Creating layer loss
I0421 23:29:34.254828  2056 net.cpp:91] Creating Layer loss
I0421 23:29:34.254832  2056 net.cpp:425] loss <- ip2
I0421 23:29:34.254837  2056 net.cpp:425] loss <- label
I0421 23:29:34.254842  2056 net.cpp:399] loss -> loss
I0421 23:29:34.254868  2056 layer_factory.hpp:77] Creating layer loss
I0421 23:29:34.255460  2056 net.cpp:141] Setting up loss
I0421 23:29:34.255470  2056 net.cpp:148] Top shape: (1)
I0421 23:29:34.255475  2056 net.cpp:151]     with loss weight 1
I0421 23:29:34.255524  2056 net.cpp:156] Memory required for data: 5193220
I0421 23:29:34.255527  2056 net.cpp:217] loss needs backward computation.
I0421 23:29:34.255532  2056 net.cpp:217] ip2 needs backward computation.
I0421 23:29:34.255537  2056 net.cpp:217] relu1 needs backward computation.
I0421 23:29:34.255539  2056 net.cpp:217] ip1 needs backward computation.
I0421 23:29:34.255543  2056 net.cpp:217] pool2 needs backward computation.
I0421 23:29:34.255547  2056 net.cpp:217] conv2 needs backward computation.
I0421 23:29:34.255550  2056 net.cpp:217] pool1 needs backward computation.
I0421 23:29:34.255554  2056 net.cpp:217] conv1 needs backward computation.
I0421 23:29:34.255563  2056 net.cpp:219] mnist does not need backward computation.
I0421 23:29:34.255566  2056 net.cpp:261] This network produces output loss
I0421 23:29:34.255574  2056 net.cpp:274] Network initialization done.
I0421 23:29:34.255964  2056 solver.cpp:181] Creating test net (#0) specified by net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:29:34.256005  2056 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0421 23:29:34.256147  2056 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/test"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:29:34.256229  2056 layer_factory.hpp:77] Creating layer mnist
I0421 23:29:34.256647  2056 net.cpp:91] Creating Layer mnist
I0421 23:29:34.256655  2056 net.cpp:399] mnist -> data
I0421 23:29:34.256681  2056 net.cpp:399] mnist -> label
I0421 23:29:34.257325  2070 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/test
I0421 23:29:34.257586  2056 data_layer.cpp:41] output data size: 1000,1,28,28
I0421 23:29:34.261629  2056 net.cpp:141] Setting up mnist
I0421 23:29:34.261688  2056 net.cpp:148] Top shape: 1000 1 28 28 (784000)
I0421 23:29:34.261713  2056 net.cpp:148] Top shape: 1000 (1000)
I0421 23:29:34.261734  2056 net.cpp:156] Memory required for data: 3140000
I0421 23:29:34.261740  2056 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0421 23:29:34.261764  2056 net.cpp:91] Creating Layer label_mnist_1_split
I0421 23:29:34.261768  2056 net.cpp:425] label_mnist_1_split <- label
I0421 23:29:34.261790  2056 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I0421 23:29:34.261800  2056 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I0421 23:29:34.261837  2056 net.cpp:141] Setting up label_mnist_1_split
I0421 23:29:34.261857  2056 net.cpp:148] Top shape: 1000 (1000)
I0421 23:29:34.261862  2056 net.cpp:148] Top shape: 1000 (1000)
I0421 23:29:34.261864  2056 net.cpp:156] Memory required for data: 3148000
I0421 23:29:34.261868  2056 layer_factory.hpp:77] Creating layer conv1
I0421 23:29:34.261922  2056 net.cpp:91] Creating Layer conv1
I0421 23:29:34.261927  2056 net.cpp:425] conv1 <- data
I0421 23:29:34.261934  2056 net.cpp:399] conv1 -> conv1
I0421 23:29:34.263245  2056 net.cpp:141] Setting up conv1
I0421 23:29:34.263262  2056 net.cpp:148] Top shape: 1000 20 24 24 (11520000)
I0421 23:29:34.263267  2056 net.cpp:156] Memory required for data: 49228000
I0421 23:29:34.263294  2056 layer_factory.hpp:77] Creating layer pool1
I0421 23:29:34.263301  2056 net.cpp:91] Creating Layer pool1
I0421 23:29:34.263306  2056 net.cpp:425] pool1 <- conv1
I0421 23:29:34.263325  2056 net.cpp:399] pool1 -> pool1
I0421 23:29:34.263388  2056 net.cpp:141] Setting up pool1
I0421 23:29:34.263399  2056 net.cpp:148] Top shape: 1000 20 12 12 (2880000)
I0421 23:29:34.263406  2056 net.cpp:156] Memory required for data: 60748000
I0421 23:29:34.263411  2056 layer_factory.hpp:77] Creating layer conv2
I0421 23:29:34.263433  2056 net.cpp:91] Creating Layer conv2
I0421 23:29:34.263437  2056 net.cpp:425] conv2 <- pool1
I0421 23:29:34.263443  2056 net.cpp:399] conv2 -> conv2
I0421 23:29:34.264458  2056 net.cpp:141] Setting up conv2
I0421 23:29:34.264472  2056 net.cpp:148] Top shape: 1000 50 8 8 (3200000)
I0421 23:29:34.264477  2056 net.cpp:156] Memory required for data: 73548000
I0421 23:29:34.264485  2056 layer_factory.hpp:77] Creating layer pool2
I0421 23:29:34.264492  2056 net.cpp:91] Creating Layer pool2
I0421 23:29:34.264497  2056 net.cpp:425] pool2 <- conv2
I0421 23:29:34.264502  2056 net.cpp:399] pool2 -> pool2
I0421 23:29:34.264539  2056 net.cpp:141] Setting up pool2
I0421 23:29:34.264546  2056 net.cpp:148] Top shape: 1000 50 4 4 (800000)
I0421 23:29:34.264550  2056 net.cpp:156] Memory required for data: 76748000
I0421 23:29:34.264554  2056 layer_factory.hpp:77] Creating layer ip1
I0421 23:29:34.264561  2056 net.cpp:91] Creating Layer ip1
I0421 23:29:34.264565  2056 net.cpp:425] ip1 <- pool2
I0421 23:29:34.264572  2056 net.cpp:399] ip1 -> ip1
I0421 23:29:34.267788  2056 net.cpp:141] Setting up ip1
I0421 23:29:34.267805  2056 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:29:34.267810  2056 net.cpp:156] Memory required for data: 78748000
I0421 23:29:34.267822  2056 layer_factory.hpp:77] Creating layer relu1
I0421 23:29:34.267830  2056 net.cpp:91] Creating Layer relu1
I0421 23:29:34.267835  2056 net.cpp:425] relu1 <- ip1
I0421 23:29:34.267841  2056 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:29:34.268110  2056 net.cpp:141] Setting up relu1
I0421 23:29:34.268123  2056 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:29:34.268129  2056 net.cpp:156] Memory required for data: 80748000
I0421 23:29:34.268133  2056 layer_factory.hpp:77] Creating layer ip2
I0421 23:29:34.268142  2056 net.cpp:91] Creating Layer ip2
I0421 23:29:34.268147  2056 net.cpp:425] ip2 <- ip1
I0421 23:29:34.268157  2056 net.cpp:399] ip2 -> ip2
I0421 23:29:34.268942  2056 net.cpp:141] Setting up ip2
I0421 23:29:34.268954  2056 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:29:34.268959  2056 net.cpp:156] Memory required for data: 81152000
I0421 23:29:34.268966  2056 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0421 23:29:34.268973  2056 net.cpp:91] Creating Layer ip2_ip2_0_split
I0421 23:29:34.268978  2056 net.cpp:425] ip2_ip2_0_split <- ip2
I0421 23:29:34.268983  2056 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0421 23:29:34.268990  2056 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0421 23:29:34.269026  2056 net.cpp:141] Setting up ip2_ip2_0_split
I0421 23:29:34.269034  2056 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:29:34.269040  2056 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:29:34.269044  2056 net.cpp:156] Memory required for data: 81960000
I0421 23:29:34.269048  2056 layer_factory.hpp:77] Creating layer accuracy
I0421 23:29:34.269054  2056 net.cpp:91] Creating Layer accuracy
I0421 23:29:34.269059  2056 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0421 23:29:34.269064  2056 net.cpp:425] accuracy <- label_mnist_1_split_0
I0421 23:29:34.269070  2056 net.cpp:399] accuracy -> accuracy
I0421 23:29:34.269084  2056 net.cpp:141] Setting up accuracy
I0421 23:29:34.269090  2056 net.cpp:148] Top shape: (1)
I0421 23:29:34.269095  2056 net.cpp:156] Memory required for data: 81960004
I0421 23:29:34.269106  2056 layer_factory.hpp:77] Creating layer loss
I0421 23:29:34.269124  2056 net.cpp:91] Creating Layer loss
I0421 23:29:34.269131  2056 net.cpp:425] loss <- ip2_ip2_0_split_1
I0421 23:29:34.269136  2056 net.cpp:425] loss <- label_mnist_1_split_1
I0421 23:29:34.269142  2056 net.cpp:399] loss -> loss
I0421 23:29:34.269150  2056 layer_factory.hpp:77] Creating layer loss
I0421 23:29:34.269815  2056 net.cpp:141] Setting up loss
I0421 23:29:34.269829  2056 net.cpp:148] Top shape: (1)
I0421 23:29:34.269834  2056 net.cpp:151]     with loss weight 1
I0421 23:29:34.269842  2056 net.cpp:156] Memory required for data: 81960008
I0421 23:29:34.269847  2056 net.cpp:217] loss needs backward computation.
I0421 23:29:34.269852  2056 net.cpp:219] accuracy does not need backward computation.
I0421 23:29:34.269857  2056 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0421 23:29:34.269861  2056 net.cpp:217] ip2 needs backward computation.
I0421 23:29:34.269866  2056 net.cpp:217] relu1 needs backward computation.
I0421 23:29:34.269870  2056 net.cpp:217] ip1 needs backward computation.
I0421 23:29:34.269875  2056 net.cpp:217] pool2 needs backward computation.
I0421 23:29:34.269879  2056 net.cpp:217] conv2 needs backward computation.
I0421 23:29:34.269883  2056 net.cpp:217] pool1 needs backward computation.
I0421 23:29:34.269888  2056 net.cpp:217] conv1 needs backward computation.
I0421 23:29:34.269893  2056 net.cpp:219] label_mnist_1_split does not need backward computation.
I0421 23:29:34.269898  2056 net.cpp:219] mnist does not need backward computation.
I0421 23:29:34.269902  2056 net.cpp:261] This network produces output accuracy
I0421 23:29:34.269907  2056 net.cpp:261] This network produces output loss
I0421 23:29:34.269920  2056 net.cpp:274] Network initialization done.
I0421 23:29:34.269965  2056 solver.cpp:60] Solver scaffolding done.
I0421 23:29:34.270251  2056 caffe.cpp:129] Finetuning from ./snapshot/_iter_30000.caffemodel
I0421 23:29:34.276504  2056 caffe.cpp:219] Starting Optimization
I0421 23:29:34.276528  2056 solver.cpp:279] Solving LeNet
I0421 23:29:34.276533  2056 solver.cpp:280] Learning Rate Policy: inv
I0421 23:29:34.276887  2056 solver.cpp:337] Iteration 0, Testing net (#0)
I0421 23:29:34.277506  2056 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:29:36.286015  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78516
I0421 23:29:36.286051  2056 solver.cpp:404]     Test net output #1: loss = 0.852909 (* 1 = 0.852909 loss)
I0421 23:29:36.288522  2056 solver.cpp:228] Iteration 0, loss = 0.435543
I0421 23:29:36.288540  2056 solver.cpp:244]     Train net output #0: loss = 0.435543 (* 1 = 0.435543 loss)
I0421 23:29:36.288565  2056 sgd_solver.cpp:106] Iteration 0, lr = 5.3e-06
I0421 23:29:37.794545  2056 solver.cpp:337] Iteration 500, Testing net (#0)
I0421 23:29:39.776813  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78214
I0421 23:29:39.776870  2056 solver.cpp:404]     Test net output #1: loss = 0.865926 (* 1 = 0.865926 loss)
I0421 23:29:41.329993  2056 solver.cpp:337] Iteration 1000, Testing net (#0)
I0421 23:29:43.299342  2056 solver.cpp:404]     Test net output #0: accuracy = 0.76833
I0421 23:29:43.299396  2056 solver.cpp:404]     Test net output #1: loss = 0.901667 (* 1 = 0.901667 loss)
I0421 23:29:43.300706  2056 solver.cpp:228] Iteration 1000, loss = 0.119934
I0421 23:29:43.300729  2056 solver.cpp:244]     Train net output #0: loss = 0.119934 (* 1 = 0.119934 loss)
I0421 23:29:43.300755  2056 sgd_solver.cpp:106] Iteration 1000, lr = 4.93437e-06
I0421 23:29:44.895473  2056 solver.cpp:337] Iteration 1500, Testing net (#0)
I0421 23:29:46.863697  2056 solver.cpp:404]     Test net output #0: accuracy = 0.77922
I0421 23:29:46.863780  2056 solver.cpp:404]     Test net output #1: loss = 0.869405 (* 1 = 0.869405 loss)
I0421 23:29:48.385115  2056 solver.cpp:337] Iteration 2000, Testing net (#0)
I0421 23:29:50.352181  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78415
I0421 23:29:50.352237  2056 solver.cpp:404]     Test net output #1: loss = 0.852796 (* 1 = 0.852796 loss)
I0421 23:29:50.353240  2056 solver.cpp:228] Iteration 2000, loss = 0.0283834
I0421 23:29:50.353260  2056 solver.cpp:244]     Train net output #0: loss = 0.0283837 (* 1 = 0.0283837 loss)
I0421 23:29:50.353271  2056 sgd_solver.cpp:106] Iteration 2000, lr = 4.62264e-06
I0421 23:29:51.842353  2056 solver.cpp:337] Iteration 2500, Testing net (#0)
I0421 23:29:53.815042  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7713
I0421 23:29:53.815101  2056 solver.cpp:404]     Test net output #1: loss = 0.889321 (* 1 = 0.889321 loss)
I0421 23:29:55.371852  2056 solver.cpp:337] Iteration 3000, Testing net (#0)
I0421 23:29:57.329780  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78214
I0421 23:29:57.329818  2056 solver.cpp:404]     Test net output #1: loss = 0.864553 (* 1 = 0.864553 loss)
I0421 23:29:57.330791  2056 solver.cpp:228] Iteration 3000, loss = 0.245476
I0421 23:29:57.330823  2056 solver.cpp:244]     Train net output #0: loss = 0.245476 (* 1 = 0.245476 loss)
I0421 23:29:57.330832  2056 sgd_solver.cpp:106] Iteration 3000, lr = 4.3533e-06
I0421 23:29:58.916510  2056 solver.cpp:337] Iteration 3500, Testing net (#0)
I0421 23:30:00.891293  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78616
I0421 23:30:00.891350  2056 solver.cpp:404]     Test net output #1: loss = 0.847951 (* 1 = 0.847951 loss)
I0421 23:30:02.420414  2056 solver.cpp:337] Iteration 4000, Testing net (#0)
I0421 23:30:04.379272  2056 solver.cpp:404]     Test net output #0: accuracy = 0.77724
I0421 23:30:04.379444  2056 solver.cpp:404]     Test net output #1: loss = 0.871865 (* 1 = 0.871865 loss)
I0421 23:30:04.380540  2056 solver.cpp:228] Iteration 4000, loss = 0.160504
I0421 23:30:04.380564  2056 solver.cpp:244]     Train net output #0: loss = 0.160504 (* 1 = 0.160504 loss)
I0421 23:30:04.380578  2056 sgd_solver.cpp:106] Iteration 4000, lr = 4.11794e-06
I0421 23:30:05.883119  2056 solver.cpp:337] Iteration 4500, Testing net (#0)
I0421 23:30:07.849696  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78616
I0421 23:30:07.849755  2056 solver.cpp:404]     Test net output #1: loss = 0.859979 (* 1 = 0.859979 loss)
I0421 23:30:09.404389  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.caffemodel
I0421 23:30:09.412395  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.solverstate
I0421 23:30:09.415091  2056 solver.cpp:337] Iteration 5000, Testing net (#0)
I0421 23:30:10.132300  2056 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:30:11.303556  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78911
I0421 23:30:11.303627  2056 solver.cpp:404]     Test net output #1: loss = 0.852415 (* 1 = 0.852415 loss)
I0421 23:30:11.306990  2056 solver.cpp:228] Iteration 5000, loss = 0.542481
I0421 23:30:11.307010  2056 solver.cpp:244]     Train net output #0: loss = 0.542481 (* 1 = 0.542481 loss)
I0421 23:30:11.307019  2056 sgd_solver.cpp:106] Iteration 5000, lr = 3.91028e-06
I0421 23:30:12.878373  2056 solver.cpp:337] Iteration 5500, Testing net (#0)
I0421 23:30:14.876651  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7802
I0421 23:30:14.876708  2056 solver.cpp:404]     Test net output #1: loss = 0.86664 (* 1 = 0.86664 loss)
I0421 23:30:16.395002  2056 solver.cpp:337] Iteration 6000, Testing net (#0)
I0421 23:30:18.359055  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7812
I0421 23:30:18.359117  2056 solver.cpp:404]     Test net output #1: loss = 0.860305 (* 1 = 0.860305 loss)
I0421 23:30:18.360141  2056 solver.cpp:228] Iteration 6000, loss = 0.0531001
I0421 23:30:18.360159  2056 solver.cpp:244]     Train net output #0: loss = 0.0531004 (* 1 = 0.0531004 loss)
I0421 23:30:18.360168  2056 sgd_solver.cpp:106] Iteration 6000, lr = 3.72551e-06
I0421 23:30:19.856755  2056 solver.cpp:337] Iteration 6500, Testing net (#0)
I0421 23:30:21.773378  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7871
I0421 23:30:21.773468  2056 solver.cpp:404]     Test net output #1: loss = 0.848745 (* 1 = 0.848745 loss)
I0421 23:30:23.337435  2056 solver.cpp:337] Iteration 7000, Testing net (#0)
I0421 23:30:25.263074  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78022
I0421 23:30:25.263129  2056 solver.cpp:404]     Test net output #1: loss = 0.858256 (* 1 = 0.858256 loss)
I0421 23:30:25.264075  2056 solver.cpp:228] Iteration 7000, loss = 0.855097
I0421 23:30:25.264096  2056 solver.cpp:244]     Train net output #0: loss = 0.855098 (* 1 = 0.855098 loss)
I0421 23:30:25.264114  2056 sgd_solver.cpp:106] Iteration 7000, lr = 3.55991e-06
I0421 23:30:26.840288  2056 solver.cpp:337] Iteration 7500, Testing net (#0)
I0421 23:30:28.769347  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78022
I0421 23:30:28.769400  2056 solver.cpp:404]     Test net output #1: loss = 0.859991 (* 1 = 0.859991 loss)
I0421 23:30:30.294864  2056 solver.cpp:337] Iteration 8000, Testing net (#0)
I0421 23:30:32.237557  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78715
I0421 23:30:32.237637  2056 solver.cpp:404]     Test net output #1: loss = 0.845919 (* 1 = 0.845919 loss)
I0421 23:30:32.238685  2056 solver.cpp:228] Iteration 8000, loss = 0.0101561
I0421 23:30:32.238706  2056 solver.cpp:244]     Train net output #0: loss = 0.0101563 (* 1 = 0.0101563 loss)
I0421 23:30:32.238782  2056 sgd_solver.cpp:106] Iteration 8000, lr = 3.41053e-06
I0421 23:30:33.736212  2056 solver.cpp:337] Iteration 8500, Testing net (#0)
I0421 23:30:35.714680  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78615
I0421 23:30:35.714859  2056 solver.cpp:404]     Test net output #1: loss = 0.849346 (* 1 = 0.849346 loss)
I0421 23:30:37.264844  2056 solver.cpp:337] Iteration 9000, Testing net (#0)
I0421 23:30:39.230391  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78514
I0421 23:30:39.230446  2056 solver.cpp:404]     Test net output #1: loss = 0.859058 (* 1 = 0.859058 loss)
I0421 23:30:39.231413  2056 solver.cpp:228] Iteration 9000, loss = 0.319804
I0421 23:30:39.231446  2056 solver.cpp:244]     Train net output #0: loss = 0.319804 (* 1 = 0.319804 loss)
I0421 23:30:39.231477  2056 sgd_solver.cpp:106] Iteration 9000, lr = 3.275e-06
I0421 23:30:40.811136  2056 solver.cpp:337] Iteration 9500, Testing net (#0)
I0421 23:30:42.805928  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78614
I0421 23:30:42.805994  2056 solver.cpp:404]     Test net output #1: loss = 0.845309 (* 1 = 0.845309 loss)
I0421 23:30:44.330078  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.caffemodel
I0421 23:30:44.336674  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.solverstate
I0421 23:30:44.339406  2056 solver.cpp:337] Iteration 10000, Testing net (#0)
I0421 23:30:45.915030  2056 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:30:46.329188  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78513
I0421 23:30:46.329258  2056 solver.cpp:404]     Test net output #1: loss = 0.843299 (* 1 = 0.843299 loss)
I0421 23:30:46.330181  2056 solver.cpp:228] Iteration 10000, loss = 0.272095
I0421 23:30:46.330200  2056 solver.cpp:244]     Train net output #0: loss = 0.272095 (* 1 = 0.272095 loss)
I0421 23:30:46.330209  2056 sgd_solver.cpp:106] Iteration 10000, lr = 3.1514e-06
I0421 23:30:47.826486  2056 solver.cpp:337] Iteration 10500, Testing net (#0)
I0421 23:30:49.808231  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78318
I0421 23:30:49.808284  2056 solver.cpp:404]     Test net output #1: loss = 0.862395 (* 1 = 0.862395 loss)
I0421 23:30:51.366505  2056 solver.cpp:337] Iteration 11000, Testing net (#0)
I0421 23:30:53.337569  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78708
I0421 23:30:53.337646  2056 solver.cpp:404]     Test net output #1: loss = 0.845594 (* 1 = 0.845594 loss)
I0421 23:30:53.338603  2056 solver.cpp:228] Iteration 11000, loss = 0.405964
I0421 23:30:53.338620  2056 solver.cpp:244]     Train net output #0: loss = 0.405965 (* 1 = 0.405965 loss)
I0421 23:30:53.338629  2056 sgd_solver.cpp:106] Iteration 11000, lr = 3.03817e-06
I0421 23:30:54.919073  2056 solver.cpp:337] Iteration 11500, Testing net (#0)
I0421 23:30:56.880406  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78812
I0421 23:30:56.880462  2056 solver.cpp:404]     Test net output #1: loss = 0.83902 (* 1 = 0.83902 loss)
I0421 23:30:58.406222  2056 solver.cpp:337] Iteration 12000, Testing net (#0)
I0421 23:31:00.361588  2056 solver.cpp:404]     Test net output #0: accuracy = 0.77922
I0421 23:31:00.361644  2056 solver.cpp:404]     Test net output #1: loss = 0.859143 (* 1 = 0.859143 loss)
I0421 23:31:00.362586  2056 solver.cpp:228] Iteration 12000, loss = 0.216479
I0421 23:31:00.362618  2056 solver.cpp:244]     Train net output #0: loss = 0.216479 (* 1 = 0.216479 loss)
I0421 23:31:00.362627  2056 sgd_solver.cpp:106] Iteration 12000, lr = 2.93399e-06
I0421 23:31:01.860366  2056 solver.cpp:337] Iteration 12500, Testing net (#0)
I0421 23:31:03.856874  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78812
I0421 23:31:03.856932  2056 solver.cpp:404]     Test net output #1: loss = 0.845258 (* 1 = 0.845258 loss)
I0421 23:31:05.407909  2056 solver.cpp:337] Iteration 13000, Testing net (#0)
I0421 23:31:07.390240  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78908
I0421 23:31:07.390357  2056 solver.cpp:404]     Test net output #1: loss = 0.837852 (* 1 = 0.837852 loss)
I0421 23:31:07.391278  2056 solver.cpp:228] Iteration 13000, loss = 0.795202
I0421 23:31:07.391295  2056 solver.cpp:244]     Train net output #0: loss = 0.795203 (* 1 = 0.795203 loss)
I0421 23:31:07.391304  2056 sgd_solver.cpp:106] Iteration 13000, lr = 2.83779e-06
I0421 23:31:08.965497  2056 solver.cpp:337] Iteration 13500, Testing net (#0)
I0421 23:31:10.891960  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78417
I0421 23:31:10.892024  2056 solver.cpp:404]     Test net output #1: loss = 0.853298 (* 1 = 0.853298 loss)
I0421 23:31:12.418900  2056 solver.cpp:337] Iteration 14000, Testing net (#0)
I0421 23:31:14.375632  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78811
I0421 23:31:14.375684  2056 solver.cpp:404]     Test net output #1: loss = 0.843586 (* 1 = 0.843586 loss)
I0421 23:31:14.376695  2056 solver.cpp:228] Iteration 14000, loss = 0.185525
I0421 23:31:14.376714  2056 solver.cpp:244]     Train net output #0: loss = 0.185525 (* 1 = 0.185525 loss)
I0421 23:31:14.376723  2056 sgd_solver.cpp:106] Iteration 14000, lr = 2.74864e-06
I0421 23:31:15.880121  2056 solver.cpp:337] Iteration 14500, Testing net (#0)
I0421 23:31:17.794540  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7901
I0421 23:31:17.794596  2056 solver.cpp:404]     Test net output #1: loss = 0.838041 (* 1 = 0.838041 loss)
I0421 23:31:19.353162  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.caffemodel
I0421 23:31:19.360047  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.solverstate
I0421 23:31:19.362884  2056 solver.cpp:337] Iteration 15000, Testing net (#0)
I0421 23:31:21.327406  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7891
I0421 23:31:21.327461  2056 solver.cpp:404]     Test net output #1: loss = 0.848831 (* 1 = 0.848831 loss)
I0421 23:31:21.328389  2056 solver.cpp:228] Iteration 15000, loss = 0.109355
I0421 23:31:21.328410  2056 solver.cpp:244]     Train net output #0: loss = 0.109355 (* 1 = 0.109355 loss)
I0421 23:31:21.328421  2056 sgd_solver.cpp:106] Iteration 15000, lr = 2.66576e-06
I0421 23:31:22.908929  2056 solver.cpp:337] Iteration 15500, Testing net (#0)
I0421 23:31:23.362732  2056 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:31:24.961616  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78812
I0421 23:31:24.961657  2056 solver.cpp:404]     Test net output #1: loss = 0.843832 (* 1 = 0.843832 loss)
I0421 23:31:26.489387  2056 solver.cpp:337] Iteration 16000, Testing net (#0)
I0421 23:31:28.522456  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7911
I0421 23:31:28.522508  2056 solver.cpp:404]     Test net output #1: loss = 0.837387 (* 1 = 0.837387 loss)
I0421 23:31:28.523406  2056 solver.cpp:228] Iteration 16000, loss = 0.0130464
I0421 23:31:28.523427  2056 solver.cpp:244]     Train net output #0: loss = 0.0130467 (* 1 = 0.0130467 loss)
I0421 23:31:28.523437  2056 sgd_solver.cpp:106] Iteration 16000, lr = 2.58849e-06
I0421 23:31:30.042888  2056 solver.cpp:337] Iteration 16500, Testing net (#0)
I0421 23:31:31.998761  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78812
I0421 23:31:31.998821  2056 solver.cpp:404]     Test net output #1: loss = 0.845172 (* 1 = 0.845172 loss)
I0421 23:31:33.554587  2056 solver.cpp:337] Iteration 17000, Testing net (#0)
I0421 23:31:35.517962  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78514
I0421 23:31:35.518007  2056 solver.cpp:404]     Test net output #1: loss = 0.844803 (* 1 = 0.844803 loss)
I0421 23:31:35.519218  2056 solver.cpp:228] Iteration 17000, loss = 0.0614063
I0421 23:31:35.519253  2056 solver.cpp:244]     Train net output #0: loss = 0.0614067 (* 1 = 0.0614067 loss)
I0421 23:31:35.519275  2056 sgd_solver.cpp:106] Iteration 17000, lr = 2.51625e-06
I0421 23:31:37.098814  2056 solver.cpp:337] Iteration 17500, Testing net (#0)
I0421 23:31:39.089838  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7901
I0421 23:31:39.090098  2056 solver.cpp:404]     Test net output #1: loss = 0.835655 (* 1 = 0.835655 loss)
I0421 23:31:40.611826  2056 solver.cpp:337] Iteration 18000, Testing net (#0)
I0421 23:31:42.585752  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78709
I0421 23:31:42.585791  2056 solver.cpp:404]     Test net output #1: loss = 0.84138 (* 1 = 0.84138 loss)
I0421 23:31:42.586853  2056 solver.cpp:228] Iteration 18000, loss = 0.915517
I0421 23:31:42.586872  2056 solver.cpp:244]     Train net output #0: loss = 0.915517 (* 1 = 0.915517 loss)
I0421 23:31:42.586896  2056 sgd_solver.cpp:106] Iteration 18000, lr = 2.44854e-06
I0421 23:31:44.083600  2056 solver.cpp:337] Iteration 18500, Testing net (#0)
I0421 23:31:46.073597  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78713
I0421 23:31:46.073659  2056 solver.cpp:404]     Test net output #1: loss = 0.845779 (* 1 = 0.845779 loss)
I0421 23:31:47.635480  2056 solver.cpp:337] Iteration 19000, Testing net (#0)
I0421 23:31:49.606886  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78813
I0421 23:31:49.606959  2056 solver.cpp:404]     Test net output #1: loss = 0.835017 (* 1 = 0.835017 loss)
I0421 23:31:49.607934  2056 solver.cpp:228] Iteration 19000, loss = 0.460677
I0421 23:31:49.607969  2056 solver.cpp:244]     Train net output #0: loss = 0.460677 (* 1 = 0.460677 loss)
I0421 23:31:49.607980  2056 sgd_solver.cpp:106] Iteration 19000, lr = 2.38494e-06
I0421 23:31:51.181267  2056 solver.cpp:337] Iteration 19500, Testing net (#0)
I0421 23:31:53.079795  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78812
I0421 23:31:53.079850  2056 solver.cpp:404]     Test net output #1: loss = 0.837118 (* 1 = 0.837118 loss)
I0421 23:31:54.605727  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.caffemodel
I0421 23:31:54.612807  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.solverstate
I0421 23:31:54.615661  2056 solver.cpp:337] Iteration 20000, Testing net (#0)
I0421 23:31:56.569600  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7871
I0421 23:31:56.569638  2056 solver.cpp:404]     Test net output #1: loss = 0.845488 (* 1 = 0.845488 loss)
I0421 23:31:56.570583  2056 solver.cpp:228] Iteration 20000, loss = 0.197903
I0421 23:31:56.570622  2056 solver.cpp:244]     Train net output #0: loss = 0.197903 (* 1 = 0.197903 loss)
I0421 23:31:56.570644  2056 sgd_solver.cpp:106] Iteration 20000, lr = 2.32506e-06
I0421 23:31:58.066606  2056 solver.cpp:337] Iteration 20500, Testing net (#0)
I0421 23:31:59.308289  2056 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:32:00.031190  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79009
I0421 23:32:00.031225  2056 solver.cpp:404]     Test net output #1: loss = 0.834514 (* 1 = 0.834514 loss)
I0421 23:32:01.587062  2056 solver.cpp:337] Iteration 21000, Testing net (#0)
I0421 23:32:03.534020  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79208
I0421 23:32:03.534077  2056 solver.cpp:404]     Test net output #1: loss = 0.832984 (* 1 = 0.832984 loss)
I0421 23:32:03.535079  2056 solver.cpp:228] Iteration 21000, loss = 0.234932
I0421 23:32:03.535099  2056 solver.cpp:244]     Train net output #0: loss = 0.234933 (* 1 = 0.234933 loss)
I0421 23:32:03.535146  2056 sgd_solver.cpp:106] Iteration 21000, lr = 2.26858e-06
I0421 23:32:05.112411  2056 solver.cpp:337] Iteration 21500, Testing net (#0)
I0421 23:32:07.068011  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78318
I0421 23:32:07.068070  2056 solver.cpp:404]     Test net output #1: loss = 0.846735 (* 1 = 0.846735 loss)
I0421 23:32:08.599887  2056 solver.cpp:337] Iteration 22000, Testing net (#0)
I0421 23:32:10.581428  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79005
I0421 23:32:10.581594  2056 solver.cpp:404]     Test net output #1: loss = 0.836307 (* 1 = 0.836307 loss)
I0421 23:32:10.582604  2056 solver.cpp:228] Iteration 22000, loss = 0.101141
I0421 23:32:10.582624  2056 solver.cpp:244]     Train net output #0: loss = 0.101141 (* 1 = 0.101141 loss)
I0421 23:32:10.582648  2056 sgd_solver.cpp:106] Iteration 22000, lr = 2.2152e-06
I0421 23:32:12.080921  2056 solver.cpp:337] Iteration 22500, Testing net (#0)
I0421 23:32:14.057041  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79012
I0421 23:32:14.057080  2056 solver.cpp:404]     Test net output #1: loss = 0.831623 (* 1 = 0.831623 loss)
I0421 23:32:15.629072  2056 solver.cpp:337] Iteration 23000, Testing net (#0)
I0421 23:32:17.602223  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78715
I0421 23:32:17.602303  2056 solver.cpp:404]     Test net output #1: loss = 0.845258 (* 1 = 0.845258 loss)
I0421 23:32:17.603317  2056 solver.cpp:228] Iteration 23000, loss = 0.553884
I0421 23:32:17.603335  2056 solver.cpp:244]     Train net output #0: loss = 0.553884 (* 1 = 0.553884 loss)
I0421 23:32:17.603348  2056 sgd_solver.cpp:106] Iteration 23000, lr = 2.16466e-06
I0421 23:32:19.182219  2056 solver.cpp:337] Iteration 23500, Testing net (#0)
I0421 23:32:21.174792  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79006
I0421 23:32:21.174834  2056 solver.cpp:404]     Test net output #1: loss = 0.8362 (* 1 = 0.8362 loss)
I0421 23:32:22.696245  2056 solver.cpp:337] Iteration 24000, Testing net (#0)
I0421 23:32:24.668239  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79307
I0421 23:32:24.668278  2056 solver.cpp:404]     Test net output #1: loss = 0.830377 (* 1 = 0.830377 loss)
I0421 23:32:24.669323  2056 solver.cpp:228] Iteration 24000, loss = 0.600094
I0421 23:32:24.669342  2056 solver.cpp:244]     Train net output #0: loss = 0.600095 (* 1 = 0.600095 loss)
I0421 23:32:24.669370  2056 sgd_solver.cpp:106] Iteration 24000, lr = 2.11674e-06
I0421 23:32:26.225540  2056 solver.cpp:337] Iteration 24500, Testing net (#0)
I0421 23:32:28.170629  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79306
I0421 23:32:28.170672  2056 solver.cpp:404]     Test net output #1: loss = 0.840202 (* 1 = 0.840202 loss)
I0421 23:32:29.730659  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_25000.caffemodel
I0421 23:32:29.737691  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_25000.solverstate
I0421 23:32:29.740550  2056 solver.cpp:337] Iteration 25000, Testing net (#0)
I0421 23:32:31.710381  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7911
I0421 23:32:31.710422  2056 solver.cpp:404]     Test net output #1: loss = 0.83547 (* 1 = 0.83547 loss)
I0421 23:32:31.711391  2056 solver.cpp:228] Iteration 25000, loss = 0.0706259
I0421 23:32:31.711410  2056 solver.cpp:244]     Train net output #0: loss = 0.0706264 (* 1 = 0.0706264 loss)
I0421 23:32:31.711418  2056 sgd_solver.cpp:106] Iteration 25000, lr = 2.07121e-06
I0421 23:32:33.296311  2056 solver.cpp:337] Iteration 25500, Testing net (#0)
I0421 23:32:35.265271  2056 solver.cpp:404]     Test net output #0: accuracy = 0.794049
I0421 23:32:35.265327  2056 solver.cpp:404]     Test net output #1: loss = 0.831604 (* 1 = 0.831604 loss)
I0421 23:32:36.796531  2056 solver.cpp:337] Iteration 26000, Testing net (#0)
I0421 23:32:36.912506  2056 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:32:38.738059  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79309
I0421 23:32:38.738118  2056 solver.cpp:404]     Test net output #1: loss = 0.838799 (* 1 = 0.838799 loss)
I0421 23:32:38.739135  2056 solver.cpp:228] Iteration 26000, loss = 0.0909812
I0421 23:32:38.739154  2056 solver.cpp:244]     Train net output #0: loss = 0.0909817 (* 1 = 0.0909817 loss)
I0421 23:32:38.739163  2056 sgd_solver.cpp:106] Iteration 26000, lr = 2.02791e-06
I0421 23:32:40.228513  2056 solver.cpp:337] Iteration 26500, Testing net (#0)
I0421 23:32:42.188401  2056 solver.cpp:404]     Test net output #0: accuracy = 0.7901
I0421 23:32:42.188515  2056 solver.cpp:404]     Test net output #1: loss = 0.837005 (* 1 = 0.837005 loss)
I0421 23:32:43.745599  2056 solver.cpp:337] Iteration 27000, Testing net (#0)
I0421 23:32:45.729136  2056 solver.cpp:404]     Test net output #0: accuracy = 0.794079
I0421 23:32:45.729195  2056 solver.cpp:404]     Test net output #1: loss = 0.830277 (* 1 = 0.830277 loss)
I0421 23:32:45.730176  2056 solver.cpp:228] Iteration 27000, loss = 0.208336
I0421 23:32:45.730196  2056 solver.cpp:244]     Train net output #0: loss = 0.208337 (* 1 = 0.208337 loss)
I0421 23:32:45.730211  2056 sgd_solver.cpp:106] Iteration 27000, lr = 1.98666e-06
I0421 23:32:47.300072  2056 solver.cpp:337] Iteration 27500, Testing net (#0)
I0421 23:32:49.298434  2056 solver.cpp:404]     Test net output #0: accuracy = 0.794059
I0421 23:32:49.298473  2056 solver.cpp:404]     Test net output #1: loss = 0.835919 (* 1 = 0.835919 loss)
I0421 23:32:50.824337  2056 solver.cpp:337] Iteration 28000, Testing net (#0)
I0421 23:32:52.779994  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79208
I0421 23:32:52.780076  2056 solver.cpp:404]     Test net output #1: loss = 0.83723 (* 1 = 0.83723 loss)
I0421 23:32:52.783252  2056 solver.cpp:228] Iteration 28000, loss = 0.240974
I0421 23:32:52.783277  2056 solver.cpp:244]     Train net output #0: loss = 0.240974 (* 1 = 0.240974 loss)
I0421 23:32:52.783291  2056 sgd_solver.cpp:106] Iteration 28000, lr = 1.94732e-06
I0421 23:32:54.272313  2056 solver.cpp:337] Iteration 28500, Testing net (#0)
I0421 23:32:56.222517  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79603
I0421 23:32:56.222599  2056 solver.cpp:404]     Test net output #1: loss = 0.829556 (* 1 = 0.829556 loss)
I0421 23:32:57.777521  2056 solver.cpp:337] Iteration 29000, Testing net (#0)
I0421 23:32:59.720509  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79207
I0421 23:32:59.720567  2056 solver.cpp:404]     Test net output #1: loss = 0.832986 (* 1 = 0.832986 loss)
I0421 23:32:59.723614  2056 solver.cpp:228] Iteration 29000, loss = 0.793009
I0421 23:32:59.723636  2056 solver.cpp:244]     Train net output #0: loss = 0.79301 (* 1 = 0.79301 loss)
I0421 23:32:59.723649  2056 sgd_solver.cpp:106] Iteration 29000, lr = 1.90975e-06
I0421 23:33:01.309519  2056 solver.cpp:337] Iteration 29500, Testing net (#0)
I0421 23:33:03.291391  2056 solver.cpp:404]     Test net output #0: accuracy = 0.78913
I0421 23:33:03.291430  2056 solver.cpp:404]     Test net output #1: loss = 0.837736 (* 1 = 0.837736 loss)
I0421 23:33:04.884963  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30000.caffemodel
I0421 23:33:04.891698  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30000.solverstate
I0421 23:33:04.894490  2056 solver.cpp:337] Iteration 30000, Testing net (#0)
I0421 23:33:06.872258  2056 solver.cpp:404]     Test net output #0: accuracy = 0.795049
I0421 23:33:06.872310  2056 solver.cpp:404]     Test net output #1: loss = 0.829263 (* 1 = 0.829263 loss)
I0421 23:33:06.873281  2056 solver.cpp:228] Iteration 30000, loss = 0.72264
I0421 23:33:06.873302  2056 solver.cpp:244]     Train net output #0: loss = 0.72264 (* 1 = 0.72264 loss)
I0421 23:33:06.873314  2056 sgd_solver.cpp:106] Iteration 30000, lr = 1.87383e-06
I0421 23:33:08.381405  2056 solver.cpp:337] Iteration 30500, Testing net (#0)
I0421 23:33:10.389175  2056 solver.cpp:404]     Test net output #0: accuracy = 0.79405
I0421 23:33:10.389214  2056 solver.cpp:404]     Test net output #1: loss = 0.830421 (* 1 = 0.830421 loss)
I0421 23:33:10.559381  2056 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30552.caffemodel
I0421 23:33:10.566155  2056 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_30552.solverstate
I0421 23:33:10.568601  2056 solver.cpp:301] Optimization stopped early.
I0421 23:33:10.568613  2056 caffe.cpp:222] Optimization Done.
I0421 23:35:22.644685  2698 caffe.cpp:185] Using GPUs 0
I0421 23:35:22.648697  2698 caffe.cpp:190] GPU 0: GeForce GTX 980M
I0421 23:35:22.802016  2698 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 5.3e-05
display: 1000
max_iter: 100000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.00028
snapshot: 5000
snapshot_prefix: "/home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/"
solver_mode: GPU
device_id: 0
net: "/home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt"
I0421 23:35:22.802265  2698 solver.cpp:91] Creating training net from net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:35:22.802623  2698 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0421 23:35:22.802639  2698 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0421 23:35:22.802789  2698 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/train"
    batch_size: 256
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:35:22.802924  2698 layer_factory.hpp:77] Creating layer mnist
I0421 23:35:22.803478  2698 net.cpp:91] Creating Layer mnist
I0421 23:35:22.803508  2698 net.cpp:399] mnist -> data
I0421 23:35:22.803593  2698 net.cpp:399] mnist -> label
I0421 23:35:22.804378  2716 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/train
I0421 23:35:22.810983  2698 data_layer.cpp:41] output data size: 256,1,28,28
I0421 23:35:22.812839  2698 net.cpp:141] Setting up mnist
I0421 23:35:22.812921  2698 net.cpp:148] Top shape: 256 1 28 28 (200704)
I0421 23:35:22.812944  2698 net.cpp:148] Top shape: 256 (256)
I0421 23:35:22.812948  2698 net.cpp:156] Memory required for data: 803840
I0421 23:35:22.812974  2698 layer_factory.hpp:77] Creating layer conv1
I0421 23:35:22.812996  2698 net.cpp:91] Creating Layer conv1
I0421 23:35:22.813004  2698 net.cpp:425] conv1 <- data
I0421 23:35:22.813019  2698 net.cpp:399] conv1 -> conv1
I0421 23:35:22.943382  2698 net.cpp:141] Setting up conv1
I0421 23:35:22.943433  2698 net.cpp:148] Top shape: 256 20 24 24 (2949120)
I0421 23:35:22.943437  2698 net.cpp:156] Memory required for data: 12600320
I0421 23:35:22.943526  2698 layer_factory.hpp:77] Creating layer pool1
I0421 23:35:22.943542  2698 net.cpp:91] Creating Layer pool1
I0421 23:35:22.943547  2698 net.cpp:425] pool1 <- conv1
I0421 23:35:22.943554  2698 net.cpp:399] pool1 -> pool1
I0421 23:35:22.943622  2698 net.cpp:141] Setting up pool1
I0421 23:35:22.943631  2698 net.cpp:148] Top shape: 256 20 12 12 (737280)
I0421 23:35:22.943635  2698 net.cpp:156] Memory required for data: 15549440
I0421 23:35:22.943653  2698 layer_factory.hpp:77] Creating layer conv2
I0421 23:35:22.943681  2698 net.cpp:91] Creating Layer conv2
I0421 23:35:22.943686  2698 net.cpp:425] conv2 <- pool1
I0421 23:35:22.943711  2698 net.cpp:399] conv2 -> conv2
I0421 23:35:22.944859  2698 net.cpp:141] Setting up conv2
I0421 23:35:22.944872  2698 net.cpp:148] Top shape: 256 50 8 8 (819200)
I0421 23:35:22.944895  2698 net.cpp:156] Memory required for data: 18826240
I0421 23:35:22.944903  2698 layer_factory.hpp:77] Creating layer pool2
I0421 23:35:22.944928  2698 net.cpp:91] Creating Layer pool2
I0421 23:35:22.944947  2698 net.cpp:425] pool2 <- conv2
I0421 23:35:22.944952  2698 net.cpp:399] pool2 -> pool2
I0421 23:35:22.945032  2698 net.cpp:141] Setting up pool2
I0421 23:35:22.945039  2698 net.cpp:148] Top shape: 256 50 4 4 (204800)
I0421 23:35:22.945056  2698 net.cpp:156] Memory required for data: 19645440
I0421 23:35:22.945060  2698 layer_factory.hpp:77] Creating layer ip1
I0421 23:35:22.945091  2698 net.cpp:91] Creating Layer ip1
I0421 23:35:22.945096  2698 net.cpp:425] ip1 <- pool2
I0421 23:35:22.945116  2698 net.cpp:399] ip1 -> ip1
I0421 23:35:22.947820  2698 net.cpp:141] Setting up ip1
I0421 23:35:22.947837  2698 net.cpp:148] Top shape: 256 500 (128000)
I0421 23:35:22.947860  2698 net.cpp:156] Memory required for data: 20157440
I0421 23:35:22.947867  2698 layer_factory.hpp:77] Creating layer relu1
I0421 23:35:22.947896  2698 net.cpp:91] Creating Layer relu1
I0421 23:35:22.947916  2698 net.cpp:425] relu1 <- ip1
I0421 23:35:22.947922  2698 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:35:22.948191  2698 net.cpp:141] Setting up relu1
I0421 23:35:22.948202  2698 net.cpp:148] Top shape: 256 500 (128000)
I0421 23:35:22.948206  2698 net.cpp:156] Memory required for data: 20669440
I0421 23:35:22.948228  2698 layer_factory.hpp:77] Creating layer ip2
I0421 23:35:22.948258  2698 net.cpp:91] Creating Layer ip2
I0421 23:35:22.948262  2698 net.cpp:425] ip2 <- ip1
I0421 23:35:22.948282  2698 net.cpp:399] ip2 -> ip2
I0421 23:35:22.948668  2698 net.cpp:141] Setting up ip2
I0421 23:35:22.948675  2698 net.cpp:148] Top shape: 256 101 (25856)
I0421 23:35:22.948680  2698 net.cpp:156] Memory required for data: 20772864
I0421 23:35:22.948705  2698 layer_factory.hpp:77] Creating layer loss
I0421 23:35:22.948730  2698 net.cpp:91] Creating Layer loss
I0421 23:35:22.948734  2698 net.cpp:425] loss <- ip2
I0421 23:35:22.948740  2698 net.cpp:425] loss <- label
I0421 23:35:22.948746  2698 net.cpp:399] loss -> loss
I0421 23:35:22.948776  2698 layer_factory.hpp:77] Creating layer loss
I0421 23:35:22.949125  2698 net.cpp:141] Setting up loss
I0421 23:35:22.949136  2698 net.cpp:148] Top shape: (1)
I0421 23:35:22.949141  2698 net.cpp:151]     with loss weight 1
I0421 23:35:22.949195  2698 net.cpp:156] Memory required for data: 20772868
I0421 23:35:22.949213  2698 net.cpp:217] loss needs backward computation.
I0421 23:35:22.949218  2698 net.cpp:217] ip2 needs backward computation.
I0421 23:35:22.949223  2698 net.cpp:217] relu1 needs backward computation.
I0421 23:35:22.949228  2698 net.cpp:217] ip1 needs backward computation.
I0421 23:35:22.949231  2698 net.cpp:217] pool2 needs backward computation.
I0421 23:35:22.949237  2698 net.cpp:217] conv2 needs backward computation.
I0421 23:35:22.949241  2698 net.cpp:217] pool1 needs backward computation.
I0421 23:35:22.949260  2698 net.cpp:217] conv1 needs backward computation.
I0421 23:35:22.949265  2698 net.cpp:219] mnist does not need backward computation.
I0421 23:35:22.949270  2698 net.cpp:261] This network produces output loss
I0421 23:35:22.949280  2698 net.cpp:274] Network initialization done.
I0421 23:35:22.950567  2698 solver.cpp:181] Creating test net (#0) specified by net file: /home/yang/workspace/grad/zhang505-yisupeng-final/lenet/yang_train_test.prototxt
I0421 23:35:22.950626  2698 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0421 23:35:22.950749  2698 net.cpp:49] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/yang/workspace/grad/zhang505-yisupeng-final/data/test"
    batch_size: 1000
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 101
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0421 23:35:22.950836  2698 layer_factory.hpp:77] Creating layer mnist
I0421 23:35:22.951334  2698 net.cpp:91] Creating Layer mnist
I0421 23:35:22.951369  2698 net.cpp:399] mnist -> data
I0421 23:35:22.951402  2698 net.cpp:399] mnist -> label
I0421 23:35:22.952095  2718 db_lmdb.cpp:38] Opened lmdb /home/yang/workspace/grad/zhang505-yisupeng-final/data/test
I0421 23:35:22.952366  2698 data_layer.cpp:41] output data size: 1000,1,28,28
I0421 23:35:22.956753  2698 net.cpp:141] Setting up mnist
I0421 23:35:22.956799  2698 net.cpp:148] Top shape: 1000 1 28 28 (784000)
I0421 23:35:22.956822  2698 net.cpp:148] Top shape: 1000 (1000)
I0421 23:35:22.956845  2698 net.cpp:156] Memory required for data: 3140000
I0421 23:35:22.956851  2698 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0421 23:35:22.956879  2698 net.cpp:91] Creating Layer label_mnist_1_split
I0421 23:35:22.956918  2698 net.cpp:425] label_mnist_1_split <- label
I0421 23:35:22.956928  2698 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_0
I0421 23:35:22.956938  2698 net.cpp:399] label_mnist_1_split -> label_mnist_1_split_1
I0421 23:35:22.957011  2698 net.cpp:141] Setting up label_mnist_1_split
I0421 23:35:22.957032  2698 net.cpp:148] Top shape: 1000 (1000)
I0421 23:35:22.957036  2698 net.cpp:148] Top shape: 1000 (1000)
I0421 23:35:22.957041  2698 net.cpp:156] Memory required for data: 3148000
I0421 23:35:22.957062  2698 layer_factory.hpp:77] Creating layer conv1
I0421 23:35:22.957074  2698 net.cpp:91] Creating Layer conv1
I0421 23:35:22.957082  2698 net.cpp:425] conv1 <- data
I0421 23:35:22.957087  2698 net.cpp:399] conv1 -> conv1
I0421 23:35:22.958696  2698 net.cpp:141] Setting up conv1
I0421 23:35:22.958729  2698 net.cpp:148] Top shape: 1000 20 24 24 (11520000)
I0421 23:35:22.958752  2698 net.cpp:156] Memory required for data: 49228000
I0421 23:35:22.958781  2698 layer_factory.hpp:77] Creating layer pool1
I0421 23:35:22.958789  2698 net.cpp:91] Creating Layer pool1
I0421 23:35:22.958811  2698 net.cpp:425] pool1 <- conv1
I0421 23:35:22.958832  2698 net.cpp:399] pool1 -> pool1
I0421 23:35:22.958884  2698 net.cpp:141] Setting up pool1
I0421 23:35:22.958894  2698 net.cpp:148] Top shape: 1000 20 12 12 (2880000)
I0421 23:35:22.958899  2698 net.cpp:156] Memory required for data: 60748000
I0421 23:35:22.958904  2698 layer_factory.hpp:77] Creating layer conv2
I0421 23:35:22.958928  2698 net.cpp:91] Creating Layer conv2
I0421 23:35:22.958933  2698 net.cpp:425] conv2 <- pool1
I0421 23:35:22.958940  2698 net.cpp:399] conv2 -> conv2
I0421 23:35:22.960137  2698 net.cpp:141] Setting up conv2
I0421 23:35:22.960151  2698 net.cpp:148] Top shape: 1000 50 8 8 (3200000)
I0421 23:35:22.960175  2698 net.cpp:156] Memory required for data: 73548000
I0421 23:35:22.960182  2698 layer_factory.hpp:77] Creating layer pool2
I0421 23:35:22.960191  2698 net.cpp:91] Creating Layer pool2
I0421 23:35:22.960196  2698 net.cpp:425] pool2 <- conv2
I0421 23:35:22.960203  2698 net.cpp:399] pool2 -> pool2
I0421 23:35:22.960252  2698 net.cpp:141] Setting up pool2
I0421 23:35:22.960259  2698 net.cpp:148] Top shape: 1000 50 4 4 (800000)
I0421 23:35:22.960263  2698 net.cpp:156] Memory required for data: 76748000
I0421 23:35:22.960269  2698 layer_factory.hpp:77] Creating layer ip1
I0421 23:35:22.960275  2698 net.cpp:91] Creating Layer ip1
I0421 23:35:22.960302  2698 net.cpp:425] ip1 <- pool2
I0421 23:35:22.960309  2698 net.cpp:399] ip1 -> ip1
I0421 23:35:22.963441  2698 net.cpp:141] Setting up ip1
I0421 23:35:22.963454  2698 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:35:22.963479  2698 net.cpp:156] Memory required for data: 78748000
I0421 23:35:22.963500  2698 layer_factory.hpp:77] Creating layer relu1
I0421 23:35:22.963522  2698 net.cpp:91] Creating Layer relu1
I0421 23:35:22.963528  2698 net.cpp:425] relu1 <- ip1
I0421 23:35:22.963534  2698 net.cpp:386] relu1 -> ip1 (in-place)
I0421 23:35:22.963865  2698 net.cpp:141] Setting up relu1
I0421 23:35:22.963881  2698 net.cpp:148] Top shape: 1000 500 (500000)
I0421 23:35:22.963886  2698 net.cpp:156] Memory required for data: 80748000
I0421 23:35:22.963907  2698 layer_factory.hpp:77] Creating layer ip2
I0421 23:35:22.963917  2698 net.cpp:91] Creating Layer ip2
I0421 23:35:22.963923  2698 net.cpp:425] ip2 <- ip1
I0421 23:35:22.963929  2698 net.cpp:399] ip2 -> ip2
I0421 23:35:22.964406  2698 net.cpp:141] Setting up ip2
I0421 23:35:22.964427  2698 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:35:22.964433  2698 net.cpp:156] Memory required for data: 81152000
I0421 23:35:22.964457  2698 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0421 23:35:22.964478  2698 net.cpp:91] Creating Layer ip2_ip2_0_split
I0421 23:35:22.964483  2698 net.cpp:425] ip2_ip2_0_split <- ip2
I0421 23:35:22.964504  2698 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0421 23:35:22.964511  2698 net.cpp:399] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0421 23:35:22.964542  2698 net.cpp:141] Setting up ip2_ip2_0_split
I0421 23:35:22.964550  2698 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:35:22.964555  2698 net.cpp:148] Top shape: 1000 101 (101000)
I0421 23:35:22.964560  2698 net.cpp:156] Memory required for data: 81960000
I0421 23:35:22.964563  2698 layer_factory.hpp:77] Creating layer accuracy
I0421 23:35:22.964570  2698 net.cpp:91] Creating Layer accuracy
I0421 23:35:22.964575  2698 net.cpp:425] accuracy <- ip2_ip2_0_split_0
I0421 23:35:22.964579  2698 net.cpp:425] accuracy <- label_mnist_1_split_0
I0421 23:35:22.964602  2698 net.cpp:399] accuracy -> accuracy
I0421 23:35:22.964620  2698 net.cpp:141] Setting up accuracy
I0421 23:35:22.964627  2698 net.cpp:148] Top shape: (1)
I0421 23:35:22.964632  2698 net.cpp:156] Memory required for data: 81960004
I0421 23:35:22.964656  2698 layer_factory.hpp:77] Creating layer loss
I0421 23:35:22.964663  2698 net.cpp:91] Creating Layer loss
I0421 23:35:22.964668  2698 net.cpp:425] loss <- ip2_ip2_0_split_1
I0421 23:35:22.964673  2698 net.cpp:425] loss <- label_mnist_1_split_1
I0421 23:35:22.964679  2698 net.cpp:399] loss -> loss
I0421 23:35:22.964699  2698 layer_factory.hpp:77] Creating layer loss
I0421 23:35:22.965385  2698 net.cpp:141] Setting up loss
I0421 23:35:22.965396  2698 net.cpp:148] Top shape: (1)
I0421 23:35:22.965402  2698 net.cpp:151]     with loss weight 1
I0421 23:35:22.965428  2698 net.cpp:156] Memory required for data: 81960008
I0421 23:35:22.965446  2698 net.cpp:217] loss needs backward computation.
I0421 23:35:22.965452  2698 net.cpp:219] accuracy does not need backward computation.
I0421 23:35:22.965457  2698 net.cpp:217] ip2_ip2_0_split needs backward computation.
I0421 23:35:22.965476  2698 net.cpp:217] ip2 needs backward computation.
I0421 23:35:22.965479  2698 net.cpp:217] relu1 needs backward computation.
I0421 23:35:22.965484  2698 net.cpp:217] ip1 needs backward computation.
I0421 23:35:22.965488  2698 net.cpp:217] pool2 needs backward computation.
I0421 23:35:22.965494  2698 net.cpp:217] conv2 needs backward computation.
I0421 23:35:22.965498  2698 net.cpp:217] pool1 needs backward computation.
I0421 23:35:22.965503  2698 net.cpp:217] conv1 needs backward computation.
I0421 23:35:22.965507  2698 net.cpp:219] label_mnist_1_split does not need backward computation.
I0421 23:35:22.965512  2698 net.cpp:219] mnist does not need backward computation.
I0421 23:35:22.965517  2698 net.cpp:261] This network produces output accuracy
I0421 23:35:22.965523  2698 net.cpp:261] This network produces output loss
I0421 23:35:22.965534  2698 net.cpp:274] Network initialization done.
I0421 23:35:22.965580  2698 solver.cpp:60] Solver scaffolding done.
I0421 23:35:22.965944  2698 caffe.cpp:129] Finetuning from ./snapshot/_iter_30000.caffemodel
I0421 23:35:22.972164  2698 caffe.cpp:219] Starting Optimization
I0421 23:35:22.972206  2698 solver.cpp:279] Solving LeNet
I0421 23:35:22.972229  2698 solver.cpp:280] Learning Rate Policy: inv
I0421 23:35:22.972740  2698 solver.cpp:337] Iteration 0, Testing net (#0)
I0421 23:35:22.973248  2698 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:35:25.027361  2698 solver.cpp:404]     Test net output #0: accuracy = 0.79506
I0421 23:35:25.027429  2698 solver.cpp:404]     Test net output #1: loss = 0.829283 (* 1 = 0.829283 loss)
I0421 23:35:25.030617  2698 solver.cpp:228] Iteration 0, loss = 0.515876
I0421 23:35:25.030668  2698 solver.cpp:244]     Train net output #0: loss = 0.515876 (* 1 = 0.515876 loss)
I0421 23:35:25.030705  2698 sgd_solver.cpp:106] Iteration 0, lr = 5.3e-05
I0421 23:35:29.176576  2698 solver.cpp:337] Iteration 500, Testing net (#0)
I0421 23:35:31.174077  2698 solver.cpp:404]     Test net output #0: accuracy = 0.76429
I0421 23:35:31.174149  2698 solver.cpp:404]     Test net output #1: loss = 0.912709 (* 1 = 0.912709 loss)
I0421 23:35:35.304921  2698 solver.cpp:337] Iteration 1000, Testing net (#0)
I0421 23:35:37.303094  2698 solver.cpp:404]     Test net output #0: accuracy = 0.74655
I0421 23:35:37.303153  2698 solver.cpp:404]     Test net output #1: loss = 0.992814 (* 1 = 0.992814 loss)
I0421 23:35:37.305150  2698 solver.cpp:228] Iteration 1000, loss = 0.186274
I0421 23:35:37.305169  2698 solver.cpp:244]     Train net output #0: loss = 0.186274 (* 1 = 0.186274 loss)
I0421 23:35:37.305220  2698 sgd_solver.cpp:106] Iteration 1000, lr = 4.93437e-05
I0421 23:35:41.437698  2698 solver.cpp:337] Iteration 1500, Testing net (#0)
I0421 23:35:43.418795  2698 solver.cpp:404]     Test net output #0: accuracy = 0.76635
I0421 23:35:43.418855  2698 solver.cpp:404]     Test net output #1: loss = 0.943874 (* 1 = 0.943874 loss)
I0421 23:35:47.572515  2698 solver.cpp:337] Iteration 2000, Testing net (#0)
I0421 23:35:49.543340  2698 solver.cpp:404]     Test net output #0: accuracy = 0.76931
I0421 23:35:49.543417  2698 solver.cpp:404]     Test net output #1: loss = 0.914507 (* 1 = 0.914507 loss)
I0421 23:35:49.545465  2698 solver.cpp:228] Iteration 2000, loss = 0.0452136
I0421 23:35:49.545501  2698 solver.cpp:244]     Train net output #0: loss = 0.0452136 (* 1 = 0.0452136 loss)
I0421 23:35:49.545511  2698 sgd_solver.cpp:106] Iteration 2000, lr = 4.62264e-05
I0421 23:35:53.676445  2698 solver.cpp:337] Iteration 2500, Testing net (#0)
I0421 23:35:55.655974  2698 solver.cpp:404]     Test net output #0: accuracy = 0.78219
I0421 23:35:55.656050  2698 solver.cpp:404]     Test net output #1: loss = 0.870595 (* 1 = 0.870595 loss)
I0421 23:35:59.796468  2698 solver.cpp:337] Iteration 3000, Testing net (#0)
I0421 23:36:01.734441  2698 solver.cpp:404]     Test net output #0: accuracy = 0.7594
I0421 23:36:01.734501  2698 solver.cpp:404]     Test net output #1: loss = 0.943597 (* 1 = 0.943597 loss)
I0421 23:36:01.736584  2698 solver.cpp:228] Iteration 3000, loss = 0.194215
I0421 23:36:01.736606  2698 solver.cpp:244]     Train net output #0: loss = 0.194215 (* 1 = 0.194215 loss)
I0421 23:36:01.736616  2698 sgd_solver.cpp:106] Iteration 3000, lr = 4.3533e-05
I0421 23:36:05.872834  2698 solver.cpp:337] Iteration 3500, Testing net (#0)
I0421 23:36:07.857983  2698 solver.cpp:404]     Test net output #0: accuracy = 0.78715
I0421 23:36:07.858042  2698 solver.cpp:404]     Test net output #1: loss = 0.873074 (* 1 = 0.873074 loss)
I0421 23:36:12.000358  2698 solver.cpp:337] Iteration 4000, Testing net (#0)
I0421 23:36:13.987972  2698 solver.cpp:404]     Test net output #0: accuracy = 0.78318
I0421 23:36:13.988054  2698 solver.cpp:404]     Test net output #1: loss = 0.855595 (* 1 = 0.855595 loss)
I0421 23:36:13.989971  2698 solver.cpp:228] Iteration 4000, loss = 0.0318459
I0421 23:36:13.989989  2698 solver.cpp:244]     Train net output #0: loss = 0.0318459 (* 1 = 0.0318459 loss)
I0421 23:36:13.989998  2698 sgd_solver.cpp:106] Iteration 4000, lr = 4.11794e-05
I0421 23:36:18.147058  2698 solver.cpp:337] Iteration 4500, Testing net (#0)
I0421 23:36:20.097825  2698 solver.cpp:404]     Test net output #0: accuracy = 0.78517
I0421 23:36:20.097895  2698 solver.cpp:404]     Test net output #1: loss = 0.852448 (* 1 = 0.852448 loss)
I0421 23:36:24.223536  2698 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.caffemodel
I0421 23:36:24.236536  2698 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_5000.solverstate
I0421 23:36:24.239531  2698 solver.cpp:337] Iteration 5000, Testing net (#0)
I0421 23:36:24.638185  2698 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:36:26.224764  2698 solver.cpp:404]     Test net output #0: accuracy = 0.78713
I0421 23:36:26.224800  2698 solver.cpp:404]     Test net output #1: loss = 0.876261 (* 1 = 0.876261 loss)
I0421 23:36:26.226960  2698 solver.cpp:228] Iteration 5000, loss = 0.578018
I0421 23:36:26.226997  2698 solver.cpp:244]     Train net output #0: loss = 0.578018 (* 1 = 0.578018 loss)
I0421 23:36:26.227027  2698 sgd_solver.cpp:106] Iteration 5000, lr = 3.91028e-05
I0421 23:36:30.373759  2698 solver.cpp:337] Iteration 5500, Testing net (#0)
I0421 23:36:32.360471  2698 solver.cpp:404]     Test net output #0: accuracy = 0.79604
I0421 23:36:32.360540  2698 solver.cpp:404]     Test net output #1: loss = 0.839884 (* 1 = 0.839884 loss)
I0421 23:36:36.501659  2698 solver.cpp:337] Iteration 6000, Testing net (#0)
I0421 23:36:38.456943  2698 solver.cpp:404]     Test net output #0: accuracy = 0.79803
I0421 23:36:38.457013  2698 solver.cpp:404]     Test net output #1: loss = 0.813671 (* 1 = 0.813671 loss)
I0421 23:36:38.459009  2698 solver.cpp:228] Iteration 6000, loss = 0.505291
I0421 23:36:38.459027  2698 solver.cpp:244]     Train net output #0: loss = 0.505291 (* 1 = 0.505291 loss)
I0421 23:36:38.459035  2698 sgd_solver.cpp:106] Iteration 6000, lr = 3.72551e-05
I0421 23:36:42.582041  2698 solver.cpp:337] Iteration 6500, Testing net (#0)
I0421 23:36:44.597862  2698 solver.cpp:404]     Test net output #0: accuracy = 0.78807
I0421 23:36:44.597932  2698 solver.cpp:404]     Test net output #1: loss = 0.841349 (* 1 = 0.841349 loss)
I0421 23:36:48.744737  2698 solver.cpp:337] Iteration 7000, Testing net (#0)
I0421 23:36:50.744848  2698 solver.cpp:404]     Test net output #0: accuracy = 0.79903
I0421 23:36:50.744910  2698 solver.cpp:404]     Test net output #1: loss = 0.83811 (* 1 = 0.83811 loss)
I0421 23:36:50.746969  2698 solver.cpp:228] Iteration 7000, loss = 0.447031
I0421 23:36:50.746990  2698 solver.cpp:244]     Train net output #0: loss = 0.447032 (* 1 = 0.447032 loss)
I0421 23:36:50.746999  2698 sgd_solver.cpp:106] Iteration 7000, lr = 3.55991e-05
I0421 23:36:54.877938  2698 solver.cpp:337] Iteration 7500, Testing net (#0)
I0421 23:36:56.799433  2698 solver.cpp:404]     Test net output #0: accuracy = 0.79705
I0421 23:36:56.799473  2698 solver.cpp:404]     Test net output #1: loss = 0.815029 (* 1 = 0.815029 loss)
I0421 23:37:00.941256  2698 solver.cpp:337] Iteration 8000, Testing net (#0)
I0421 23:37:02.887357  2698 solver.cpp:404]     Test net output #0: accuracy = 0.802
I0421 23:37:02.887398  2698 solver.cpp:404]     Test net output #1: loss = 0.800826 (* 1 = 0.800826 loss)
I0421 23:37:02.889576  2698 solver.cpp:228] Iteration 8000, loss = 0.443503
I0421 23:37:02.889597  2698 solver.cpp:244]     Train net output #0: loss = 0.443503 (* 1 = 0.443503 loss)
I0421 23:37:02.889607  2698 sgd_solver.cpp:106] Iteration 8000, lr = 3.41053e-05
I0421 23:37:07.014518  2698 solver.cpp:337] Iteration 8500, Testing net (#0)
I0421 23:37:08.936861  2698 solver.cpp:404]     Test net output #0: accuracy = 0.795059
I0421 23:37:08.936902  2698 solver.cpp:404]     Test net output #1: loss = 0.833337 (* 1 = 0.833337 loss)
I0421 23:37:13.091884  2698 solver.cpp:337] Iteration 9000, Testing net (#0)
I0421 23:37:15.058758  2698 solver.cpp:404]     Test net output #0: accuracy = 0.80692
I0421 23:37:15.058799  2698 solver.cpp:404]     Test net output #1: loss = 0.812472 (* 1 = 0.812472 loss)
I0421 23:37:15.060773  2698 solver.cpp:228] Iteration 9000, loss = 0.0471717
I0421 23:37:15.060811  2698 solver.cpp:244]     Train net output #0: loss = 0.0471725 (* 1 = 0.0471725 loss)
I0421 23:37:15.060827  2698 sgd_solver.cpp:106] Iteration 9000, lr = 3.275e-05
I0421 23:37:19.202394  2698 solver.cpp:337] Iteration 9500, Testing net (#0)
I0421 23:37:21.201675  2698 solver.cpp:404]     Test net output #0: accuracy = 0.808909
I0421 23:37:21.201745  2698 solver.cpp:404]     Test net output #1: loss = 0.798852 (* 1 = 0.798852 loss)
I0421 23:37:25.350087  2698 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.caffemodel
I0421 23:37:25.361438  2698 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_10000.solverstate
I0421 23:37:25.364665  2698 solver.cpp:337] Iteration 10000, Testing net (#0)
I0421 23:37:26.388365  2698 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:37:27.344838  2698 solver.cpp:404]     Test net output #0: accuracy = 0.807899
I0421 23:37:27.344897  2698 solver.cpp:404]     Test net output #1: loss = 0.798387 (* 1 = 0.798387 loss)
I0421 23:37:27.346983  2698 solver.cpp:228] Iteration 10000, loss = 0.345473
I0421 23:37:27.347018  2698 solver.cpp:244]     Train net output #0: loss = 0.345474 (* 1 = 0.345474 loss)
I0421 23:37:27.347033  2698 sgd_solver.cpp:106] Iteration 10000, lr = 3.1514e-05
I0421 23:37:31.492656  2698 solver.cpp:337] Iteration 10500, Testing net (#0)
I0421 23:37:33.462044  2698 solver.cpp:404]     Test net output #0: accuracy = 0.80397
I0421 23:37:33.462101  2698 solver.cpp:404]     Test net output #1: loss = 0.829433 (* 1 = 0.829433 loss)
I0421 23:37:37.593775  2698 solver.cpp:337] Iteration 11000, Testing net (#0)
I0421 23:37:39.583699  2698 solver.cpp:404]     Test net output #0: accuracy = 0.79896
I0421 23:37:39.583737  2698 solver.cpp:404]     Test net output #1: loss = 0.800354 (* 1 = 0.800354 loss)
I0421 23:37:39.585767  2698 solver.cpp:228] Iteration 11000, loss = 0.0634554
I0421 23:37:39.585788  2698 solver.cpp:244]     Train net output #0: loss = 0.0634565 (* 1 = 0.0634565 loss)
I0421 23:37:39.585796  2698 sgd_solver.cpp:106] Iteration 11000, lr = 3.03817e-05
I0421 23:37:43.726632  2698 solver.cpp:337] Iteration 11500, Testing net (#0)
I0421 23:37:45.720834  2698 solver.cpp:404]     Test net output #0: accuracy = 0.8099
I0421 23:37:45.720898  2698 solver.cpp:404]     Test net output #1: loss = 0.794225 (* 1 = 0.794225 loss)
I0421 23:37:49.861546  2698 solver.cpp:337] Iteration 12000, Testing net (#0)
I0421 23:37:51.870695  2698 solver.cpp:404]     Test net output #0: accuracy = 0.80694
I0421 23:37:51.870770  2698 solver.cpp:404]     Test net output #1: loss = 0.800469 (* 1 = 0.800469 loss)
I0421 23:37:51.872838  2698 solver.cpp:228] Iteration 12000, loss = 0.364224
I0421 23:37:51.872858  2698 solver.cpp:244]     Train net output #0: loss = 0.364224 (* 1 = 0.364224 loss)
I0421 23:37:51.872866  2698 sgd_solver.cpp:106] Iteration 12000, lr = 2.93399e-05
I0421 23:37:56.011458  2698 solver.cpp:337] Iteration 12500, Testing net (#0)
I0421 23:37:57.953784  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81388
I0421 23:37:57.953842  2698 solver.cpp:404]     Test net output #1: loss = 0.809417 (* 1 = 0.809417 loss)
I0421 23:38:02.097638  2698 solver.cpp:337] Iteration 13000, Testing net (#0)
I0421 23:38:04.037489  2698 solver.cpp:404]     Test net output #0: accuracy = 0.8069
I0421 23:38:04.037529  2698 solver.cpp:404]     Test net output #1: loss = 0.790497 (* 1 = 0.790497 loss)
I0421 23:38:04.039557  2698 solver.cpp:228] Iteration 13000, loss = 0.509826
I0421 23:38:04.039577  2698 solver.cpp:244]     Train net output #0: loss = 0.509827 (* 1 = 0.509827 loss)
I0421 23:38:04.039584  2698 sgd_solver.cpp:106] Iteration 13000, lr = 2.83779e-05
I0421 23:38:08.175271  2698 solver.cpp:337] Iteration 13500, Testing net (#0)
I0421 23:38:10.130214  2698 solver.cpp:404]     Test net output #0: accuracy = 0.8109
I0421 23:38:10.130270  2698 solver.cpp:404]     Test net output #1: loss = 0.784791 (* 1 = 0.784791 loss)
I0421 23:38:14.270915  2698 solver.cpp:337] Iteration 14000, Testing net (#0)
I0421 23:38:16.243912  2698 solver.cpp:404]     Test net output #0: accuracy = 0.80495
I0421 23:38:16.243983  2698 solver.cpp:404]     Test net output #1: loss = 0.800407 (* 1 = 0.800407 loss)
I0421 23:38:16.245998  2698 solver.cpp:228] Iteration 14000, loss = 0.0539236
I0421 23:38:16.246038  2698 solver.cpp:244]     Train net output #0: loss = 0.0539244 (* 1 = 0.0539244 loss)
I0421 23:38:16.246047  2698 sgd_solver.cpp:106] Iteration 14000, lr = 2.74864e-05
I0421 23:38:20.393018  2698 solver.cpp:337] Iteration 14500, Testing net (#0)
I0421 23:38:22.359138  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81188
I0421 23:38:22.359211  2698 solver.cpp:404]     Test net output #1: loss = 0.797111 (* 1 = 0.797111 loss)
I0421 23:38:26.491377  2698 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.caffemodel
I0421 23:38:26.502859  2698 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_15000.solverstate
I0421 23:38:26.505977  2698 solver.cpp:337] Iteration 15000, Testing net (#0)
I0421 23:38:28.320443  2698 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:38:28.463861  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81484
I0421 23:38:28.463897  2698 solver.cpp:404]     Test net output #1: loss = 0.783507 (* 1 = 0.783507 loss)
I0421 23:38:28.466063  2698 solver.cpp:228] Iteration 15000, loss = 0.0230065
I0421 23:38:28.466100  2698 solver.cpp:244]     Train net output #0: loss = 0.0230076 (* 1 = 0.0230076 loss)
I0421 23:38:28.466112  2698 sgd_solver.cpp:106] Iteration 15000, lr = 2.66576e-05
I0421 23:38:32.604687  2698 solver.cpp:337] Iteration 15500, Testing net (#0)
I0421 23:38:34.576763  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81287
I0421 23:38:34.576822  2698 solver.cpp:404]     Test net output #1: loss = 0.782314 (* 1 = 0.782314 loss)
I0421 23:38:38.715646  2698 solver.cpp:337] Iteration 16000, Testing net (#0)
I0421 23:38:40.662286  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81091
I0421 23:38:40.662341  2698 solver.cpp:404]     Test net output #1: loss = 0.804539 (* 1 = 0.804539 loss)
I0421 23:38:40.664491  2698 solver.cpp:228] Iteration 16000, loss = 0.0792769
I0421 23:38:40.664546  2698 solver.cpp:244]     Train net output #0: loss = 0.0792778 (* 1 = 0.0792778 loss)
I0421 23:38:40.664561  2698 sgd_solver.cpp:106] Iteration 16000, lr = 2.58849e-05
I0421 23:38:44.821184  2698 solver.cpp:337] Iteration 16500, Testing net (#0)
I0421 23:38:46.807627  2698 solver.cpp:404]     Test net output #0: accuracy = 0.80891
I0421 23:38:46.807685  2698 solver.cpp:404]     Test net output #1: loss = 0.787111 (* 1 = 0.787111 loss)
I0421 23:38:50.948776  2698 solver.cpp:337] Iteration 17000, Testing net (#0)
I0421 23:38:52.896462  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81485
I0421 23:38:52.896536  2698 solver.cpp:404]     Test net output #1: loss = 0.782296 (* 1 = 0.782296 loss)
I0421 23:38:52.898504  2698 solver.cpp:228] Iteration 17000, loss = 0.0246848
I0421 23:38:52.898536  2698 solver.cpp:244]     Train net output #0: loss = 0.0246858 (* 1 = 0.0246858 loss)
I0421 23:38:52.898545  2698 sgd_solver.cpp:106] Iteration 17000, lr = 2.51625e-05
I0421 23:38:57.041647  2698 solver.cpp:337] Iteration 17500, Testing net (#0)
I0421 23:38:58.908998  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81287
I0421 23:38:58.909055  2698 solver.cpp:404]     Test net output #1: loss = 0.784697 (* 1 = 0.784697 loss)
I0421 23:39:03.049861  2698 solver.cpp:337] Iteration 18000, Testing net (#0)
I0421 23:39:05.051586  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81482
I0421 23:39:05.051656  2698 solver.cpp:404]     Test net output #1: loss = 0.797979 (* 1 = 0.797979 loss)
I0421 23:39:05.053591  2698 solver.cpp:228] Iteration 18000, loss = 0.491307
I0421 23:39:05.053608  2698 solver.cpp:244]     Train net output #0: loss = 0.491308 (* 1 = 0.491308 loss)
I0421 23:39:05.053617  2698 sgd_solver.cpp:106] Iteration 18000, lr = 2.44854e-05
I0421 23:39:09.190759  2698 solver.cpp:337] Iteration 18500, Testing net (#0)
I0421 23:39:11.184761  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81189
I0421 23:39:11.184823  2698 solver.cpp:404]     Test net output #1: loss = 0.783395 (* 1 = 0.783395 loss)
I0421 23:39:15.343987  2698 solver.cpp:337] Iteration 19000, Testing net (#0)
I0421 23:39:17.329350  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81288
I0421 23:39:17.329416  2698 solver.cpp:404]     Test net output #1: loss = 0.778184 (* 1 = 0.778184 loss)
I0421 23:39:17.331382  2698 solver.cpp:228] Iteration 19000, loss = 0.390454
I0421 23:39:17.331416  2698 solver.cpp:244]     Train net output #0: loss = 0.390455 (* 1 = 0.390455 loss)
I0421 23:39:17.331428  2698 sgd_solver.cpp:106] Iteration 19000, lr = 2.38494e-05
I0421 23:39:21.483922  2698 solver.cpp:337] Iteration 19500, Testing net (#0)
I0421 23:39:23.399714  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81386
I0421 23:39:23.399772  2698 solver.cpp:404]     Test net output #1: loss = 0.788113 (* 1 = 0.788113 loss)
I0421 23:39:27.532495  2698 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.caffemodel
I0421 23:39:27.543685  2698 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20000.solverstate
I0421 23:39:27.546996  2698 solver.cpp:337] Iteration 20000, Testing net (#0)
I0421 23:39:29.427847  2698 blocking_queue.cpp:50] Data layer prefetch queue empty
I0421 23:39:29.504492  2698 solver.cpp:404]     Test net output #0: accuracy = 0.81581
I0421 23:39:29.504547  2698 solver.cpp:404]     Test net output #1: loss = 0.789964 (* 1 = 0.789964 loss)
I0421 23:39:29.506557  2698 solver.cpp:228] Iteration 20000, loss = 0.394469
I0421 23:39:29.506577  2698 solver.cpp:244]     Train net output #0: loss = 0.39447 (* 1 = 0.39447 loss)
I0421 23:39:29.506587  2698 sgd_solver.cpp:106] Iteration 20000, lr = 2.32506e-05
I0421 23:39:31.944913  2698 solver.cpp:454] Snapshotting to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20287.caffemodel
I0421 23:39:31.956157  2698 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/yang/workspace/grad/zhang505-yisupeng-final/snapshot/_iter_20287.solverstate
I0421 23:39:31.958413  2698 solver.cpp:301] Optimization stopped early.
I0421 23:39:31.958439  2698 caffe.cpp:222] Optimization Done.
